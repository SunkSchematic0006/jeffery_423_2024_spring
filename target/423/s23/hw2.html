<h3>CSE 423 Homework 2: A Lexical Analyzer</h3>

Due: Tuesday February 7, 11:59pm<p>

In this assignment you will write a lexical analyzer in
<code>flex(1)</code>, for a subset of Python known as the <code>PunY</code>
language that is documented in its own reference document, with a
link given below.

<p>
<A name="engineering"><h3> Engineering Requirements </h3></A>

In this and all subsequent assignments in 423, please meet the following
engineering requirements. Points will be assigned in grading for them.

<dl>
<dt> Mandatory .zip file unpacks to the current directory.
<dd> Turnin Must be a .zip containing a valid compressed archive that can
     be uncompressed on Linux via the command <code>unzip</code>.
     It may not be a <code>.tar</code> or a <code>.rar</code>
     or a <code>.bzip</code> or whatever, whether disguised or renamed or not.
     The <code>.zip</code> must unpack into the current directory, not a
     subdirectory*.  *Actually, subdirectories are OK, but there must be a
     top-level makefile that builds an executable named puny in the top-level
     directory from where you were unzipped.
<dt> "make" just works on login.cs.nmt.edu; it builds an executable
     named <code>puny</code>.
<dd> You have to supply a makefile that contains build rules as follows.
<dt> Separate Compilation.
<dd> The compiler must be invoked separately on each source file.
     E.g. all .c files must be linked together via .o object files, no using
     <code>#include</code> of .c files.  No including any code (function
     bodies) in .h files.
<dt> No Warnings... and don't say that you didn't try
<dd> The gcc compiler must be invoked with <code>-Wall</code>
     on all compilation lines. If you are using another language,
     which must be approved by Dr. J, you must also seek to use all
     its warning options, or get any omissions approved.
     Points will be lost if you don't fix warnings.  There are some common
     lex/flex warnings, such as about not using <code>input()</code> that
     are no big deal, but use
<pre>
%option noinput
%option nounput
</pre>
     to shut them up.  See the instructor if you are unable to fix a warning.
<dt> Valgrind validation
<dd> You should test your work on lovecraft.cs.nmt.edu both with and without
     valgrind.
     Valgrind output should be free of memory errors.  You will also become
     more experienced with gdb in this class, but valgrind is your first line
     of defense.  For the purposes of this class, a "memory error" is a
     message from valgrind indicating a
     read or write of one or more bytes of illegal, out-of-bounds,
     or uninitialized memory.
     Other valgrind messages may be useful but will not cost you points.
</dl>

<h3> Feature Requirements </h3>

In this document, the term "must" indicates a feature that is
required for passing grade, while the term "should" indicates a feature
that is expected for a grade of "A" or "B".  If you do not know what
something means, or don't know how to do it, you are encouraged to ask
and find out rather than turning in a homework that does not meet the
specifications.

<p>

Your program executable must be named <code>puny</code>.  Your program
should read in source file(s) named on the command line and
write output with one line for each token, described below.
Source files
must accept the extension <code>.py</code>. The compiler should
automatically add
<code>.py</code> to the end of filenames if no other extension is given.
(Eventually in a later homework, the compiler will automatically name the
executable the same name as the first argument.
For this assignment there is no output executable.)

<p>

Compilers and related tools are used by programs such as <code>make(1)</code>
that read the process exit status to tell whether all is well. Your
program's exit status should return 0 if there are no errors, and a nonzero
number to indicate errors. For lexical errors, return 1.

<h3> Language Details </h3>

The <code>PunY</code> language is (not) described (yet) at
<A href="http://www.cs.nmt.edu/~jeffery/courses/423/punyref.html">
http://www.cs.nmt.edu/~jeffery/courses/423/punyref.html</A>. As this
is a new language this semester, these details will be filled in
and corrected and amended as needed in response to student questions.

<h3> Starting Points </h3>

<ul>
<li> You must write a <A href="punylex.l">punylex.l</A> that matches the
     tokens of the Python language.
<li> you will need to make up a set of integer codes for yylex() to return;

<li> you will need to read Python references and get precise definitions
     of the literal constants, including escape characters in strings, etc.

<li>
     In C this is done as a set of #define's or an enum, in a .h file.
<li>
     Classic yacc used the name y.tab.h for this file; can be named otherwise.

<li> Indent/dedent tokens will require special treatment, as will tabs.

<li> A full Python lexer would have to account for UTF-8 multi-byte characters.
     I guess for our subset of PunY I can live without.

<li> For further notes,
     see discussion in section <code>yylex()</code> and <code>main()</code>,
     below.
</ul>

<h3> Python Lex Tokens </h3>

These are from a text file in the Python implementation but are not in Flex
format, and for some of them we will have to ascertain precise definitions.

<pre>
ENDMARKER
NAME
NUMBER
STRING
NEWLINE
INDENT
DEDENT

LPAR                    '('
RPAR                    ')'
LSQB                    '['
RSQB                    ']'
COLON                   ':'
COMMA                   ','
SEMI                    ';'
PLUS                    '+'
MINUS                   '-'
STAR                    '*'
SLASH                   '/'
VBAR                    '|'
AMPER                   '&'
LESS                    '<'
GREATER                 '>'
EQUAL                   '='
DOT                     '.'
PERCENT                 '%'
LBRACE                  '{'
RBRACE                  '}'
EQEQUAL                 '=='
NOTEQUAL                '!='
LESSEQUAL               '<='
GREATEREQUAL            '>='
TILDE                   '~'
CIRCUMFLEX              '^'
LEFTSHIFT               '<<'
RIGHTSHIFT              '>>'
DOUBLESTAR              '**'
PLUSEQUAL               '+='
MINEQUAL                '-='
STAREQUAL               '*='
SLASHEQUAL              '/='
PERCENTEQUAL            '%='
AMPEREQUAL              '&='
VBAREQUAL               '|='
CIRCUMFLEXEQUAL         '^='
LEFTSHIFTEQUAL          '<<='
RIGHTSHIFTEQUAL         '>>='
DOUBLESTAREQUAL         '**='
DOUBLESLASH             '//'
DOUBLESLASHEQUAL        '//='
AT                      '@'
ATEQUAL                 '@='
RARROW                  '->'
ELLIPSIS                '...'
COLONEQUAL              ':='

OP
AWAIT
ASYNC
TYPE_IGNORE
TYPE_COMMENT
ERRORTOKEN

# These aren't used by the C tokenizer but are needed for tokenize.py
COMMENT
NL
ENCODING
</pre>

<h3> "Fixing" the Literal Constants </h3>

We will give various examples of regular expressions for literal constants
in lecture, but your mission is to get the literal constants for Python as
correct as you can manage.

<ul>
<li> If some legal Python token is supposed to be in PunY,
     add or correct regexes for it to punylex.l
<li> If some legal Python token is <b>not</b> supposed to be in PunY, have
     your lexical analyzer report a lexical error and stop execution.
<li> Place a special focus on literal constants (strings, integers, reals...)
<li> Catch lexical errors related to literal constants and report them
     (with filename and line number)
     and stop, instead of just returning bogus output.
<li> Examples: what does your lexical analyzer do with
<pre>
     'hello
     /* world
     12e
</pre>
</ul>

<h3> Lexical Attributes </h3>

In your <code>yylex()</code>, you should compute attributes for each token,
and store them in a global variable named <code>yytoken</code>. Note that
this is not part of the lex/yacc public interface, although it is named so
as to be a recognizable extension of said interface.  You should use the
following token type, or a compatible extension of it.

<p>

<pre>
struct token {
   int category;   /* the integer code returned by yylex */
   char *text;     /* the actual string (lexeme) matched */
   int lineno;     /* the line number on which the token occurs */
   char *filename; /* the source file in which the token occurs */
   int ival;       /* for integer constants, store binary value here */
   double dval;	   /* for real constants, store binary value here */
   char *sval;     /* for string constants, malloc space, de-escape, store */
                   /*    the string (less quotes and after escapes) here */
   }
</pre>

<p>

In this homework your <code>main()</code> procedure should 
build a LINKED LIST of all the token structs, each of which is created by
<code>yylex()</code>.  In the next assignment, we will discard the linked
list and instead insert all these tokens into a tree.<p>

Example linked list structure:

<pre>
   struct tokenlist {
      struct token *t;
      struct tokenlist *next;
      }
</pre>

Use the <code>malloc()</code> function to allocate chunks of memory for
<code>struct token</code> and <code> struct tokenlist</code>.

<h3> <code>yylex()</code> and <code>main()</code> </h3>

Your <code>yylex()</code> should return a different unique integer &gt; 257
for each reserved word, and for each other token category (identifier,
integer literal constant, string literal constant, addition operator, etc).
Numbers &gt; 257 are required for the sake of compatibility with the
parser generator tool.  For each such number, you must <code>#define</code>
a symbol, as in

<pre>
#define IDENTIFIER 260
</pre>

This is required for the sake of readability.  Your <code>yylex()</code>
should return -1 when it hits end of file.  In this homework, your
<code>yylex()</code> should recognize lines beginning with # and treat them
as comments, i.e. delete the line contents silently.  In later homework,
treatment of preprocessor directives will become more interesting.

<p>

In this assignment, there should be (at least) two separately-compiled .c
files, a .h file and a makefile. The <code>yylex()</code> function must be
called by a <code>main()</code> function in a loop.  For each token, the
<code>main()</code> function should
write out a line containing the token category (an integer
&gt; 257) and lexical attributes.

<h3> Turn in... </h3>

An electronic copy via Canvas.  The electronic copy should be a
compressed archive .zip file, containing makefile, flex punylex.l file, main.c
file, and ytab.h file.  </pre>

If you add any other source files to your program, be sure you add it/them
to the makefile rules and .zip containing the set of files that you turn in.

<h3> Example </h3>

<p>

For an example input file named hello.py that contains:
<p>
<table border>
<tr><td>
<pre>
      print('hello,\tpuny!\n')
</pre>
</table>

<p>

your output should look something like the following. Integer categories
are for illustration purposes; <em>your integer codes may be different</em>.

<p>

<table border>
<tr><td>
<pre>
Category	Text		Lineno		Filename	Ival/Sval
-------------------------------------------------------------------------
271		print		3		hello.py
290		(		3		hello.py
272		'Hello\tpuny!\n' 3		hello.py	Hello	puny!

291		)		3		hello.py
</pre>
</table>
