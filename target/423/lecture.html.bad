<H1> Dr. J's Compiler and Translator Design Lecture Notes </h1>

(C) Copyright 2011-2017 by Clinton Jeffery and/or original authors where
appropriate.  For use in Dr. J's Compiler classes only.  Lots of material
in these notes originated with Saumya Debray's Compiler course notes from
the University of Arizona, for which I owe him a debt of thanks.  Various
portions of his notes were in turn inspired by the ASU red dragon book.

<table>
<tr align=top>
<td>
<ul>
<li> <A href="intro">Introduction</A>
<ul>
<li> <A href="#1">Lecture 1</A> (<A href="445-1.pptx">pptx</A>)
</ul>
<li> <A href="#lexical">Lexical Analysis</A>
<ul>
<li> <A href="#2">Lecture 2</A> (<A href="445-2.pptx">pptx</A>)
<li> <A href="#3">Lecture 3</A> (<A href="445-3.pptx">pptx</A>)
<li> <A href="#4">Lecture 4</A>
<li> <A href="#5">Lecture 5</A> (<A href="445-5.pptx">pptx</A>)
<li> <A href="#6">Lecture 6</A>
<li> <A href="#7">Lecture 7</A> (<A href="445-7.pptx">pptx</A>)
</ul>
</ul>
<ul>
<li> <A href="#syntax">Syntax Analysis</A>
<ul>
<li> <A href="#8">Lecture 8</A>
<li> <A href="#9">Lecture 9</A>
<li> <A href="#10">Lecture 10</A>
<li> <A href="#11">Lecture 11</A>
<li> <A href="#12">Lecture 12</A>
<li> <A href="#13">Lecture 13</A>
<li> <A href="#14">Lecture 14</A>
<li> <A href="#15">Lecture 15</A>
<li> <A href="#16">Lecture 16</A>
<li> <A href="#17">Lecture 17</A>
</ul>
</ul>
</td>
<td align=top>
<ul>
<li> <A href="#semantic">Semantic Analysis</A>
<ul>
<li> <A href="#18">Lecture 18</A>
<li> <A href="#19">Lecture 19</A>
<li> <A href="#20">Lecture 20</A>
<li> <A href="#21">Lecture 21</A>
<li> <A href="#22">Lecture 22</A>
<li> <A href="#23">Lecture 23</A>
<li> <A href="#24">Lecture 24</A>
<li> <A href="#25">Lecture 25</A>
<li> <A href="#26">Lecture 26</A>
<li> <A href="#27">Lecture 27</A>
<li> <A href="#28">Lecture 28</A>
<li> <A href="#29">Lecture 29</A>
</ul>
</ul>

<ul>
<li> Midterm Review
<ul>
<li> <A href="#30">Lecture 30</A>
</ul>
</ul>

<td>

<ul>
<li> <A href="#codegen">Intermediate Code Generation</A>
<ul>
<li> <A href="#31">Lecture 31</A>
<li> <A href="#32">Lecture 32</A>
<li> <A href="#33">Lecture 33</A>
<li> <A href="#34">Lecture 34</A>
<li> <A href="#35">Lecture 35</A>
<li> <A href="#36">Lecture 36</A>
<li> <A href="#37">Lecture 37</A>
<li> <A href="#38">Lecture 38</A>
<li> <A href="#39">Lecture 39</A>
<li> <A href="#40">Lecture 40</A>
<li> <A href="#41">Lecture 41</A>
<li> <A href="#42">Lecture 42</A>
<li> <A href="#43">Lecture 43</A>
<li> <A href="#44">Lecture 44</A>
<li> <A href="#45">Lecture 45</A>
</ul>
</td>
<td align=top>
<li> <A href="#finalcode">Final Code Generation</A>
<ul>
<li> <A href="#46">Lecture 46</A>
<li> <A href="#47">Lecture 47</A>
<li> <A href="#48">Lecture 48</A>
<li> <A href="#49">Lecture 49</A>
<li> <A href="#50">Lecture 50</A>
<li> <A href="#51">Lecture 51</A>
</ul>
<li> <A href="#optimization">Optimization</A>
<ul>

<li> <A href="#52">Lecture 52</A>
<li> <A href="#53">Lecture 53</A>
<li> <A href="#54">Lecture 54</A>
<li> <A href="#55">Lecture 55</A>
<li> <A href="#56">Lecture 56</A>

</ul>
</td>
</tr>
</table>

<p>
<font size=1> <A name=1>lecture #1</A> began here</font>
<p>

<A name="intro">
<h3>Why study compilers?</h3>
</a>

Computer scientists study compiler construction for the
following reasons:

<ul>
<li> Experience with large-scale
applications development. Your compiler may be the largest
program you write as a student.  Experience working with really big
data structures and complex interactions between algorithms will
help you out on your next big programming project.

<li> A shining triumph of CS theory.
It demonstrates the value of theory over the impulse to just "hack up"
a solution.

<li> A basic element of programming language research.
Many language researchers write compilers for the languages they design.

<li> Many applications have similar properties to one or more phases of
a compiler, and compiler expertise and tools can help an application
programmer working on other projects besides compilers.

</ul>

CS 445 is labor intensive. This is a good thing: there is no way to
learn the skills necessary for writing big programs without this kind
of labor-intensive experience.

<h3> Some Tools we will use </h3>

Labs and lectures will discuss all of these, but if you do not know them
already, the sooner you go learn them, the better.

<dl>
<dt> C and "make".
<dd> If you are not expert with these yet, you will be a lot closer
     by the time you pass this class.
<dt> lex and yacc
<dd> These are compiler-writers tools, but they are useful for other
     kinds of applications, almost anything with a complex file format
     to read in can benefit from them.
<dt> gdb
<dd> If you do not know a source-level debugger well, start learning.
      You will need one to survive this class.
<dt> e-mail
<dd> Regularly e-mailing your instructor is a crucial part of class
     participation.  If you aren't asking questions, you aren't doing
     your job as a student.
<dt> web
<dd> This is where you get your lecture notes, homeworks, and labs,
     and turnin all your work.
</dl>

<h3> Compilers - What Are They and What Kinds of Compilers are Out There? </h3>

The purpose of a compiler is: to translate a program in some language (the
<i>source language</i>) into a lower-level language (the <I>target
language</i>).  The compiler itself is written in some language, called
the <i>implementation language</i>.  To write a compiler you have to be
very good at programming in the implementation language, and have to
think about and understand the source language and target language.<p>

There are several major kinds of compilers:

<dl>
<dt> Native Code Compiler
<dd> Translates source code into hardware (assembly or machine code)
     instructions.  Example: gcc.

<dt> Virtual Machine Compiler
<dd> Translates source code into an abstract machine code, for execution
     by a virtual machine interpreter.  Example: javac.

<dt> JIT Compiler
<dd> Translates virtual machine code to native code.  Operates within
     a virtual machine. Example: Sun's HotSpot java machine.

<dt> Preprocessor
<dd> Translates source code into simpler or slightly lower level source code,
     for compilation by another compiler.  Examples: cpp, m4.

<dt> Pure interpreter
<dd> Executes source code on the fly, without generating machine code.
     Example: Lisp.
</dl>

OK, so a pure interpreter is not really a compiler.  Here are some more tools,
by way of review, that compiler people might be directly concerned with, even
if they are not themselves compilers.
You should learn any of these terms that you don't already know.

<dl>
<dt> assembler
<dd> a translator from human readable (ASCII text) files of machine
instructions into the actual binary code (object files) of a machine.
<dt> linker
<dd> a program that combines (multiple) object files to make an executable.
     Converts names of variables and functions to numbers (machine addresses).
<dt> loader
<dd> Program to load code.  On some systems, different executables start at
     different base addresses, so the loader must patch the executable with
     the actual base address of the executable.
<dt> preprocessor
<dd> Program that processes the source code before the compiler sees it.
     Usually, it implements macro expansion, but it can do much more.
<dt> editor
<dd> Editors may operate on plain text, or they may be wired into the rest
     of the compiler, highlighting syntax errors as you go, or allowing
     you to insert or delete entire syntax constructs at a time.
<dt> debugger
<dd> Program to help you see what's going on when your program runs.
     Can print the values of variables, show what procedure called what
     procedure to get where you are, run up to a particular line, run
     until a particular variable gets a special value, etc.
<dt> profiler
<dd> Program to help you see where your program is spending its time, so
     you can tell where you need to speed it up.
</dl>



<h3> Phases of a Compiler </h3>

<dl>
<dt>Lexical Analysis:</dt>
<dd>Converts a sequence of characters into words, or <I>tokens</i></dd>
<dt>Syntax Analysis:</dt>
<dd>Converts a sequence of tokens into a <I>parse tree</i></dd>
<dt>Semantic Analysis:</dt>
<dd>Manipulates parse tree to verify symbol and type information</dd>
<dt>Intermediate Code Generation:</dt>
<dd>Converts parse tree into a sequence of intermediate code instructions</dd>
<dt>Optimization:</dt>
<dd>Manipulates intermediate code to produce a more efficient program</dd>
<dt>Final Code Generation:</dt>
<dd>Translates intermediate code into final (machine/assembly) code</dd>
</dl>


<H3> Example of the Compilation Process </h3>

Consider the example statement; its translation to machine code
illustrates some of the issues involved in compiling.
<table border><tr><td><pre>
position = initial + rate * 60
</pre></table>
30 or so characters, from a single line of source code, are first
transformed by lexical analysis into a sequence of 7 tokens.  Those
tokens are then used to build a tree of height 4 during syntax analysis.
Semantic analysis may transform the tree into one of height 5, that
includes a type conversion necessary for real addition on an integer
operand.  Intermediate code generation uses a simple traversal
algorithm to linearize the tree back into
a sequence of machine-independent three-address-code instructions.
<p>
<table border><tr><td><pre>

  t1 = inttoreal(60)&nbsp;&nbsp;
  t2 = id<sub>3</sub> * t1
  t3 = id<sub>2</sub> + t2
  id<sub>1</sub> = t3</pre></table>

<p>
Optimization of the intermediate code allows the four instructions to
be reduced to two machine-independent instructions.  Final code generation
might implement these two instructions using 5 machine instructions, in
which the actual registers and addressing modes of the CPU are utilized.
<p>

<table border><tr><td><pre>

  MOVF	id<sub>3</sub>, R2&nbsp;&nbsp;
  MULF	#60.0, R2
  MOVF	id<sub>2</sub>, R1
  ADDF	R2, R1
  MOVF	R1, id<sub>1</sub>
</pre>
</tr>
</table>


<h4> Reading! </h4>

Read Sections 3-5 of the Flex manual,
<A href="http://westes.github.io/flex/manual/">Lexical Analysis With Flex</A>.
<p>

Also please make sure you read the class lecture notes and the related
chapters/sections of the course text. Please ask questions about
whatever is not totally clear.  You can <em>Ask Questions</em> in class or
via e-mail.

<p>

Note: although the whole course's lecture notes are ALL available to you up
front, I generally revise each lecture's notes, making additions,
corrections and adaptations to this year's homeworks, the night before each
lecture.  The best time to print hard copies of the lecture notes is one
day at a time, right before the lecture is given.  Or just read online.
<p>

<p>
<A name="lexical">
<font size=1> <A name=2>lecture #2</A> began here</font>
</A>
<p>

<h3> Reading Assignment </h3>

Read the Louden text chapters 1-2.  Except you may SKIP the parts that
describe the TINY project. Within the Scanning chapter, there are large
portions on the finite automata that should be CS 385 review; you may
SKIM that material, unless you don't know it or don't remember it.

<h3> Overview of Lexical Analysis </h3>

A lexical analyzer, also called a <em>scanner</em>, typically has the
following functionality and characteristics.

<ul>

<li> Its primary function is to convert from a (often very long) sequence of
characters into a (much shorter, perhaps 10X shorter) sequence of tokens.
This means less work for subsequent phases of the compiler.

<li> The scanner must Identify and Categorize specific character sequences
into tokens.  It must know whether every two adjacent characters in the file
belong together in the same token, or whether the second character must be
in a different token.

<li> Most lexical analyzers discard comments &amp; whitespace. In most
languages these characters serve to separate tokens from each other, but
once lexical analysis is completed they serve no purpose.  On the other
hand, the exact line # and/or column # may be useful in reporting errors,
so some record of what whitespace has occurred may be retained.  <em>Note:</em>
in some languages, even popular ones, whitespace is significant.

<li> Handle lexical errors (illegal characters, malformed tokens) by
reporting them intelligibly to the user.

<li> Efficiency is crucial; a scanner may perform elaborate input buffering

<li> Token categories can be (precisely, formally) specified using regular
expressions, e.g.
<pre>
	 IDENTIFIER=[a-zA-Z][a-zA-Z0-9]*
</pre>

<li> Lexical Analyzers can be written by hand, or implemented automatically
using finite automata.
</ul>

<h3> What is a "token" ?</h3>

In compilers, a "token" is:

<ol>
<li> a single word of source code input (a.k.a. "lexeme")
<li> an integer code that refers to a single word of input
<li> a set of lexical attributes computed from a single word of input
</ol>

Programmers think about all this in terms of #1. Syntax checking uses
#2. Error reporting, semantic analysis, and code generation require #3.  In
a compiler written in C, for each token you allocate a C struct to store (3)
for each token.

<h4> Auxiliary data structures </h4>

You were presented with the phases of the compiler, from lexical and syntax
analysis, through semantic analysis, and intermediate and final code 
generation.  Each phase has an input and an output to the next phase.
But there are a few data structures
we will build that  survive across multiple phases: the literal table,
the symbol table, and the error handler.

<dl>
<dt> lexeme table
<dd> a table that stores lexeme values, such as strings and variable
     names, that may occur in many places.  Only one copy of each
     unique string and name needs to be allocated in memory.
<dt> symbol table
<dd> a table that stores the names defined (and visible with) each
     particular scope.  Scopes include: global, and procedure (local).
     More advanced languages have more scopes such as class (or record)
     and package.
<dt> error handler
<dd> errors in lexical, syntax, or semantic analysis all need a common
     reporting mechanism, that shows where the error occurred (filename,
     line number, and maybe column number are useful).
</dl>

<h4> Reading Named Files in C using <code>stdio</code> </h4>

In this class you are opening and reading files.  Hopefully this is review
for you; if not, you will need to learn it quickly.  To do any "standard
I/O" file processing, you start by including the header:

<pre>
#include &lt;stdio.h&gt;
</pre>
This defines a data type <code>(FILE *)</code> and gives prototypes for
relevant functions.  The following code opens a file using a string filename,
reads the first character (into an int variable, not a char, so that it can
detect end-of-file; EOF is not a legal char value).
<pre>
   FILE *f = fopen(filename, "r");
   int i = fgetc(f);
   if (i == EOF) /* empty file... */
</pre>


<h3> Command line argument handling and file processing in C </h3>
The following example is from Kernighan & Ritchie's "The C Programming
Language", page 162.
<A name="sample.c">
<pre>
#include &lt;stdio.h&gt;

/* cat: concatenate files, version 1 */
int main(int argc, char *argv[])
{
   FILE *fp;
   void filecopy(FILE *, FILE *);

   if (argc == 1)
      filecopy(stdin, stdout);
   else
      while (--argc > 0)
         if ((fp = fopen(*++argv, "r")) == NULL) {
            printf("cat: can't open %s\n", *argv);
            return 1;
            }
         else {
            filecopy(fp, stdout);
            fclose(fp);
            }
   return 0;
}

void filecopy(FILE *ifp, FILE *ofp)
{
   int c;

   while ((c = getc(ifp)) != EOF)
      putc(c, ofp);
}
</pre>

<em> Warning: while using and adapting the above code is fair game in this
class, the yylex() function is very different than the filecopy() function!
It takes no parameters!  It returns an integer every time it finds a token!
So if you "borrow" from this example, delete filecopy() and write yylex()
from scratch.  Multiple students have fallen into this trap before you.
</em>

<h3> A Brief Introduction to Make </h3>

It is not a good idea to write a large program like a compiler as a single
source file.  For one thing, every time you make a small change, you would
need to recompile the whole program, which will end up being many thousands
of lines.  For another thing, parts of your compiler may be generated by
"compiler construction tools" which will write separate files.  In any case,
this class will require you to use multiple source files, compiled
separately, and linked together to form your executable program. This
would be a pain, except we have "make" which takes care of it for us.

Make uses an input file named "makefile", which stores in ASCII text form
a collection of rules for how to build a program from its pieces.  Each
rule shows how to build a file from its source files, or dependencies.
For example, to compile a file under C:
<pre>
foo.o : foo.c
	gcc -c foo.c
</pre>
The first line says to build foo.o you need foo.c, and the second line,
which <em><b>must</b></em> being with a tab, gave a command-line to
execute whenever foo.o should be rebuilt, i.e. when it is missing or
when foo.c has been changed and need to be recompiled.
<p>
The first rule in the makefile is what "make" builds by default, but
note that make dependencies are recursive: before it checks whether
it needs to rebuild foo.o from foo.c it will check whether foo.c needs
to be rebuilt using some other rule.  Because of this post-order
traversal of the "dependency graph", the first rule in your makefile
is usually the last one that executes when you type "make".  For a
C program, the first rule in your makefile would usually be the
"link" step that assembles objects files into an executable as in:
<pre>
compiler: foo.o bar.o baz.o
	gcc -o compiler foo.o bar.o baz.o
</pre>

There is a lot more to "make" but we will take it one step at a time.
<p>

You can read or skim the
<A href="https://www.gnu.org/software/make/manual/make.pdf">GNU make
manual</A>, particularly section 2, to learn more about make.

<!--
This
<A href="http://developers.sun.com/solaris/articles/make_utility.html">
article on Make</A>
may be useful to you.
-->
You can find useful on-line documentation
on "make" (manual page, Internet reference guides, etc) if you look.


<h3> A couple finer points for <A href="hw1.html">HW#1</A> </h3>

<dl>
<dt> extern vs. #include: when do you use the one, when the other?
<dd> extern's can be done without an #include, to tell one module
    about global variables defined in another module. But if you are
    going to share that extern with a lot of different modules, put
    it in an #include.
    Use #include in order to share types, externs, function prototypes,
    and symbolic #define's across multiple files. That is all. No code.
<dt> public interface to yylex(): no, you can't add your own parameters
<dd> You might be tempted to return a token structure pointer, or
     add some parameters to tell it what filename it is reading from.
     But you can't.  Leave yylex()'s interface alone, the parser will
     call it with its current interface.
</dl>

<p>
<font size=1> <A name=3>lecture #3</A> began here</font>
<p>

<h3> Regular Expressions </h3>

The notation we use to precisely capture all the variations that a given
category of token may take are called "regular expressions" (or, less
formally, "patterns".  The word "pattern" is really vague and there are
lots of other notations for patterns besides regular expressions).
Regular expressions are a shorthand notation
for sets of strings.  In order to even talk about "strings" you have
to first define an <em>alphabet</em>, the set of characters which can
appear.

<ol>
<li> Epsilon (&epsilon;) is a regular expression denoting the set
     containing the empty string
<li> Any letter in the alphabet is also a regular expression denoting
     the set containing a one-letter string consisting of that letter.
<li> For regular expressions r and s, <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r | s<br>
     is a regular expression denoting the union of r and s
<li> For regular expressions r and s, <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r s<br>
     is a regular expression denoting the set of strings consisting of
     a member of r followed by a member of s
<li> For regular expression r, <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r*<br>
     is a regular expression denoting the set of strings consisting of
     zero or more occurrences of r.
<li> You can parenthesize a regular expression to specify operator
     precedence (otherwise, alternation is like plus, concatenation
     is like times, and closure is like exponentiation)
</ol>

Although these operators are sufficient to describe all regular languages,
in practice everybody uses extensions:

<ul>
<li> For regular expression r, <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r+<br>
     is a regular expression denoting the set of strings consisting of
     one or more occurrences of r.  Equivalent to rr*
<li> For regular expression r, <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r?<br>
     is a regular expression denoting the set of strings consisting of
     zero or one occurrence of r.  Equivalent to r|&epsilon;
<li> The notation [abc] is short for a|b|c.  [a-z] is short for a|b|...|z.
     [^abc] is short for: any character other than a, b, or c.
</ul>


<h4> What is a "lexical attribute" ? </h4>

A lexical attribute is a piece of information about a token.  These typically
include:

<table>
<tr><td> category <td> an integer code used to check syntax
<tr><td> lexeme <td> actual string contents of the token
<tr><td> line, column, file <td> where the lexeme occurs in source code
<tr><td> value <td> for literals, the binary data they represent
</table>


<h4> Avoid These Common Bugs in Your Homeworks! </h4>

<ol>
<li> yytext or yyinput were not declared global
<li> main() does not have its required argc, argv parameters!
<li> main() does not call yylex() in a loop or check its return value
<li> getc() EOF handling is missing or wrong!  check EVERY all to getc() for EOF!
<li> opened files not (all) closed! file handle leak!
<li> end-of-comment code doesn't check for */
<li> yylex() is not doing the file reading
<li> yylex() does not skip multiple spaces, mishandles spaces at the front
     of input, or <em>requires</em> certain spaces in order to function OK
<li> extra or bogus output not in assignment spec
<li> = instead of ==
</ol>

<h3> Some Regular Expression Examples </h3>

Regular expressions are the preferred notation for
specifying patterns of characters that define token categories.  The best
way to get a feel for regular expressions is to see examples.  Note that
regular expressions form the basis for pattern matching in many UNIX tools
such as grep, awk, perl, etc. <p>

What is the regular expression for each of the different lexical items that
appear in C programs?  How does this compare with another, possibly simpler
programming language such as BASIC?

<!--This is the first of many things where BASIC is somewhat easier 
to deal with than C.-->

<table border>
<tr> <th> lexical category <th> BASIC <th> C </tr>
<tr> <td> operators <td> the characters themselves <td>      For operators that are regular expression operators we need mark them
     with double quotes or backslashes to indicate you mean the character,
     not the regular expression operator.  Note several operators have a
     common prefix. The lexical analyzer needs to look ahead to tell
     whether an = is an assignment, or is followed by another = for example.
 </tr>
<tr> <td> reserved words <td> the concatenation of characters; case insensitive <td>
     Reserved words are also matched by the regular expression for identifiers,
     so a disambiguating rule is needed.
</tr>

<tr>
<td> identifiers <td> no _; $ at ends of some; 2 significant letters!?; case insensitive <td> [a-zA-Z_][a-zA-Z0-9]*
</tr>

<tr>
<td> numbers <td> ints and reals, starting with [0-9]+ <td> 0x[0-9a-fA-F]+ etc.
</tr>

<tr> <td> comments <td> REM.* <td> C's comments are tricky regexp's

<tr> <td> strings <td> almost ".*"; no escapes <td> escaped quotes

<tr> <td> what else?

</table>


<!--
<h3> Language Candidates </h3>

Hypothesize that "it would be wise" for us to choose a language subset project
that is defined by somebody and NOT to make up one from scratch.  FYI: Python
does not have a widely-available YACC parser; Ruby apparently does.

<dl>
<dt> C-based: C-minus-minus
<dd> Pro: familiar.  Con: boring.  And name overloaded by multiple groups.
     Difference from Heckendorn: I will make you target x86_64 assembler.
<dt> Java-based: <A href="http://BantamJava.com"> BantamJava </A> or <A href="http://www2.cs.uidaho.edu/~jeffery/godiva/">Godiva-0</A>
<dd> Pro: Java knowledge is useful.
<dt> TRS-80 Extended Color BASIC
<dd> Pro: can run cool 80's games on it.  Con: BASIC is so last-century
<dt> Ruby-based
<dd> Pro: cool.  Con: ill-defined.  Would we just copy and study the existing
     implementation?  Blah
<dt> Plzero (Pascal language 0, I think)
<dd> Pro: I am sure I have a copy lying around that is newer than the one at
      <A href="http://www.moorecad.com/standardpascal/plzero.pas">moorecad</A>.
<dt> Unicon-based
<dd> Pro: Jeffery knows it inside and out. Con: who cares about Unicon?
</dl>
-->

<h3> <tt>lex(1)</tt> and <tt>flex(1)</tt> </h3>

These programs generally take a lexical specification given in a .l file
and create a corresponding C language lexical analyzer in a file named
lex.yy.c.  The lexical analyzer is then linked with the rest of your compiler.
<p>

The C code generated by lex has the following public interface.  Note the
use of global variables instead of parameters, and the use of the prefix
yy to distinguish scanner names from your program names.  This prefix is
also used in the YACC parser generator.
<pre>
FILE *yyin;	/* set this variable prior to calling yylex() */
int yylex();	/* call this function once for each token */
char yytext[];	/* yylex() writes the token's lexeme to an array */
                /* note: with flex, I believe extern declarations must read
                   extern char *yytext;
                 */
int yywrap();   /* called by lex when it hits end-of-file; see below */
</pre>
<p>

The .l file format consists of a mixture of lex syntax and C code fragments.
The percent sign (%) is used to signify lex elements.  The whole file is
divided into three sections separated by %%:
<pre>
   header
%%
   body
%%
   helper functions
</pre>
<p>

The header consists of C code fragments enclosed in %{ and %} as well as
macro definitions consisting of a name and a regular expression denoted
by that name.  lex macros are invoked explicitly by enclosing the
macro name in curly braces.  Following are some example lex macros.
<pre>
letter		[a-zA-Z]
digit		[0-9]
ident		{letter}({letter}|{digit})*
</pre>
<p>

The body consists of of a sequence of regular expressions for different
token categories and other lexical entities.  Each regular expression can
have a C code fragment enclosed in curly braces that executes when that
regular expression is matched.  For most of the regular expressions this
code fragment (also called a <em>semantic action</em> consists of returning
an integer that identifies the token category to the rest of the compiler,
particularly for use by the parser to check syntax.  Some typical regular
expressions and semantic actions might include:

<pre>
" "		{ /* no-op, discard whitespace */ }
{ident}		{ return IDENTIFIER; }
"*"		{ return ASTERISK; }
"."		{ return PERIOD; }
</pre>

You also need regular expressions for lexical errors such as unterminated
character constants, or illegal characters.
<p>

The helper functions in a lex file typically compute lexical attributes,
such as the actual integer or string values denoted by literals.  One
helper function you have to write is yywrap(), which is called when lex
hits end of file.  If you just want lex to quit, have yywrap() return 1.
If your yywrap() switches yyin to a different file and you want lex to continue
processing, have yywrap() return 0.  The lex or flex library (-ll or -lfl)
have default yywrap() function which return a 1, and flex has the directive
<code>%option noyywrap</code> which allows you to skip writing this function.

<p>
<font size=1> <A name=4>lecture #4</A> began here</font>
<p>

<h3> Student Feedback from Last Lecture </h3>

<dl>
<dt> Online tool for testing regular expressions
<dd> <A href="http://rubular.com/">Rubular.com</A> let's you interactively
play with Ruby regular expressions on a web page. I tried it and found
that since its regular expression language is not that of Flex,
it is not very useful for this course. If anyone finds an interactive web
regex tester for the flex language, please feel invited to share it.
<dt> Proposed regex solution
<dd> How about this (amalgamated) student proposed solution:
<pre>
"/*"(("*"+[^/*])|[^*]+)*"*"+"/"
</pre>
Correct or not?  If incorrect, give example where it fails.
Can it match <code>/* this **/ regex here*/</code>

<dt> Is the "struct token" supposed to be in our main(), and we use yylex()
along with other variables within lex.yy.c to fill the "struct token" with
the required information?

<dd> Great question. At least a pointer to a struct token should probably be
in main().  yylex() should allocate a struct token, fill it, and make it
visible to main(), probably by assigning its address to some global pointer
variable.  main() should build the linked list in a loop, calling yylex() each
time through the loop. It should then print the output by looping through
the linked list.

<dt> Discussion of include stacks. Thank you to the student contributor of
this example:
<dd>
<pre>
Here's the code I'm using to push/pop the lexer state.
It uses a couple of user-defined functions and a struct
to store auxiliary information such as the filename,
but the rest are standard C/flex functions.

/*
 * Return 1 if done, 0 if yyin points at more input
 */
int yywrap() {
    // Is the state stack empty?
    if(include_stack == NULL) {
        return 1;
    } else {
        fclose(yyin);

        // Grab the previous input file from the state stack.
        yypop_buffer_state();
        SourceFileState state = pop_file_state(&include_stack);
        filename = state.filename;
        line_num = state.line_num;
        return 0;
    }
}

/*
 * Handles a "user" include directive (using double quotes)
 */
static void handle_user_include() {
    char *fname = strchr(yytext, '\"')+1;
    fname[strlen(fname)-1] = '\0';
    fname = strdup(fname);

    FILE* input_file = fopen(fname, "r");
    if(!input_file) {
        fprintf(stderr, "Unable to open include file %s: ", fname);
        perror("");
        exit(1);
    }

    // Push flex's internal buffer state.
    yypush_buffer_state(yy_create_buffer(input_file, YY_BUF_SIZE));
    // Push "auxiliary" file data.
    push_file_state(&include_stack, filename, line_num);
    filename = fname;
    line_num = 1;
}
</pre>

<A href="http://westes.github.io/flex/manual/Multiple-Input-Buffers.html#Multiple-Input-Buffers">Section 11 of the Flex manual</A> discusses functions such as
yypush_buffer_state().
</dl>

<h3> A Short Comment on Lexing C Reals </h3>

C float and double constants have to have at least one digit, either
before or after the required decimal.  This is a pain:
<pre>
([0-9]+.[0-9]* | [0-9]*.[0-9]+) ...
</pre>

You might almost be happier if you wrote

<pre>
([0-9]*.[0-9]*)    { return (strcmp(yytext,".")) ? REAL : PERIOD; }
</pre>

You-all know C's ternary e1 ? e2 : e3 operator, don't ya? Its an if-then-else
expression, very slick.

<h3> Lex extended regular expressions </h3>

Lex further extends the regular expressions with several helpful operators.
Lex's regular expressions include:

<dl>
<dt> c
<dd> normal characters mean themselves
<dt> \c
<dd> backslash escapes remove the meaning from most operator characters.
     Inside character sets and quotes, backslash performs C-style escapes.
<dt> "s"
<dd> Double quotes mean to match the C string given as itself.
     This is particularly useful for multi-byte operators and may be
     more readable than using backslash multiple times.
<dt> [s]
<dd> This character set operator matches any one character among those in s.
<dt> [^s]
<dd> A negated-set matches any one character not among those in s.
<dt> .
<dd> The dot operator matches any one character except newline: [^\n]
<dt> r*
<dd> match r 0 or more times.
<dt> r+
<dd> match r 1 or more times.
<dt> r?
<dd> match r 0 or 1 time.
<dt> r{m,n}
<dd> match r between m and n times.
<dt> r<sub>1</sub>r<sub>2</sub>
<dd> concatenation. match r<sub>1</sub> followed by r<sub>2</sub>
<dt> r<sub>1</sub>|r<sub>2</sub>
<dd> alternation. match r<sub>1</sub> or r<sub>2</sub>
<dt> (r)
<dd> parentheses specify precedence but do not match anything
<dt> r<sub>1</sub>/r<sub>2</sub>
<dd> lookahead.  match r<sub>1</sub> when r<sub>2</sub> follows, without
     consuming r<sub>2</sub>
<dt> ^r
<dd> match r only when it occurs at the beginning of a line
<dt> r$
<dd> match r only when it occurs at the end of a line
</dl>





<!--
<h3> Introductory Comments on BASIC </h3>

We are doing (a large subset of) TRS-80 Color Computer Extended BASIC.
Compared with last semester, you will get more help from me on getting
started with each assignment, and be asked to do a little more.

<li>  The main language reference manuals are on our class web page
<A href="http://www2.cs.uidaho.edu/~jeffery/courses/445/">
http://www2.cs.uidaho.edu/~jeffery/courses/445/</A>

<li> I am writing my own commentary on it 
<A href="http://www2.cs.uidaho.edu/~jeffery/courses/445/basic.html">
http://www2.cs.uidaho.edu/~jeffery/courses/445/basic.html</A>
where most of your formal specifications will appear.

<li> I have made a start at some pseudocode
<A href="http://www2.cs.uidaho.edu/~jeffery/courses/445/basic.icn">
http://www2.cs.uidaho.edu/~jeffery/courses/445/basic.icn</A>
for the "interpreter loop" that will wrap around your compiler.
-->


<h3> Lexical Attributes and Token Objects </h3>

Besides the token's category, the rest of the compiler may need several
pieces of information about a token in order to perform semantic analysis,
code generation, and error handling.  These are stored in an object instance
of class Token, or in C, a struct.  The fields are generally something like:

<pre>
struct token {
   int category;
   char *text;
   int linenumber;
   int column;
   char *filename;
   union literal value;
}
</pre>

The union literal will hold computed values of integers, real numbers, and
strings.  <em>In your homework assignment, I am requiring you to compute
column #'s; not all compilers require them, but they are easy.  Also: in
our compiler project we are not worrying about optimizing our use of memory,
so am not requiring you to use a union</em>.

<h3> Flex Manpage Examplefest </h3>

To read a UNIX "man page", or manual page, you type "man <em>command</em>" 
where command is the UNIX program or library function you need information
on.  Read the man page for man to learn more advanced uses ("man man").
<p>

It turns out the flex man page is intended to be pretty complete, enough
so that we can draw our examples from it. Perhaps what you should figure
out from these examples is that flex is actually... flexible.  The first
several examples use flex as a filter from standard input to standard
output.


<UL>
<li> sneaky string removal tool:
<pre>
           %%
           "zap me"
</pre>
<li> excess whitespace trimmer
<pre>
           %%
           [ \t]+        putchar( ' ' );
           [ \t]+$       /* ignore this token */
</pre>
<li> sneaky string substitution tool:

<pre>
           %%
           username    printf( "%s", getlogin() );
</pre>
</ul>


<ul>
<li> Line Counter/Word Counter

<pre>
                   int num_lines = 0, num_chars = 0;

           %%
           \n      ++num_lines; ++num_chars;
           .       ++num_chars;

           %%
           main()
                   {
                   yylex();
                   printf( "# of lines = %d, # of chars = %d\n",
                           num_lines, num_chars );
                   }
</pre>


<li> Toy compiler example

<pre>
           /* scanner for a toy Pascal-like language */

           %{
           /* need this for the call to atof() below */
           #include &lt;math.h&gt;
           %}

           DIGIT    [0-9]
           ID       [a-z][a-z0-9]*

           %%

           {DIGIT}+    {
                       printf( "An integer: %s (%d)\n", yytext,
                               atoi( yytext ) );
                       }

           {DIGIT}+"."{DIGIT}*        {
                       printf( "A float: %s (%g)\n", yytext,
                               atof( yytext ) );
                       }

           if|then|begin|end|procedure|function        {
                       printf( "A keyword: %s\n", yytext );
                       }

           {ID}        printf( "An identifier: %s\n", yytext );

           "+"|"-"|"*"|"/"   printf( "An operator: %s\n", yytext );

           "{"[^}\n]*"}"     /* eat up one-line comments */

           [ \t\n]+          /* eat up whitespace */

           .           printf( "Unrecognized character: %s\n", yytext );

           %%

           main( argc, argv )
           int argc;
           char **argv;
               {
               ++argv, --argc;  /* skip over program name */
               if ( argc &gt; 0 )
                       yyin = fopen( argv[0], "r" );
               else
                       yyin = stdin;

               yylex();
               }
</pre>

</ul>


<h3> On the use of character sets (square brackets) in lex and similar tools </h3>

A student recently sent me an example regular expression for comments that read:
<pre>
   COMMENT [/*][[^*/]*[*]*]]*[*/]
</pre>
One problem here is that square brackets are not parentheses, they do not nest,
they do not support concatenation or other regular expression operators. They
mean exactly: "match any one of these characters" or for ^: "match any one
character that is not one of these characters".  Note also that you
<em>can't</em> use ^ as a "not" operator outside of square brackets: you
can't write the expression for "stuff that isn't */" by saying (^ "*/")

<p>
<font size=1> <A name=5>lecture #5</A> began here</font>
<p>


<h3> Mailbag </h3>

<dl>
<dt> My tokens' "text" field in my linked list are all messed up when I go
     back through the list at the end. What do I do?
<dd> Remember to make a physical copy of yytext each token, because it
      overwrites itself each time it matches a regular expression in yylex().
     Typically a physical copy of a C string is made using strdup(),
     which is equivalent to a malloc() followed by strcpy().

<dt> C++ concatenates adjacent string literals, e.g. "Hello" " world"
Does our lexer need to do that?
<dd>
This feature is not used in Soule's CS 120 text. You do not have to do it.
It could be done in the lexer, in the parser, or sneakily in-between.
Can you think of a
way to get the job done without too much pain? Be careful to consider 3+
adjacent string literals
("Hello" " world" "how are you" and so on)
<dt> How do I handle escapes in svals? Do I need to worry about more than
\n \t \\ and \r?
<dd>
You replace the two-or-more characters with a single, encoded character.
'\\' followed by 'n' become a control-J character.  120++ needs
\n \t \' \\ \" and \0 -- these are the ones that appear in the text.
You can do additional ones like \r but they are not required.
</dl>


<A name=finiteautomata>
<h3> Finite Automata </h3>
</A>

Efficiency in lexical analyzers based on regular expressions is all about
how to best implement those wonders of CS 385: the finite automata. Today
we briefly review some highlights from theory of computation with an eye
towards implementation.  Maybe I will accidentally describe something
differently than how you heard it before.

<p>

A finite automaton (FA) is an abstract, mathematical machine, also known as a
finite state machine, with the following components:

<ol>
<li> A set of states S
<li> A set of input symbols E (the alphabet)
<li> A transition function move(state, symbol) : new state(s)
<li> A start state S0
<li> A set of final states F
</ol>

The word <em>finite</em> refers to the set of states: there is a fixed size
to this machine.  No "stacks", no "virtual memory", just a known number of
states.  The word <em>automaton</em> refers to the execution mode: there is
no brain, not so much as a instruction set and sequence of instructions.
The entire logic is just a hardwired short loop that executes the same
instruction over and over:

<pre>
   while ((c=getchar()) != EOF) S := move(S, c);
</pre>

What this "finite automaton algorithm" lacks in flexibility,
it makes up in speed.

<!--
<h3> <A href="http://www2.cs.uidaho.edu/~jeffery/courses/445/hw1/">Sample Solution for Homework #1</A> </h3>-->

<h3> DFAs </h3>

The type of finite automata that is easiest to understand and simplest to
implement <!--(maybe even in hardware)--> is called a deterministic finite
automaton (DFA).  The word <em>deterministic</em> here refers to the return
value of
function move(state, symbol), which goes to at most one state.

Example:
<p>
<pre>
S = {s0, s1, s2}
E = {a, b, c}
move = { (s0,a):s1; (s1,b):s2; (s2,c):s2 }
S0 = s0
F = {s2}
</pre>
<p>

Finite automata correspond in a 1:1 relationship to transition diagrams;
from any transition diagram one can write down the formal automaton in
terms of items #1-#5 above, and vice versa.  To draw the transition diagram
for a finite automaton:
<ul>
<li> draw a circle for each state s in S; put a label inside the circles
     to identify each state by number or name
<li> draw an arrow between S<sub>i</sub> and S<sub>j</sub>, labeled with x
     whenever the transition says to move(S<sub>i</sub>, x) : S<sub>j</sub>
<li> draw a "wedgie" into the start state S0 to identify it
<li> draw a second circle inside each of the final states in F
</ul>

<h3> The Automaton Game </h3>

If I give you a transition diagram of a finite automaton, you can hand-simulate
the operation of that automaton on any input I give you.


<h4> DFA Implementation </h4>

The nice part about DFA's is that they are efficiently implemented
on computers.  What DFA does the following code correspond to?  What
is the corresponding regular expression?  You can speed this code
fragment up even further if you are willing to use goto's or write
it in assembler.

<pre>
state := S0
for(;;)
   switch (state) {
   case 0: 
      switch (input) {
         'a': state = 1; input = getchar(); break;
         'b': input = getchar(); break;
	 default: printf("dfa error\n"); exit(1);
         }
   case 1: 
      switch (input) {
         EOF: printf("accept\n"); exit(0);
	 default: printf("dfa error\n"); exit(1);
         }
      }
</pre>

Flex has extra complications. It accepts multiple regular expressions, runs
them all in parallel in one big DFA, and adds semantics to break ties. These
extra complications might be viewed as "breaking" the strict rules of DFA's,
but they don't really mess up the fast DFA implementation.

<h4> Deterministic Finite Automata Examples </h4>

A lexical analyzer might associate different final states with different
token categories. In this fragment, the final states are marked by
"return" statements that say what category to return. What is incomplete
or wrong here?
<p>


<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/dfacat.png">
<p>


C Comments:<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/dfa-ccom.png">
<p>



<h3> C Comments Redux </h3>

<ul>
<li>
It takes less than 5 minutes to find a solution on the internet, but many
of them are in fact suboptimal.
<li> A FA might or might not give solution hints for the corresponding regex.
<li> Here is an internet solution, cleaned up:

<pre>
"/*"([^*]|"*"+[^/*])*"*"+"/"
</pre>
<li> Think hard about it. Does it have bugs?
</ul>


<h4> Nondeterministic Finite Automata (NFA's)</h4>

Notational convenience motivates more flexible machines in which function
move() can go to more than one state on a given input symbol, and some
states can move to other states even without consuming an input symbol
(&epsilon;-transitions).
<p>

Fortunately, one can prove that for any NFA, there is an equivalent DFA.
They are just a notational convenience. So, finite automata help us get
from a set of regular expressions to a computer program that recognizes
them efficiently.

<h4> NFA Examples </h4>

&epsilon;-transitions make it simpler to merge automata:<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-eps.gif">
<p>

multiple transitions on the same symbol handle common prefixes:<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-less.gif">
<p>

factoring may optimize the number of states.  Is this picture OK/correct?<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-less2.gif">
<p>


<h4> C Pointers, malloc, and your future </h4>

For most of you success as a computer scientist may boil down to whether you can
master the concept of dynamically allocated memory.  In C this means pointers and
the malloc() family of functions.  Here are some tips:

<ul>
<li> Draw "memory box" pictures of your variables.  Pencil and paper
     understanding of memory leads to correct running programs.
<li> Always initialize local pointer variables.  Consider this code:
<pre>
void f() {
   int i = 0;
   struct tokenlist *current, *head;
   ...
   foo(current)
}
</pre>

Here, <code>current</code> is passed in as a parameter to foo, but it is a
pointer that hasn't been pointed at anything. I cannot tell you how many
times I personally have written bugs myself or fixed bugs in student code,
caused by reading or writing to pointers that weren't pointing at anything
in particular.  Local variables that weren't initialized point at random
garbage. If you are lucky this is a coredump, but you might not be lucky,
you might not find out where the mistake was, you might just get a wrong answer.
This can all be fixed by

<pre>
   struct tokenlist *current = NULL, *head = NULL;
</pre>

<LI> Avoid this common C bug:
<pre>
struct token *t = (struct token *)malloc(sizeof(struct token *)));
</pre>
This compiles, but causes coredumps during program execution.  Why?

<li> Check your malloc() return value to be sure it is not NULL.
Sure, modern programs will "never run out of memory".  Wrong. malloc() can
return NULL even on big machines.  Operating systems often place limits on
memory far beneath the hardware capabilities.  wormulon is a conspicuous
example.
</ul>


<h4> NFA examples - from regular expressions </h4>

Can you draw an NFA corresponding to the following?

<pre>
(a|c)*b(a|c)*

(a|c)*|(a|c)*b(a|c)*

(a|c)*(b|&epsilon;)(a|c)*
</pre>




<h3> Regular expressions can be converted automatically to NFA's </h3>

Each rule in the definition of regular expressions has a corresponding
NFA; NFA's are <i>composed</i> using &epsilon; transitions.  This is called
"Thompson's construction" <!--(Louden pg. 64, ASU Algorithm 3.3-->).
We will work
examples such as (a|b)*abb in class and during lab.

<ol>
<li> For &epsilon;, draw two states with a single &epsilon; transition.
     <br><img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-th2.gif">
<li> For any letter in the alphabet, 
     draw two states with a single transition labeled with that letter.
     <br><img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-th1.gif">
<li> For regular expressions r and s, draw r | s
     by adding a new start state with &epsilon; transitions to the start
     states of r and s, and a new final state with &epsilon; transitions
     from each final state in r and s.
     <br><img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-th4.gif">
<li> For regular expressions r and s, draw rs
     by adding &epsilon; transitions from the final states of r to the
     start state of s.
     <br><img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-th3.gif">
<li> For regular expression r, draw r*
     by adding new start and final states, and &epsilon; transitions
<ul>
	<li> from the start state to the final state,
	<li> from the final  state back to the start state,
	<li> from the new start to the old start and from the old final
            states to the new final state.
</ul>
     <br><img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/fa-th5.gif">
<li> For parenthesized regular expression (r) you can use the NFA for r.
</ol>

<p>
<font size=1> <A name=6>lecture #6</A> began here</font>
<p>


<h3> Mailbag </h3>

<dl>
<dt> Will you always call "make" on our submissions?
<dd> Yes. I expect you to use make and provide a makefile in each
     homework. Turn in the whole source, not just "changed" or
     "new" files for some assignments. My script will
     unpack your .zip file by saying "unzip" in some new test directory
     and then run "make" and then run your executable named 120++. If
     anything goes wrong (say, you unzipping into a subdirectory the script
     does not know the name of) you will lose a few points.
<br><br>
     On the other hand, I do not want the tool-generated files
     (lex.yy.c, cgram.tab.c) or .o or executables.  The makefile should
     contain correct dependencies to rerun flex (and later, bison) and
     generate these files whenever source (.l, .y , etc.) files are changed.
<dt> The O'Reilly book recommended using Flex states instead of that big
     regular expression for C comments.  Is that reasonable?
<dd> Yes, you may implement the most elegant correct answer you can
     devise, not just what you see in class.
<dt> Are we free to explore non-optimal solutions?
<dd> I do not want to read lots of extra pages of junk code, but you are free
     to explore alternatives and submit the most elegant solution you come
     up with, regardless of its optimality. Note that there are some parts
     of the implementation that I might mandate. For example, the symbol table
     is best done as a hash table. You could use some other fancy data
     structure that you love, but if you give me a linked list I will be
     disappointed. Then again, a working linked list implementation would get
     more points than a failed complicated implementation.
<dt> Is it OK to allocate a token structure inside main() after yylex()
     returns the token?

<dd> No. In the next phase of your compiler, you will not call yylex(), the
Bison-generated parser will call yylex().  There is a way for
the parser to grab your token if you've stored it in a global variable,
but there is not a way for the parser to build the token structure itself.

</dl>


<h3> NFA's can be converted automatically to DFA's </h3>

In: NFA N<br>
Out: DFA D<br>
Method: Construct transition table Dtran (a.k.a. the "move function").
Each DFA state is a set of
NFA states. Dtran simulates in parallel all possible moves N can make
on a given string.
<p>

Operations to keep track of sets of NFA states:
<p>
<dl>
<dt>&epsilon;_closure(s)</dt>
<dd> set of states reachable from state s via &epsilon;</dd>
<dt>&epsilon;_closure(T)</dt>
<dd> set of states reachable from any state in set T via &epsilon;</dd>
<dt>move(T,a)</dt>
<dd> set of states to which there is an NFA transition from states in T on symbol a</dd>
</dl>
<p>

NFA to DFA Algorithm:
<p>
<pre>
Dstates := {&epsilon;_closure(start_state)}
while T := unmarked_member(Dstates) do {
	mark(T)
	for each input symbol a do {
		U := &epsilon;_closure(move(T,a))
		if not member(Dstates, U) then
			insert(Dstates, U)
		Dtran[T,a] := U
	}
}
</pre>

<h3> Practice converting NFA to DFA </h3>

OK, you've seen the algorithm, now can you use it?<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/nfadfa.gif">
<p>
...
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
...did you get:<p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/nfadfa2.gif">
<p>
<br>
<br>
<br>
<br>
OK, how about this one: <p>
<img src="http://www2.cs.uidaho.edu/~jeffery/courses/445/nfadfa3.gif">
<p>


<!--
<h2> Some Remarks (BASIC)</h2>

<ul>
<li> Color BASIC has two-letter identifiers.  We are doing Extended Color
     BASIC, so we will allow longer identifier names.  But in the current
     assignment #2 you will only be tested on short variable names.
<li> Whether we return the same or a different category for integer constants
     and for line numbers depends very much on the grammar we use to parse
     our language,
     and whether it can understand what's going on with the
     line numbers if it doesn't have two categories.  My instinct tells me
     we will probably need either a token for newline characters or will
     need to detect line numbers as distinct from normal integer constants.
     Before we can answer that question, we need to learn about syntax!
</ul>
-->


<h3> Lexical Analysis and the Literal Table </h3>

In many compilers, the memory management components of the compiler interact
with several phases of compilation, starting with lexical analysis.

<p>

<ul>
<li> Efficient storage is necessary to handle large input files.
<li> There is a colossal amount of duplication in lexical data:
     variable names, strings and other literal values duplicate frequently
<li> What token type to use may depend on previous declarations.
</ul>
<p>
A hash table or other efficient data structure can avoid this duplication.
The software engineering design pattern to use is called the "flyweight".


<h3> Literal Table: Usage Example </h3>

Example abbreviated from [ASU86]: Figure 3.18, p. 109.  Use "install_id()"
instead of "strdup()" to avoid duplication in the lexical data.

<pre>
%{
/* #define's for token categories LT, LE, etc.
%}

digit   [0-9]
id	[a-zA-Z_][a-zA-Z_0-9]*
num     {digit}+(\.{digit}+)?

%%

[ \t\n]+ { /* discard */ }
if	 { return IF; }
then	 { return THEN; }
else	 { return ELSE; }
{id}	 { yylval.id = install_id(); return ID; }
{num}    { yylval.num = install_num(); return NUMBER; }
"&lt;"	 { yylval.op = LT; return RELOP; }
"&gt;"	 { yylval.op = GT; return RELOP; }

%%

install_id()
{
   /* insert yytext into the literal table */
}

install_num()
{
   /* insert (binary number corresponding to?) yytext into the literal table */
}
</pre>

So how would you implement a literal table using a hash table?  We will see
more hash tables when it comes time to construct the symbol tables with which
variable names and scopes are managed, so you had better become fluent.

<p>
<font size=1> <A name=7>lecture #7</A> began here</font>
<p>
<h3>Mailbag</h3>

<dl>

<dt> You mentioned that the next homework assignment, we won't be calling
yylex() from main (which is why you previously mentioned
you cannot allocate token structure in main() ). I have followed that rule,
but I question how will linked lists be set up in Homework #2 then?
<dd> In HW#2 the linked list will be subsumed by you building a tree data
structure. If you built a linked list inside yylex(), that may be harmless
and left in place. If you malloc'ed the token structs inside yylex() but
built the linked list in your main(), your linked list can just go away in
HW#2.

<dt> Can you test my scanner and see if I get an "A"?
<dd> No.
<dt> Can you post tests so I can see if my scanner gets an "A"?
<dd> See <A href="120pp/examp.html">these 120++ sample files</A>.
     If you share additional tests that you devise, for example
     when you have questions, I will add them to this collection
     for use by the class.
<dt> So if I run OK on these files, do I get an "A"?
<dd> Maybe.
     Dr. Soule's CS 120 textbook ("A Project Based Introduction to C++")
     is available in electronic or paper form and any C++ program in it,
     or that only uses features it describes, is fair game.
     You should devise "coverage tests" to hit all described features.

<dt>
Each time a regular <em>user-written</em> include file is "handled",
we must stop the current yyin file (and save it), and open this new
file and assigning it yyin, and read from the user-defined include
file. Once we finish with the include file, do we have to go back
(somehow) to where yylex was reading from the original yyin?

<dd> Yes. Excellent question because it points out that
switching between files is more than just switching saving and restoring
the global FILE * yyin variable, which (if left open) would retain its
cursor position: flex seems to do its own layer of buffering in addition to
the buffering built-in to C stdio files.
Note that the sample clex.l file omits the saving and restoring of flex's
buffer state, and naively tries to just assign a new yyin and go on.
<p>

Section 11 of the flex manual provides the tools to correct and complete
(or replace entirely) the include mechanism started in clex.l.  Function
<code>yypush_buffer_state()</code>, or function
<code>yy_switch_to_buffer()</code>, allows for switching back (either by
popping, or switching again) after an include file is completed.

<dt>
From reading the Homework #1 page, I can't seem to understand how
you want <em>system includes</em> to be handled. I'm still not really sure
what you want the 120++ lexical anyalzyer to do for system includes,
could you provide some more info?
<dd>
I dismiss the notion of including g++'s real system includes, because they
are too large and liable to have non-standard extensions.
You have two options: 1) set a global boolean flag for each
system include, from the small set that are allowed in 120++. Use those
flags later in your compiler to hardwire "built-in" globals/functions
into your global symbol table.
OR 2) implement the system includes by having the system includes pull in
a set of tiny "120++ include files" that you write for your compiler. For
portability, these should be located either relative to your compiler's
binary location, or built-in such that your compiler can place them in
the current directory as temporary files during the compile.

<dt>
When displaying the token per line, does that mean once we encounter a
user-defined include, it should begin displaying token info for its tokens?

<dd> Yes, and then switch back to the main source file after the #include has
been processed.
</dl>

<h3> Major Data Structures in a Compiler </h3>

<dl>
<dt> token
<dd> contains an integer category, lexeme, line #, column #, filename...
     We could build these into a link list, but instead we'll use them
     as leaves in a tree structure.
<dt> syntax tree
<dd> contains grammar information about a sequence of related tokens.
     leaves contain lexical information (tokens).  internal nodes
     contain grammar rules and pointers to tokens or other tree nodes.
<dt> symbol table
<dd> contains variable names, types, and information needed to generate
     code for a name (such as its address, or constant value).  Look ups
     are by name, so we'll need a hash table.
<dt> intermediate &amp; final code
<dd> We'll need link lists or similar structures to hold sequences of machine
     instructions
</dl>



<h4> Quick Note on things to look for in HW </h4>

<ul>
<li> Adding reserved words is trivial. But clex.l was for C, and we
     need some of the C++ reserved words for 120++.
<li> Also look for: any new data types (besides bool) and their new
     types of literal constants?
     New literals require new nontrivial regular expressions in your
     lex file.
<li> If there are bugs in the clex.l file you were given, the
ones we would be most likely to care about are bugs in the regular expressions
for literal constants.  This calls for close scrutiny and painstaking attention
to detail...
</ul>


<A name="syntax"><h2> Syntax Analysis </h2></A>

<em>Parsing</em> is the act of performing syntax analysis to verify an input
program's compliance with the source language.  A by-product of this process
is typically a tree that represents the structure of the program.


<h3> Context Free Grammars </h3>

A context free grammar G has:

<ul>
<li> A set of terminal symbols, T
<li> A set of nonterminal symbols, N
<li> A start symbol, s, which is a member of N
<li> A set of production rules of the form A -> &omega;,
     where A is a nonterminal and &omega; is a string of terminal and 
     nonterminal symbols.
</ul>

A context free grammar can be used to <em>generate</em> strings in the
corresponding language as follows:
<pre>let X = the start symbol s
while there is some nonterminal Y in X do
   apply any one production rule using Y, e.g. Y -> &omega;
</pre>
When X consists only of terminal symbols, it is a string of the language
denoted by the grammar.  Each iteration of the loop is a
<em>derivation step</em>.  If an iteration has several nonterminals
to choose from at some point, the rules of derviation would allow any of these
to be applied.  In practice, parsing algorithms tend to always choose the
leftmost nonterminal, or the rightmost nonterminal, resulting in strings
that are <em>leftmost derivations</em> or <em>rightmost derivations</em>.

<h3> Context Free Grammar Examples </h3>

Well, OK, so how much of the <!--BASIC--> C language grammar can we come up
with in class today?  Start with expressions, work on up to statements, and
work there up to entire functions, and programs.


<h3> Context Free Grammar Example (from BASIC) </h3>

How many terminals and non-terminals does the grammar below use?
Compared to the little grammar we started last time, how does this rate?
What parts make sense, and what parts seem bogus?

<pre>
Program : Lines
Lines   : Lines Line
Lines   : Line
Line    : INTEGER StatementList
StatementList : Statement COLON StatementList
StatementList : Statement
Statement: AssignmentStatement
Statement: IfStatement
<em> REMark: ... BASIC has many other statement types </em>

AssignmentStatement : Variable ASSIGN Expression
Variable : IDENTIFIER
<em> REMark: ... BASIC has at least one more Variable type: arrays </em>

IfStatement: IF BooleanExpression THEN Statement
IfStatement: IF BooleanExpression THEN Statement ELSE Statement

Expression: Expression PLUS Term
Expression: Term
Term      : Term TIMES Factor
Term      : Factor
Factor    : IDENTIFIER
Factor    : LEFTPAREN Expression RIGHTPAREN
<em> REMark: ... BASIC has more expressions </em>
</pre>

<p>
<font size=1> <A name=8>lecture #8</A> began here</font>
<p>

No class on Monday; September 4, 2017 is Labor Day.

<h3>Mailbag </h3>

<dl>
<dt>
Are we required to be using a lexical analysis error function lexerr()?
<dd>
<ul>
<li>
Whether you have a helper function with that particular name is up to you.
<li> You should report lexical errors in a manner that is helpful to the user.
Include line #, filename, and nature of the error if possible.
<li>Many lexical errors will consist of "C++ token X is not legal in 120++".
<li>You are allowed to stop with an error exit status when you find an error.
</ul>

<dt>
The HW1 Specification says we are to use at least 2 separately compiled
.c files. does lex.yy.c count as one of them,
or are you looking for yet another .c file, aside from lex.yy.c? 

<dd>
lex.yy.c counts. You may have more, but you should at least have a lex.yy.c
or other lex-compatible module, and a main function in a separate .c file

<dt> The 120++ manual's lexical section does not include short/long int
and double types as it does in section 1, from Dr. Soule's book. Are we not
including these?
<dd> We are including short/long reserved words, they are in the 120++ book.
Subject to semantics requirements of ANSI C++, we might simplify our code
generation and implement all sizes as the same
thing (say, a 64-bit integer).

<dt> In reference to including libraries such as string or cstime, should be just use a flag to determine if these have been added, such as we did for iostream?
<dd> I recommend one flag for each supported include.  Technically, you do not
have to work out all the details in HW#1, since it will mainly start to get
used in the semantic analysis phase.

<dt> With those includes in mind, should we process functions included in those libraries differently? Such as cin.ignore() or rand(). 
<dd> These items are processed normally in the lexical and syntax analysis
phases*.  By the time we do semantic analysis, we will need a strategy for
pre-initializing the symbol table based on include-flags.   *Except:
typedef's, classes, and predefined type names may affect our syntax analysis.

<dt>
With the string library being included, do we not need to worry about char* types being present?
<dd> char * does appear in 120++. We will need to support basic pointer types.
More details will be needed in semantic analysis.  char  and * are two separate
tokens in your scanner of course.

<dt>
Do we need to include the referencing operator &amp;
<dd>
Yes.

<dt>
Is the distinction between and bitwise AND and referencing operation done by our grammar? 
<dd> Yes, the lexical analyzer just returns a token saying it saw an &amp;
and the syntax analyzer has to decide if that is used as a binary operator,
a unary modifer to a parameter, a unary address-of operator, etc.

</dl>


<h3> A brief aside on casting your mallocs </h3>

<li> If you don't put a prototype for malloc(), C thinks it returns an int.
<pre>
#include &lt;stdlib.h&gt;
</pre>
includes prototypes for malloc(), free(), etc.  malloc() returns a void *.
<br><br>

<li> void * means "pointer that points at nothing", or "pointer that points
     at anything".  You need to cast it to what you are really pointing at,
     as in:
<pre>
union lexval *l = (union lexval *)malloc(sizeof(union lexval));
</pre>
Note the stupid duplication of type information; no language is perfect!
Anyhow, always cast your mallocs.  The program may work without the cast,
but you need to fix every warning, so you don't accidentally let a serious
one through.<br><br>


<h3> Grammar Ambiguity </h3>

The grammar

<pre>
E -> E + E
E -> E * E
E -> ( E )
E -> ident
</pre>

allows two different derivations for strings such as "x + y * z".
The grammar is ambiguous, but the semantics of the language dictate
a particular operator precedence that should be used.  One way to
eliminate such ambiguity is to rewrite the grammar. For example,
we can force the precedence we want by adding some nonterminals and
production rules.

<pre>
E -> E + T
E -> T
T -> T * F
T -> F
F -> ( E )
F -> ident
</pre>

Given the arithmetic expression grammar from last lecture:
<p>

How can a program figure that x + y * z is legal?<br>
How can a program figure out that x + y (* z) is illegal?
<p>





<h3> Recursive Descent Parsing </h3>

Perhaps the simplest parsing method, for a large subset of context free
grammars, is called recursive descent.  It is simple because the algorithm
closely follows the production rules of nonterminal symbols.

<ul>
<li> Write 1 procedure per nonterminal rule
<li> Within each procedure, a) match terminals at appropriate positions,
     and b) call procedures for non-terminals.
<li> Pitfalls:
<ol><li> left recursion is FATAL
<li> must distinguish between several
     production rules, or potentially, one has to
     try all of them via <em>backtracking</em>.
</ol>
</ul>


<h4> Recursive Descent Parsing Example #1</h4>

Consider the grammar we gave above.  There will be functions for
E, T, and F.  The function for F() is the "easiest" in some sense: based
on a single token it can decide which production rule to use.  The
parsing functions return 0 (failed to parse) if the nonterminal in
question cannot be derived from the tokens at the current point.
A nonzero return value of N would indicate success in parsing using
production rule #N.

<pre>
int F()
{
   int t = yylex();
   if (t == IDENT) return 6;
   else if (t == LP) {
      if (E() && (yylex()==RP) return 5;
      }
   return 0;
}
</pre>

Comment #1: if F() is in the middle of a larger parse of E() or T(), F()
may succeed, but the subsequent parsing may fail. The parse may have
to <em>backtrack</em>, which would mean we'd have to be able to put
tokens back for later parsing.  Add a memory (say, a gigantic array or
link list for example) of already-parsed tokens
to the lexical analyzer, plus backtracking logic to E() or T() as needed.
The call to F() may get repeated following a different production rule
for a higher nonterminal.
<p>

Comment #2: in a real compiler we need more than "yes it parsed" or
"no it didn't": we need a parse tree if it succeeds, and we need a
useful error message if it didn't.
<p>

Question: for E() and T(), how do we know which production rule to try?
Option A: just blindly try each one in turn.
Option B: look at the first (current) token, only try those rules that
start with that token (1 character lookahead).  If you are lucky, that
one character will uniquely select a production rule. If that is always
true through the whole grammar, no backtracking is needed.
<p>


Question: how do we know which rules start with whatever token we are
looking at?  Can anyone suggest a solution, or are we stuck?
<p>

Below is an industrious start of an implementation of the
corresponding recursive descent parser for non-terminal <code>T</code>.
Now is student-author time, what is our next step?  What is wrong with
this picture?


<pre>
int T()
{
   if (T() && (yylex()==ASTERISK) && F()) return 3;
   ... more to be filled in, like rule 4...
}
</pre>





<h3> Removing Left Recursion </h3>


<pre>
E -> E + T | T
T -> T * F | F
F -> ( E ) | ident
</pre>

We can remove the left recursion by introducing new nonterminals
and new production rules.

<pre>
E  -> T E'
E' -> + T E' | &epsilon;
T  -> F T'
T' -> * F T' | &epsilon;
F  -> ( E ) | ident
</pre>

Getting rid of such <em>immediate left recursion</em> is not enough, one must
get rid of indirect left recursion, where two or more nonterminals are
mutually left-recursive.
One can rewrite <em>any</em> CFG to remove left recursion (Algorithm 4.19). 

<pre>
for i := 1 to n do
   for j := 1 to i-1 do begin
      replace each A<sub>i</sub> -&gt; A<sub>j</sub> &gamma; with productions
         A<sub>i</sub> -&gt; &delta;<sub>1</sub>&gamma; | &delta;<sub>2</sub>&gamma; | ... | &delta;<sub>k</sub>&gamma;, where
            A<sub>j</sub> -&gt; &delta;<sub>1</sub> | &delta;<sub>2</sub> | ... | &delta;<sub>k</sub> are all current A<sub>j</sub>-productions
      end
   eliminate immediate left recursion
</pre>

<p>
<font size=1> <A name=9>lecture #9</A> began here</font>
<p>

<h3> Mailbag </h3>

The theme of today's exciting mailbag is: for HW#1, just worry about
lexical aspects. Leave syntax and semantics for when they are assigned.

<dl>
<dt>I'm still not super clear what should happen with system includes.
They will eventually be stored in a symbol table for Bison. The symbol 
table should be a hash table. But what is being stored there?
Should I look up every variable and function name in the 6 libraries 
mentioned by 120++, and manually add them all to the symbol table?
<dd> By Homework #2, we will have to know what identifiers are the names
of types (classes) in order to return the correct category for them.
By homework #3 we will want to store type information for these system
functions and parameters and such, and be able to check calls for validity.

<dt>
120++ mentions "built-in" classes or functions -- <<, >>, cout, 
cin, string, ifstream, etc. What does that mean in terms of the lexer?
<dd> The lexer should include operators (separate integer categories/codes)
     for all C++ operators.  The lexer should treat built-ins that are not
     reserved words as identifiers.  We will need to revisit this topic
     next assignment.

<dt> I was curious if character literals (i.e. 'a' or 'b') need to be stored as an ival or maybe sval?

<dd> Yes. Easiest would be to treat them similar to a string and store an
sval that is only one character long.

<dt> Are we supposed to distinguish between pointers and multiplication for our scanner? They use the same symbol, but are we supposed to know when it's considered a pointer and when it's considered multiplication?

<dd> No. The lexical analyzer doesn't know the surrounding syntax context
that would say what the token is used for, it just knows it sees a *
</dl>


<h3> Where We Are </h3>

<ul>
<li> We started in on recursive descent parsing by observing that for some
grammar rules, we could just write the code easy peasy by  matching
the first token and then calling nonterminal functions.
<li> Then we hit a wall, because the other nonterminals were left recursive,
we had to solve the infinite recursion problem, which is detailed in your
dragon book.
<li> If we ever clear the left recursion hurdle, THEN we can worry about the
backtracking problem: if we try to parse rule 1, and get into it a ways, and
find that it doesn't work, we have to "undo" all our parsing (and possibly
lexing) back to the starting point in order to try subsequent grammar rules
for a given nonterminal.
<li> We missed a question back there.
</ul>


<h3> Removing Left Recursion, part 2 </h3>

Left recursion can be broken into three cases
<p>
<h4>case 1: trivial</h4>

<pre>
A : A &alpha; | &beta;
</pre>

The recursion must always terminate by A finally deriving &beta; so you
can rewrite it to the equivalent
<pre>
A : &beta; A'
A' : &alpha; A' | &epsilon;
</pre>

Example:
<pre>
E : E op T | T
</pre>
can be rewritten
<pre>
E : T E'
E' : op T E' | &epsilon;
</pre>

<h4>case 2: non-trivial, but immediate</h4>

In the more general case, there may be multiple recursive productions
and/or multiple non-recursive productions.
<pre>
A : A &alpha;<sub>1</sub> | A &alpha;<sub>2</sub> | ... | &beta;<sub>1</sub> | &beta;<sub>2</sub>
</pre>

As in the trivial case, you get rid of left-recursing A and introduce an A'

<pre>
A :  &beta;<sub>1</sub> A' | &beta;<sub>2</sub> A' | ...
A' : &alpha;<sub>1</sub> A' | &alpha;<sub>2</sub> A' | ... | &epsilon;
</pre>


<h4> case 3: mutual recursion </h4>

<ol>
<li> Order the nonterminals in some order 1 to N.
<li> Rewrite production rules to eliminate all
     nonterminals in leftmost positions that refer to a "previous" nonterminal.
     When finished, all productions' right hand symbols start with a terminal
     or a nonterminal that is numbered equal or higher than the nonterminal
     no the left hand side.
<li> Eliminate the direct left recusion as per cases 1-2.
</ol>


<h4> Left Recursion Versus Right Recursion: When does it Matter? </h4>

A student came to me once with what they described as an operator precedence
problem where 5-4+3 was computing the wrong value (-2 instead of 4).  What
it really was, was an associativity problem due to the grammar:
<pre>
E : T + E | T - E | T
</pre>

The problem here is that right recursion is forcing right associativity, but
normal arithmetic requires left associativity.  Several solutions are:
(a) rewrite the grammar to be left recursive, or (b) rewrite the grammar
with more nonterminals to force the correct precedence/associativity,
or (c) if using YACC or Bison, there are "cheat codes" we will discuss later
to allow it to be majorly ambiguous and specify associativity separately
(look for %left and %right in YACC manuals).


<h3> Recursive Descent Parsing Example #2</h3>

The grammar

<pre>
S -> A B C
A -> a A
A -> <em>&epsilon;</em>
B -> b
C -> c
</pre>

maps to pseudocode like the following. (:= is an assignment operator)

<pre>
procedure S()
  if A() & B() & C() then succeed # matched S, we win
end

procedure A()
  if yychar == a then { # use production 2
     yychar := scan()
     return A()
     }
  else
     succeed # production rule 3, match &epsilon;
end

procedure B()
   if yychar == b then {
      yychar := scan()
      succeed
      }
   else fail
end

procedure C()
   if yychar == c then {
      yychar := scan()
      succeed
      }
   else fail
end
</pre>

<h3> Backtracking? </h3>


Could your current token begin more than one of your possible production rules?
Try all of them, remember and reset state for each try.
<pre>
S -> cAd
A -> ab
A -> a
</pre>

<em>Left factoring</em> can often solve such problems:

<pre>
S -> cAd
A -> a A'
A'-> b
A'-> (&epsilon;)
</pre>

One can also perform left factoring <!--(Algorithm 4.2)--> to reduce or
eliminate the lookahead or backtracking needed to tell which production rule
to use.  If the end result has no lookahead or backtracking needed, the
resulting CFG can be solved by a "predictive parser" and coded easily in a
conventional language.  If backtracking is needed, a recursive descent
parser takes more work to implement, but is still feasible.

As a more concrete example:

<pre>
S -> <b>if</b> E <b>then</b> S
S -> <b>if</b> E <b>then</b> S<sub>1</sub> else S<sub>2</sub>
</pre>

can be factored to:

<pre>
S -> <b>if</b> E <b>then</b> S S'
S'-> else S<sub>2</sub> | &epsilon;
</pre>



<h3> Some More Parsing Theory </h3>

Automatic techniques for constructing parsers start with computing some
basic functions for symbols in the grammar.  These functions are useful
in understanding both recursive descent and bottom-up LR parsers.

<h3> First(a) </h3>

First(a) is the set of terminals that begin strings derived from a,
which can include &epsilon;.

<ol>
<li> First(X) starts with the empty set.
<li> if X is a terminal, First(X) is {X}.
<li> if X -> &epsilon; is a production, add &epsilon; to First(X).
<li> if X is a non-terminal and X -> Y<sub>1</sub> Y<sub>2</sub> ... Y<sub>k</sub> is a production,
     add First(Y<sub>1</sub>) to First(X).
<li><pre>for (i = 1; if Y<sub>i</sub> can derive &epsilon;; i++)
        add First(Y<sub>i+1</sub>) to First(X)
</pre>
</ol>


<h3> First(a) examples </h3>

by the way, this stuff is all in section 4.3 in your text.
<p>
Last time we looked at an example with E, T, and F, and + and *.
The first-set computation was not too exciting and we need more
examples.

<pre>
stmt : if-stmt | OTHER
if-stmt:  IF LP expr RP stmt else-part
else-part: ELSE stmt | &epsilon;
expr: IDENT | INTLIT
</pre>

What are the First() sets of each nonterminal?

<p>
<font size=1> <A name=10>lecture #10</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
</dl>




<h3> Follow(A) </h3>

Follow(A) for nonterminal A is the set of terminals that can appear
immediately to the right of A in some sentential form S -> aAxB...
To compute Follow, apply these rules to all nonterminals in the grammar:

<ol>
<li> Add $ to Follow(S)
<li> if A -> aB&beta; then add First(b) - &epsilon; to Follow(B)
<li> if A -> aB or A -> aB&beta; where &epsilon; is in First(&beta;), then add
     Follow(A) to Follow(B).
</ol>


<h3> Last time: Follow() Example </h3>

For the grammar:

<pre>
stmt : if-stmt | OTHER
if-stmt:  IF LP expr RP stmt else-part
else-part: ELSE stmt | &epsilon;
expr: IDENT | INTLIT
</pre>

It can get pretty muddy on the Follow() function, for even this simple grammar.
It helps if you follow the algorithm, instead of just "eyeballing it".

<pre>
For all non-terminals X in the grammar do
   1. if X is the start symbol, add $ to Follow(X)
   2. if N -&gt; &alpha;X&beta; then add First(&beta;) - &epsilon; to Follow(X)
   3. if N -&gt; &alpha;X or N -&gt; &alpha;X&beta; where &epsilon; is in
       First(&beta;) then add Follow(N) to Follow(X)
</pre>

Since the algorithm depends on First(), what are First sets again?
<pre>
First(stmt) = {IF, OTHER}
First(if-stmt) = {IF}
First(else-part) = {ELSE, &epsilon;}
First(expr) = {IDENT, INTLIT}
</pre>


Because each non-terminal has three steps, and our toy grammar has
4 non-terminals, there are 12 steps.
When you just apply these twelve steps, brute force, it is clear
that the statement of what to do to compute them was not an algorithm,
it was only a declarative specification, and there is an ordering needed
in order to compute the result.
<pre>
   1. stmt is the start symbol, add $ to Follow(stmt)
   2. if N -&gt; &alpha; stmt &beta; then add First(&beta;) - &epsilon; to Follow(stmt)
	---- add First(else-part)-&epsilon; to Follow(stmt)
   3. if N -&gt; &alpha; stmt or N -&gt; &alpha; stmt &beta; where &epsilon;
	 is in First(&beta;) then add Follow(N) to Follow(stmt)
	---- add Follow(else-part) to Follow(stmt)
   4. if-stmt is not the start symbol (noop)
   5. if N -&gt; &alpha;if-stmt&beta; then add First(&beta;) - &epsilon; to Follow(if-stmt)
	---- n/a
   6. if N -&gt; &alpha;if-stmt or N -&gt; &alpha;if-stmt&beta; where &epsilon; is in
       First(&beta;) then add Follow(N) to Follow(if-stmt)
	---- add Follow(stmt) to Follow(if-stmt)
   7. else-part is not the start symbol (noop)
   8. if N -&gt; &alpha;else-part&beta; then add First(&beta;) - &epsilon; to Follow(else-part)
	---- n/a
   9. if N -&gt; &alpha;else-part or N -&gt; &alpha;else-part&beta; where &epsilon; is in
       First(&beta;) then add Follow(N) to Follow(else-part)
	--- add Follow(if-stmt) to Follow(else-part)
   10. expr is not the start symbol (noop)
   11. if N -&gt; &alpha;expr&beta; then add First(&beta;) - &epsilon; to Follow(expr)
	---- add RP to Follow(expr)
   12. if N -&gt; &alpha;expr or N -&gt; &alpha;expr&beta; where &epsilon; is in
       First(&beta;) then add Follow(N) to Follow(expr)
	---- n/a
</pre>

What is the dependency graph? Does it have any cycles?  If it has cycles,
you will have to iterate to a fixed point.
<pre>
Follow(stmt) depends on Follow(else-part)
Follow(if-stmt) depends on Follow(stmt)
Follow(else-part) depends on Follow(if-stmt)
</pre>
If I read this right, there is a 3-way mutual recursion cycle.


<h3> Can we First/Follow Anything Else </h3>

Like preferably, a real-world grammar example?  Please remember that real
world grammars for languages like ANSI C are around 400+ production rules,
so in-class examples will by necessity be toys.  If I pick a random* (*LOL)
<A href="unigram.y">YACC grammar</A>, can we First/Follow any of its non-terminals?



<h3> On resizing arrays in C </h3>

The sval attribute in homework #2 is a perfect example of a problem which a
BCS major might not be expected to manage, but a CS major should be able to
do by the time they graduate.  This is not to encourage any of you to consider
BCS, but rather, to encourage you to learn how to solve problems like these.
<p>

The problem can be summarized as: step through yytext, copying each piece
out to sval, removing doublequotes and plusses between the pieces, and
evaluating CHR$() constants.<p>

Space allocated with malloc() can be increased in size by realloc().
realloc() is awesome.  But, it COPIES and MOVES the old chunk of
space you had to the new, resized chunk of space, and frees the old
space, so you had better not have any other pointers pointing at
that space if you realloc(), and you have to update your pointer to
point at the new location realloc() returns.

<pre>
i = 0; j = 0;
while (yytext[i] != '\0') {
   if (yytext[i] == '\"') {
      /* copy string into sval */
      i++;
      while (yytext[i] != '\"') {
         sval[j++] = yytext[i++];
         }
      }
   else if ((yytext[i] == 'C') || (yytext[i] == 'c')) {
      /* handle CHR$(...) */
      i += 5;
      k = atoi(yytext + i);
      sval[j++] = k;           /* might check for 0-255 */
      while (yytext[i] != ')') i++;
      }
   /* else we can just skip it */
   i++;
}
sval[j] = '\0'; /* NUL-terminate our string */
</pre>

There is one more problem: how do we allocate memory for sval, and how big
should it be?

<ul>
<li> Solution #1: sval = malloc(strlen(yytext)+1) is very safe, but wastes
     space.
<li> Solution #2: you could malloc a small amount and grow the array as
     needed.
<pre>
sval = strdup("");
...
sval = appendstring(sval, yytext[i]); /* instead of sval[j++] = yytext[i] */
</pre>
where the function appendstring could be:
<pre>
char *appendstring(char *s, char c)
{
    i = strlen(s);
    s = realloc(s, i+2);
    s[i] = c;
    s[i+1] = '\0';
    return s;
}
</pre>
Note: it is very inefficient to grow your array one character at
a time; in real life people grow arrays in large chunks at a time.

<li> Solution #3: use solution one and then shrink your array when you
find out how big it actually needs to be.
<pre>
sval = malloc(strlen(yytext)+1);
/* ... do the code copying into sval; be sure to NUL-terminate */
sval = realloc(sval, strlen(sval)+1);
</ul>

<p>
<font size=1> <A name=11>lecture #11</A> began here</font>
<p>

<p>
<font size=1> <A name=12>lecture #12</A> began here</font>
<p>

<h3> Comments on HW 1 </h3>

These comments are based on the ~1/3 of the class that turned in printed
copy by Friday 5pm.
<UL>
<li> page counts: 5 to 25.
<li> better solutions' lexer actions looked like
<pre>
...regex...      { return token(TERMSYM); }
</pre>
<li> Use &lt;= 80 columns next time.
<li> Comment non-trivial helper functions.  Comment non-trivial code.
     Comment appropriate for a CS professional reader, not a newbie tutorial.
<li> Do not leave in excessive commented-out debugging code or whatever.
<li> Does lexer know about C++ library things like cin?  Can it report good
     errors for those if the required system include file was not included?
<li> Fancier formatting might calculate field widths from actual data
     and use a variable to specify field widths in the printf
<li> Remind yourself of the difference between NULL and '\0' and 0
<li> Avoid O(n<sup>2</sup>) or worse, if at all possible
<li> Avoid big quantities of duplicate code
<li> Use a switch when appropriate instead of long chain of if-statements
<li> On strings, allocate one byte extra for NUL.
<li>  On all pointers,
     don't allocate and then just point the pointer someplace else
     (memory leak)
<li> Don't allocate the same thing over and over unless copies may need
     to be modified.
<li> Check all allocations for NULL return (good to have a helper function)
<li> Check all fopen() calls for NULL return (good to have a helper function)
<li> Beware losing the base pointer that you allocated. You can only free()
     if you still know where the start of what you allocated was.
<li> Avoid duplicate calls to strlen(), especially in a loop!
<li> Use strcpy() instead of strncpy() unless you are really
     copying only part of a string, or
     copying a string into a limited-length buffer.
</UL>

<A name=yacc>
<h3> YACC </h3>
</A>

YACC ("yet another compiler compiler") is a popular tool which originated at
AT&T Bell Labs.  YACC takes a context free grammar as input, and generates a
parser as output.  Several independent, compatible implementations (AT&amp;T
yacc, Berkeley yacc, GNU Bison) for C exist, as well as many implementations
for other popular languages. There also exist other more "modern" parser
generators, but they are often less portable and are
heavily inspired/influenced by YACC so it is what we will study. <p>

YACC files end in .y and take the form
<pre>
declarations
%%
grammar
%%
subroutines
</pre>
The declarations section defines the terminal symbols (tokens) and
nonterminal symbols. The most useful declarations are:
<dl>
<dt> %token a
<dd> declares terminal symbol a; YACC can generate a set of #define's
that map these symbols onto integers, in a y.tab.h file. <em><b> Note: don't
#include your y.tab.h file from your grammar .y file, YACC generates the
same definitions and declarations directly in the .c file, and including
the .tab.h file will cause duplication errors.</b></em>
<dt> %start A
<dd> specifies the start symbol for the grammar (defaults to nonterminal
     on left side of the first production rule).
</dl>
<p>
The grammar gives the production rules, interspersed with program code
fragments called semantic actions that let the programmer do what's
desired when the grammar productions are reduced.  They follow the
syntax
<pre>
A : body ;
</pre>
Where body is a sequence of 0 or more terminals, nonterminals, or semantic
actions (code, in curly braces) separated by spaces.  As a notational
convenience, multiple production rules may be grouped together using the
vertical bar (|).


<h3> Bottom Up Parsing </h3>

Bottom up parsers start from the sequence of terminal symbols and work
their way back up to the start symbol by repeatedly replacing grammar
rules' right hand sides by the corresponding non-terminal.  This is
the reverse of the derivation process, and is called "reduction".
<p>

Example. For the grammar
<pre>
(1)	S->aABe
(2)	A->Abc
(3)	A->b
(4)	B->d
</pre>
the string "abbcde" can be parsed bottom-up by the following reduction
steps:
<pre>
abbcde
aAbcde
aAde
aABe
S
</pre>


<h3> Handles </h3>

Definition: a <em>handle</em> is a substring that
<ol>
<li> matches a right hand side of a production rule in the grammar and
<li> whose reduction to the nonterminal on the left hand side of that
     grammar rule is a step along the reverse of a rightmost derivation.
</ol>

<A name=shiftreduce>
<h3> Shift Reduce Parsing </h3>     
</A>

A shift-reduce parser performs its parsing using the following structure
<pre>
<u>Stack</u>					<u>Input</u>
$						&omega;$
</pre>
At each step, the parser performs one of the following actions.
<ol>
<li> Shift one symbol from the input onto the parse stack
<li> Reduce one handle on the top of the parse stack. The symbols
     from the right hand side of a grammar rule are popped off the
     stack, and the nonterminal symbol is pushed on the stack in their place.
<li> Accept is the operation performed when the start symbol is alone
     on the parse stack and the input is empty.
<li> Error actions occur when no successful parse is possible.
</ol>

<p>
<font size=1> <A name=13>lecture #13</A> began here</font>
<p>

<h3> More Comments on HW#1 </h3> 

<dl>
<dt> can't [mc]alloc() in a global initializer
<dd> malloc() is a runtime allocation from a memory region that does not
exist at compile or link time.
<dt> don't use raw constants like 260
<dd> use symbol names.
<dt> OR (vertical bar |) means nothing inside square brackets
<dd> square brackets are an implicit shortcut for a whole lot of ORs anyhow
<dt> If you didn't allocate your token inside yylex() actions...
<dd> You have to go back and do it, you need it for HW#2.
<dt> If your regex's were broken
<dd> If you know it, and were lazy, then fix it.  If you don't know it,
     then woe is you on the midterm and/or final, you need to learn these,
     and devise some (hard) tests!
<dt> A couple of pairs of solutions looked eerily similar
<dd> As in, improbably similar.
     Just a gentle reminder, talking about code OK, sharing code not OK.
     Possible exceptions for shared-with-whole-class, minor utility funcs
     that are cited
</dl>

<h3> The YACC Value Stack </h3>

<ul>
<li> YACC's parse stack contains only "states"
<li> YACC maintains a parallel set of values
<li> $ is used in semantic actions to name elements on the value stack
<li> $$ denotes the value associated with the LHS (nonterminal) symbol
<li> $n denotes the value associated with RHS symbol at position n.
<li> Value stack typically used to construct the parse tree
<li> Typical rule with semantic action: A : b C d { $$ = tree(R,3,$1,$2,$3); }
<li> The default value stack is an array of integers
<li> The value stack can hold arbitrary values in an array of unions
<li> The union type is declared with %union and is named YYSTYPE
</ul>


<h4> Getting Lex and Yacc to talk </h4>

YACC uses a global variable named yylval, of type YYSTYPE, to receive
lexical information from the scanner.  Whatever is in this variable
each time yylex() returns to the parser will get copied over to the
top of the value stack when the token is shifted onto the parse stack.
<p>

You can either declare that struct token may appear in the %union,
and put a mixture of struct node and struct token on the value stack,
or you can allocate a "leaf" tree node, and point it at your struct
token.  Or you can use a tree type that allows tokens to include
their lexical information directly in the tree nodes.  If you have
more than one %union type possible, be prepared to see type conflicts
and to declare the types of all your nonterminals. <p>

Getting all this straight takes some time; you can plan on it.  Your best
bet is to draw pictures of how you want the trees to look, and then make the
code match the pictures.  No pictures == "Dr. J will ask to see your
pictures and not be able to help if you can't describe your trees."

<h3> Declaring value stack types for terminal and nonterminal symbols </h3>

Unless you are going to use the default (integer) value stack, you will
have to declare the types of the elements on the value stack.  Actually,
you do this by declaring which
union member is to be used for each terminal and nonterminal in the
grammar.
<p>
Example: in the cocogram.y that I gave you we could add a %union declaration
with a union member named treenode:
<pre>
%union {
  nodeptr treenode;
}
</pre>

This will produce a compile error if you haven't declared a nodeptr type
using a typedef, but that is another story.  To declare that a nonterminal
uses this union member, write something like:
<pre>
%type < treenode > function_definition
</pre>

Terminal symbols use %token to perform the corresponding declaration.
If you had a second %union member (say struct token *tokenptr) you
might write:
<pre>
%token < tokenptr > SEMICOL
</pre>


<h3> Conflicts in Shift-Reduce Parsing </h3>

"Conflicts" occur when an ambiguity in the grammar creates a situation
where the parser does not know which step to perform at a given point
during parsing.  There are two kinds of conflicts that occur.

<dl>
<dt> shift-reduce
<dd> a shift reduce conflict occurs when the grammar indicates that
     different successful parses might occur with either a shift or a reduce
     at a given point during parsing.  The vast majority of situations where
     this conflict occurs can be correctly resolved by shifting.
<dt> reduce-reduce
<dd> a reduce reduce conflict occurs when the parser has two or more
     handles at the same time on the top of the stack.  Whatever choice
     the parser makes is just as likely to be wrong as not.  In this case
     it is usually best to rewrite the grammar to eliminate the conflict,
     possibly by factoring.
</dl>

Example shift reduce conflict:
<pre>
S->if E then S
S->if E then S else S
</pre>
<P>
In many languages two nested "if" statements produce a situation where
an "else" clause could legally belong to either "if".  The usual rule
(to shift) attaches the else to the nearest (i.e. inner) if statement.
<p>

Example reduce reduce conflict:
<pre>
(1)	S -> id LP plist RP
(2)	S -> E GETS E
(3)	plist -> plist, p
(4)	plist -> p
(5)	p -> id
(6)	E -> id LP elist RP
(7)	E -> id
(8)	elist -> elist, E
(9)	elist -> E
</pre>
By the point the stack holds ...id LP id<br>
the parser will not know which rule to use to reduce the id: (5) or (7).

<p>
<font size=1> <A name=14>lecture #14</A> began here</font>
<p>



<h3> Further Discussion of Reduce Reduce and Shift Reduce Conflicts </h3>

The following grammar, based loosely on our expression grammar from
last time, illustrates a reduce reduce conflict, and how you have to
exercise care when using epsilon productions.  Epsilon productions
were helpful for some of the grammar rewriting methods, such as removing
left recursion, but used indiscriminately, they can cause much trouble.

<pre>
T : F | F T2 ;
T2 : p F T2 | ;
F : l T r | v ;
</pre>

The reduce-reduce conflict occurs after you have seen an F.  If the next
symbol is a p there is no question of what to do, but if the next symbol
is the end of file, do you reduce by rule #1 or #4 ?
<p>

A slightly different grammar is needed to demonstrate a shift-reduce conflict:

<pre>
T : F g;
T : F T2 g;
T2 : t F T2 ;
T2 : ;
F : l T r ;
F : v ;
</pre>

This grammar is not much different than before, and has the same problem,
but the surrounding context (the "calling environments") of F cause the
grammar to have a shift-reduce instead of reduce-reduce.  Once again,
the trouble is after you have seen an F and dwells on the question of
whether to reduce the epsilon production, or instead to shift, upon
seeing a token g.
<p>

The .output file generated by "bison -v" explains these conflicts in
considerable detail.  Part of what you need to interpret them are the
concepts of "items" and "sets of items" discussed below.


<A name=precedence>
<h4> YACC precedence and associativity declarations </h4>
</A>

YACC headers can specify precedence and associativity rules for otherwise
heavily ambiguous grammars.  Precedence is determined by increasing order
of these declarations.  Example:

<pre>
%right ASSIGN
%left PLUS MINUS
%left TIMES DIVIDE
%right POWER
%%
expr: expr ASSIGN expr
    | expr PLUS expr
    | expr MINUS expr
    | expr TIMES expr
    | expr DIVIDE expr
    | expr POWER expr
    ;
</pre>

<A name=yyerror>
<h4> YACC error handling and recovery </h4>
</A>
<ul>
<li> Use special predefined token <code>error</code> where errors expected
<li> On an error, the parser pops states until it enters one that has an
     action on the error token.
<li> For example: statement: error ';' ;
<li> The parser must see 3 good tokens before it decides it has recovered.
<li> yyerrok tells parser to skip the 3 token recovery rule
<li> yyclearin throws away the current (error-causing?) token
<li> yyerror(s) is called when a syntax error occurs (s is the error message)
</ul>

<p>
<font size=1> <A name=15>lecture #15</A> began here</font>
<p>
<h3> Mailbag </h3>

<dl>
<dt> Why did you write that I "need to allocate token struct here"
     in my .l file
<dd> For HW#2 and beyond, the (pointer to) token struct has to be in
     a global variable named yylval by the time yylex() returns each
     token.  You can't do it in main() or someplace after yylex() has
     returned, once Bison has taken over.
<dt> Why didn't you grade our HW#1 executions?  Are you going to?
<dd> I'm providing rapid feedback based on looking at your code.
     If time allows, I will also run your HW#1 and give additional
     points and additional feedback based on that.
<dt> How are we supposed to integrate the tokens we created in the
     lexer with the tokens in the Bison .y file?
<dd> Any which way you can.  Probably, you
     either rename yours to use their names, or rename theirs to
     use your names.  Example pseudocode for your homework task:
<pre>
     1. for each terminal symbol in your .l {
        identify the corresponding terminal symbol in their .l
        replace your name with their name in your .l
	}
     2. for each terminal symbol in their .l {
        if it is in your .l due to step 1, then skip to the next
	else add it in some appropriate manner to your .l
	}
</pre>

<dt> what action should be taken in the case of epsilon statements
<dd> either $$=NULL; or $$=alctree(EPSILON, 0); (i.e. a leaf)

<dt> Would I be setting myself up for failure if I attempt to write
     my own grammar?
<dd> Go right ahead, the 120++ language is a pretty small subset of
     C++, but this is still a pretty large undertaking.

</dl>

<h3> Improving YACC's Error Reporting </h3>

yyerror(s) overrides the default error message, which usually just says either
"syntax error" or "parse error", or "stack overflow".
<p>

You can easily add information in your own yyerror() function, for example
GCC emits messages that look like:
<pre>
goof.c:1: parse error before '}' token
</pre>
using a yyerror function that looks like
<pre>
void yyerror(char *s)
{
   fprintf(stderr, "%s:%d: %s before '%s' token\n",
	   yyfilename, yylineno, s, yytext);
}
</pre>
<p>

You could instead, use the error recovery mechanism to produce better messages.
For example
<pre>
lbrace : LBRACE | { error_code=MISSING_LBRACE; } error ;
</pre>
Where LBRACE is an expected token {<br>
This uses a global variable error_code to pass parse information to yyerror().
<p>
Another related option is to call yyerror() explicitly with a better message
string, and tell the parser to recover explicitly:
<pre>
package_declaration: PACKAGE_TK error
	{ yyerror("Missing name"); yyerrok; } ;
</pre>
<p>

But, using error recovery to perform better error reporting runs against
conventional wisdom that you should use error tokens very sparingly.
What information from the parser determined we had an error in the first
place?  Can we use that information to produce a better error message?


<h3> LR Syntax Error Messages: Advanced Methods </h3>

The pieces of information that YACC/Bison use to determine that there
is an error in the first place are the parse state (yystate) and the
current input token (yychar). These are exactly the pieces of information
one might use to produce better diagnostic error messages without
relying on the error recovery mechanism and mucking up the grammar
with a lot of extra production rules that feature the <code>error</code> token.
<p>

Even just the parse state is enough to do pretty good error messages.
yystate is not part of YACC's public interface, though, so you may
have to play some tricks to pass it as a parameter into yyerror() from
yyparse().  Say, for example:
<pre>
#define yyerror(s) __yyerror(s,yystate)
</pre>

Inside __yyerror(msg, yystate) you can use a switch statement or a global
array to associate messages with specific parse states.  But, figuring
out which parse state means which syntax error message would be by trial
and error.
<p>

A tool called Merr is available that let's you generate this yyerror
function from examples: you supply the sample syntax errors and messages,
and Merr figures out which parse state integer goes with which message.
Merr also uses the yychar (current input token) to refine the diagnostics
in the event that two of your example errors occur on the same parse state.
See the <A href="http://unicon.sf.net/merr/">Merr</A> web page.


<!--
There are test cases posted on the class website.  If you create additional
test cases, turn them in (say, in a subdirectory test/ in your tar file).
Extra credit will be awarded to the best suite of test cases.-->


<h3> LR vs. LL vs. LR(0) vs. LR(1) vs. LALR(1) </h3>

The first char ("L") means input tokens are read from the left
(left to right).  The second char ("R" or "L") means parsing
finds the rightmost, or leftmost, derivation.  Relevant
if there is ambiguity in the grammar.  (0) or (1) or (k) after
the main lettering indicates how many lookahead characters are
used.  (0) means you only look at the parse stack, (1) means you
use the current token in deciding what to do, shift or reduce.
(k) means you look at the next k tokens before deciding what
to do at the current position.


<h3> LR Parsers </h3>

LR denotes a class of bottom up parsers that is capable of handling virtually
all programming language constructs.  LR is efficient; it runs in linear time
with no backtracking needed.  The class of languages handled by LR is a proper
superset of the class of languages handled by top down "predictive parsers".
LR parsing detects an error as soon as it is possible to do so.  Generally
building an LR parser is too big and complicated a job to do by hand, we use
tools to generate LR parsers.
<p>


The LR parsing algorithm is given below.<!--See Figure 4.29 for a schematic.-->
<pre>
ip = first symbol of input
repeat {
   s = state on top of parse stack
   a = *ip
   case action[s,a] of {
      SHIFT s': { push(a); push(s') }
      REDUCE A->&beta;: {
         pop 2*|&beta;| symbols; s' = new state on top
         push A
         push goto(s', A)
         }
      ACCEPT: return 0 /* success */
      ERROR: { error("syntax error", s, a); halt }
      }
   }
</pre>



<!--
<h3> Little-known Mysteries of the BASIC Language </h3>

<ul>
<li> BASIC has arrays.  They default to a size of 11 elements.
     Other array sizes are specified via a DIM statement, as in:<br>
     10 DIM A(100)
<li> BASIC has three versions of every variable name (number, string, array)
<li> Variables in BASIC are preinitialized to 0.
<li> PRINT statements have multiple arguments (implicit concatenation?)
<li> PRINT statements can have commas or semicolons between their arguments
<li> Some of our tests have syntax errors; others use features (e.g. graphics)
     that are beyond our scope. Extra credit, but only if you catch up first.
<li> cocogram.y is not infallible, you are to fix it, and then brag about it
<li> Jimenez' COCO emulator is how I test what should and should not work.
</ul>
-->

<p>
<font size=1> <A name=16>lecture #16</A> began here</font>
<p>

<h3> Midterm Exam Date Discussion </h3>

We will need to have a midterm on Oct 13, 14, 15, or 16.
Please consider the matter and be prepared to vote on it soon.

<p>

<H3>Constructing SLR Parsing Tables: </H3>

<P>
<DFN>Definition: An LR(0) item of a grammar G is a production
of G with a dot at some position of the RHS.</DFN>
<P>
Example: The production A-&gt;aAb gives the items: 
<P>
A -&gt; . a A b<br>
A -&gt; a . A b<br>
A -&gt; a A . b<br>
A -&gt; a A b .
<P>
Note: A production A-&gt; &epsilon; generates
only one item:
<P>
A -&gt; .
<P>
Intuition: an item A-&gt; &alpha; . &beta; denotes:
<OL>
<LI>&alpha; - we have already seen a string
derivable from &alpha;
<LI>&beta; - we hope to see a string derivable
from &beta;
</OL>

<H3>Functions on Sets of Items </H3>

<P>
<DFN>Closure: if I is a set of items for a grammar G, then closure(I)
is the set of items constructed as follows:</DFN>
<OL>
<LI><DFN>Every item in I is in closure(I).</DFN>
<LI><DFN>If A-&gt;</DFN>&alpha; . <DFN>B</DFN>&beta;<DFN>
is in closure(I) and B-&gt;</DFN>&gamma;<DFN>
is a production, then add B-&gt; .</DFN>&gamma;<DFN>
to closure(I).</DFN> 
</OL>

<P>
These two rules are applied repeatedly until no new items can
be added.
<P>
Intuition: If A -&gt; &alpha; . B &beta; is in
closure(I) then we hope to see a string derivable from B in the
input. So if B-&gt; &gamma; is a production,
we should hope to see a string derivable from &gamma;.
Hence, B-&gt;.&gamma; is in closure(I).<BR>

<P>
Goto: if I is a set of items and X is a grammar symbol, then goto(I,X)
is defined to be:
<P>
goto(I,X) = closure({[A-&gt;&alpha;X.&beta;] | [A-&gt;&alpha;.X&beta;]
is in I})
<P>
Intuition: 
<UL>
<LI>[A-&gt;&alpha;.X&beta;]
is in I =&gt; we've seen a string derivable
from &alpha;; we hope to see a string derivable
from X&beta;.
<LI>Now suppose we see a string derivable from X
<LI>Then, we should &quot;goto&quot; a state where we've seen
a string derivable from &alpha;X, and where
we hope to see a string derivable from &beta;.
The item corresponding to this is [A-&gt;&alpha;X.&beta;] 
</UL>


<UL>
<LI>Example: Consider the grammar
</UL>

<PRE>
<FONT SIZE=3>	E -&gt; E+T | T
	T -&gt; T*F | F
	F -&gt; (E) | id 
</font></pre>

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
Let I = {[E -&gt; E . + T]} then:
<pre><font size=3>        goto(I,+) = closure({[E -&gt; E+.T]})
		  = closure({[E -&gt; E+.T], [E -&gt; .T*F], [T -&gt; .F]})
		  = closure({[E -&gt; E+.T], [E -&gt; .T*F], [T -&gt; .F], [F-&gt; .(E)], [F -&gt; .id]})
		  = { [E -&gt; E + .T],[T -&gt; .T * F],[T -&gt; .F],[F -&gt; .(E)],[F -&gt; .id]}</FONT>
</PRE>

<H3>The Set of Sets of Items Construction</H3>

<OL>
<LI>Given a grammar G with start symbol S, construct the augmented
grammar by adding a special production S'-&gt;S where S' does
not appear in G.
<LI>Algorithm for constructing the canonical collection of
sets of LR(0) items for an augmented grammar G': 
</OL>

<P>
<FONT SIZE=3 FACE="Courier New"></FONT>
<PRE>
<TT>	begin
	   C := { closure({[S' -&gt; .S]}) };
</TT>	   <TT>repeat
	      for each set of items I in C:
		  for each grammar symbol X:
   		     if goto(I,X) != 0 and goto(I,X) is not in C then
		 	 add goto(I,X) to C;
	   until no new sets of items can be added to C;
	   return C;
	end<BR>
</TT>
</PRE>




<P>
<DFN>Valid Items: an item A -&gt; </DFN>&beta;<DFN><SUB>
1</SUB>. </DFN>&beta;<DFN> <SUB>2</SUB>
 is valid for a viable prefix </DFN>&alpha;<DFN>
</DFN>&beta;<DFN><SUB> 1  </SUB>if
there is a derivation:</DFN>
<PRE>
<FONT SIZE=3 FACE="Courier New">S' =&gt;<SUP>*</SUP><SUB>rm</SUB> </FONT>&alpha;<FONT SIZE=3 FACE="Courier New">A</FONT>&omega;<FONT SIZE=3 FACE="Courier New"> =&gt;<SUP>*</SUP><SUB>rm</SUB></FONT>&alpha; &beta;<SUB><FONT SIZE=3 FACE="Courier New">1</FONT></SUB>&beta;<SUB><FONT SIZE=3 FACE="Courier New"> 2</FONT></SUB>&omega;
</PRE>

<P>
Suppose A -&gt; &beta;<SUB>1</SUB>.&beta; <sub>2</sub> is valid for &alpha;&beta;<SUB>1</SUB>,
and &alpha;B<SUB>1</SUB> is on the parsing
stack
<OL>
<LI>if &beta;<SUB>2</SUB> != &epsilon;,
we should shift
<LI>if &beta;<SUB>2</SUB> = &epsilon;,
A -&gt; &beta;<SUB>1</SUB> is the handle,
and we should reduce by this production 
</OL>

<P>
Note: two valid items may tell us to do different things for the
same viable prefix. Some of these conflicts can be resolved using
lookahead on the input string. 

<p>
<font size=1> <A name=17>lecture #17</A> began here</font>
<p>
<h3> Mailbag </h3>

<dl>
<dt> It seems from what you said yesterday that we don't have to grow
our hash table. If this is the case, what would be a reasonable size of
the table?  n=20?
<dd> n=41. Fixed-size tables should use a prime. For the size of inputs
     I will ever manage in this class, probably a prime near 100 would do.
<dt> Are we just supporting class, enum, typedef, and namespace identifiers and <em>not</em> structs?
<dd> 120++ has to my knowledge class, typedef, and struct but not
     enum or namespaces other than std.  Furthermore, typedef and
     struct are only mentioned in passing and not used significantly.
     About the only mention of them is
<pre>
typedef struct pet{
int happy;
int hunger;
char name[100];
} pet;
pet pet1, pet2;
</pre>
  For the purposes of our class, the Best thing to do would be to
make the "type names table" record, for each name, whether it was
a struct label, a typedef name, or a class name.  However, you would
score quite well on tests if all you managed was to return CLASS_NAME
instead of IDENTIFIER for names that were declared as the names of
classes.
</dl>

<H3>Constructing an SLR Parsing Table</H3>

<OL>
<LI>Given a grammar G, construct the augmented grammar by adding
the production S' -&gt; S.
<LI>Construct C = {I<SUB>0</SUB>, I<SUB>1</SUB>, &#133; I<SUB>n</SUB>},
the set of sets of LR(0) items for G'.
<LI>State I is constructed from I<SUB>i</SUB>, with parsing action
determined as follows:
<UL>
<LI>[A -&gt; &alpha;.aB] is in
I<SUB>i</SUB>, where a is a terminal; goto(I<SUB>i</SUB>,a) = I<SUB>j</SUB>
: set action[i,a] = &quot;shift j&quot;
<LI>[A -&gt; &alpha;.] is in
I<SUB>i</SUB> : set action[i,a] to &quot;reduce A -&gt; x&quot;
for all a &isin; FOLLOW(A), where A != S'
<LI>[S' -&gt; S] is in I<SUB>i</SUB> :
set action[i,$] to &quot;accept&quot; 
</UL>

<LI>goto transitions constructed as follows: for all non-terminals:
if goto(I<SUB>i</SUB>, A) = I<SUB>j</SUB>, then goto[i,A] = j
<LI>All entries not defined by (3) &amp; (4) are made &quot;error&quot;.
If there are any multiply defined entries, grammar is not SLR.
<LI>Initial state S<SUB>0</SUB> of parser: that constructed from
I<SUB>0</SUB> or [S' -&gt; S] 
</OL>

<P>

<P>
Example:
<PRE>
<FONT SIZE=3>	S -&gt; aABe		FIRST(S) = {a}		FOLLOW(S) = {$}
	A -&gt; Abc		FIRST{A} = {b}		FOLLOW(A) = {b,d}
	A -&gt; b			FIRST{B} = {d}		FOLLOW{B} = {e}
	B -&gt; d			FIRST{S'}= {a}		FOLLOW{S'}= {$}
</FONT>I<SUB>0</SUB><FONT FACE="Courier New"> = closure([S'-&gt;.S]
   = closure([S'-&gt;.S],[S-&gt;.aABe])
goto(I<SUB>0</SUB>,S) = closure([S'-&gt;S.]) = I<SUB>1
</SUB>goto(I<SUB>0</SUB>,a) = closure([S-&gt;a.ABe])
	    = closure([S-&gt;a.ABe],[A-&gt;.Abc],[A-&gt;.b]) = I<SUB>2
</SUB>goto(I<SUB>2</SUB>,A) = closure([S-&gt;aA.Be],[A-&gt;A.bc])
	    = closure([S-&gt;aA.Be],[A-&gt;A.bc],[B-&gt;.d]) = I<SUB>3
</SUB>goto(I<SUB>2</SUB>,b) = closure([A-&gt;b.]) = I<SUB>4
</SUB>goto(I<SUB>3</SUB>,B) = closure([S-&gt;aAB.e]) = I<SUB>5
</SUB>goto(I<SUB>3</SUB>,b) = closure([A-&gt;Ab.c]) = I<SUB>6
</SUB>goto(I<SUB>3</SUB>,d) = closure([B-&gt;d.]) = I<SUB>7
</SUB>goto(I<SUB>5</SUB>,e) = closure([S-&gt;aABe.]) = I<SUB>8
</SUB>goto(I<SUB>6</SUB>,c) = closure([A-&gt;Abc.]) = I<SUB>9</SUB></FONT>
</PRE>


<h3> Fun with Parsing </h3>

Let's play a "new fun game"* and see what we can do with the following subset
of the C grammar:

<table>
<tr><th> C grammar subset <th> First sets
<tr><td>
<pre>
ats : INT | TYPEDEF_NAME | s_u_spec ;
s_u_spec : s_u LC struct_decl_lst RC |
	s_u IDENT LC struct_decl_lst RC |
	s_u IDENT ;
s_u : STRUCT | UNION ;
struct_decl_lst : s_d | struct_decl_lst s_d ;
s_d : s_q_l SM |
	s_q_l struct_declarator_lst SM ;
s_q_l : ats | ats s_q_l ;
struct_declarator_lst:
	declarator |
	struct_declarator_list CM declarator ;
declarator: IDENT |
	declarator LB INTCONST RB ;
</pre>
<td>
<pre>
First(ats) = { INT, TYPEDEF_NAME, STRUCT, UNION }
First(s_u_spec) = { STRUCT, UNION }
First(s_u) = { STRUCT, UNION }
First(struct_decl_lst) = { INT, TYPEDEF_NAME, STRUCT, UNION }
First(s_d) = { INT, TYPEDEF_NAME, STRUCT, UNION }
First(s_q_l) = { INT, TYPEDEF_NAME, STRUCT, UNION}
First(struct_declarator_lst) = { IDENT }
First(declarator) = { IDENT }
</pre>
</table>

<pre>
Follow(ats) = { $, INT, TYPEDEF_NAME, STRUCT, UNION, IDENT, SM }
Follow(s_u_spec) = { $, INT, TYPEDEF_NAME, STRUCT, UNION, IDENT, SM }
Follow(s_u) = { LC, IDENT }
Follow(struct_decl_lst) = { RC, INT, TYPEDEF_NAME, STRUCT, UNION }
Follow(s_d) = { RC, INT, TYPEDEF_NAME, STRUCT, UNION }
Follow(s_q_l) = { IDENT, SM }
Follow(struct_declarator_lst) = { CM, SM }
Follow(declarator) = { LB , CM, SM }
</pre>

Now, Canonical Sets of Items for this Grammar:

<pre>
I<sub>0</sub> = closure([S' -&gt; . ats]) =
	 closure({[S' -&gt; . ats], [ ats -&gt; . INT ],
	 	  [ ats -&gt; . TYPEDEF_NAME ], [ ats -&gt; . s_u_spec ],
		  [ s_u_spec -&gt; . s_u LC struct_decl_lst RC],
		  [ s_u_spec -&gt; . s_u IDENT LC struct_decl_lst RC],
		  [ s_u_spec -&gt; . s_u IDENT ],
		  [ s_u -&gt; . STRUCT ],
		  [ s_u -&gt; . UNION ]
		  })

goto(I<sub>0</sub>, ats) = closure({[S' -&gt; ats .]}) = {[S' -&gt; ats .]} = I<sub>1</sub>

goto(I<sub>0</sub>, INT) = closure({[ats -&gt; INT .]}) = {[ats -&gt; INT .]} = I<sub>2</sub>
goto(I<sub>0</sub>, TYPEDEF) = closure({[ats -&gt; TYPEDEF_NAME .]}) = {[ats -&gt; TYPEDEF_NAME .]} = I<sub>3</sub>
goto(I<sub>0</sub>, s_u_spec) = closure({[ats -&gt; s_u_spec .]}) = {[ats -&gt; s_u_spec .]} = I<sub>4</sub>

goto(I<sub>0</sub>, s_u) = closure({
		  [ s_u_spec -&gt; s_u . LC struct_decl_lst RC],
		  [ s_u_spec -&gt; s_u . IDENT LC struct_decl_lst RC],
		  [ s_u_spec -&gt; s_u . IDENT ]}) = I<sub>5</sub>

goto(I<sub>0</sub>, STRUCT) = closure({[ s_u -&gt; STRUCT .]}) = I<sub>6</sub>
goto(I<sub>0</sub>, UNION) = closure({[ s_u -&gt; UNION .]}) = I<sub>7</sub>

goto(I<sub>5</sub>, LC) = closure({[ s_u_spec -&gt; s_u LC . struct_decl_lst RC],
[ struct_decl_lst -&gt; . s_d ],
[ struct_decl_lst -&gt; . struct_decl_lst s_d ],
[ s_d -&gt; . s_q_l SM],
[ s_d -&gt; . s_q_l struct_declarator_lst SM],
[ s_q_l -&gt; . ats ],
[ s_q_l -&gt; . ats s_q_l ],
[ ats -&gt; . INT ],
[ ats -&gt; . TYPEDEF_NAME ],
[ ats -&gt; . s_u_spec ],
})
</pre>

<font size=1>* Arnold Schwartzenegger. Do you know the movie? </font>

<p>
<font size=1> <A name=18>lecture #18</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt> I have mysterious syntax errors, what do I do?
<dd> #define YYDEBUG and set yydebug=1 and read the glorious output, especially
   the last couple shifts or reduces before the syntax error.
<dt> I can't fix some of the shift/reduce conflicts, what do I do?
<dd> Nothing. You do not have to fix shift/reduce conflicts.
<dt> I can't fix some of the reduce/reduce conflicts, what do I do?
<dd> Possibly nothing. Bison will always use one of the possibles,
     and never use the other, but that might not kill 120++. It is only
     a deal breaker and has to be fixed if it prevents us from parsing
     correctly and building our tree. Sometimes epsilon
     rules can be removed successfully by adding grammar rules in a
     parent non-terminal that omit an epsilon-deriving child, and then
     modifying the child to not derive epsilon. This might or might not
     help reduce your number of reduce/reduce conflicts.
<dt> Willink's files are incomplete and/or need more logic to enable lexer
     backtracking (?!). What do I do?
<dd> You are welcome to incorporate additional material from Willink's website.
     I did not omit anything on purpose, I just tried to keep the scope to a
     minimum for this assignment.  Ultimately you get to decide which of these
     fine parsers to use, or whether you prefer to write one yourself for a
     subset of C++.
</dl>

<h3> On Tree Traversals </h3>

Trees are classic data structures.
<ul>
<li> Trees have nodes and edges; they are
a special case of graphs.
<li> Tree edges are directional, with roles "parent"
and "child" attributed to the source and destination of the edge.
<li> A tree has the property that every node has zero or one parent.
<li> A node with no parents is called a root.
<li> A node with no children is called a leaf.
<li> A node that is neither a root nor a leaf is an "internal node".
<li> Trees have a size (total # of nodes), a height (maximum count
     of nodes from root to a leaf),
     and an "arity" (maximum number of children in any one node).
</ul>
<p>

Parse trees are k-ary, where there is a
variable number of children bounded by a value k determined by the grammar.
You may wish to consult your old data structures book, or look at some books
from the library, to learn more about trees if you are not totally
comfortable with them.

<p>
<pre>
#include &lt;stdarg.h&gt;

struct tree {
   short label;			/* what production rule this came from */
   short nkids;			/* how many children it really has */
   struct tree *child[1];	/* array of children, size varies 0..k */
				/* Such an array has to be the LAST
				   field of a struct, and "there can
				   be only ONE" for this to work. */
};

struct tree *alctree(int label, int nkids, ...)
{
   int i;
   va_list ap;
   struct tree *ptr = malloc(sizeof(struct tree) +
                             (nkids-1)*sizeof(struct tree *));
   if (ptr == NULL) {fprintf(stderr, "alctree out of memory\n"); exit(1); }
   ptr-&gt;label = label;
   ptr-&gt;nkids = nkids;
   va_start(ap, nkids);
   for(i=0; i &lt; nkids; i++)
      ptr-&gt;child[i] = va_arg(ap, struct tree *);
   va_end(ap);
   return ptr;
}
</pre>
<P>

Besides a function to allocate trees, you need to write one or more recursive
functions to visit each node in the tree, either top to bottom (preorder),
or bottom to top (postorder).  You might do many different traversals on the
tree in order to write a whole compiler: check types, generate machine-
independent intermediate code, analyze the code to make it shorter, etc.
You can write 4 or more different traversal functions, or you can write
1 traversal function that does different work at each node, determined by
passing in a function pointer, to be called for each node.

<pre>
void postorder(struct tree *t, void (*f)(struct tree *))
{
   /* postorder means visit each child, then do work at the parent */
   int i;
   if (t == NULL) return;

   /* visit each child */
   for (i=0; i &lt; t-&gt; nkids; i++)
      postorder(t->child[i], f);

   /* do work at parent */
   f(t);
}
</pre>

You would then be free to write as many little helper functions as you
want, for different tree traversals, for example:
<pre>
void printer(struct tree *t)
{
   if (t == NULL) return;
   printf("%p: %d, %d children\n", t, t->label, t->nkids);
}
</pre>


<h3> Compiling <A href="cgram.y">cgram.y</A> </h3>

It was ripped out of an anesthetized patient...for transplanting,
the buck ultimately stops with you.  Cgram.y was already legal Bison,
but to compile the resulting cgram.tab.c, cgram.y needed a %union
definition.  In order to link or work properly, it will still need
you to write helper functions and coordinate its token definitions
with your lexical analyzer / flex output.  The -d flag causes Bison
to write out a compatible header file to define tokens for flex.


<h3> Parse Tree Example </h3>

Let's do this by way of demonstrating what yydebug=1 does for you, on a
very simple example such as:

<pre>
int fac(unsigned n)
{
   return !n ? 1 : n*fac(n-1);
}
</pre>

Short summary: yydebug generates 1100 lines of tracing output
that explains the parse in Complete Detail.  From which we ought
to be able to build our parse tree example.



<A name="semantic">
<h3> Semantic Analysis </h3>
</A>

Semantic ("meaning") analysis refers to a phase of compilation in which the
input program is studied in order to determine what operations are to be
carried out.  The two primary components of a classic semantic analysis
phase are variable reference analysis and type checking.  These components
both rely on an underlying symbol table.
<p>

What we <em>have</em> at the start of semantic analysis is a syntax tree that
corresponds to the source program as parsed using the context free grammar.
Semantic information is added by annotating grammar symbols with
<em>semantic attributes</em>, which are defined by <em>semantic rules</em>.
A semantic rule is a specification of how to calculate a semantic attribute
that is to be added to the parse tree.
<p>
So the input is a syntax tree...and the output is the same tree, only
"fatter" in the sense that nodes carry more information. 
Another output of semantic analysis are error messages detecting many
types of semantic errors.
<p>

Two typical examples of semantic analysis include:
<dl>
<dt> variable reference analysis
<dd> the compiler must determine, for each use of a variable, which
     variable declaration corresponds to that use.  This depends on
     the semantics of the source language being translated.
<dt> type checking
<dd> the compiler must determine, for each operation in the source code,
     the types of the operands and resulting value, if any.
</dl>
<p>

Notations used in semantic analysis:<br>
<dl>
<dt> <strong><u>syntax-directed definitions</u></strong> </dt>
<dd> high-level (<em>declarative</em>) specifications of semantic rules </dd>
<dt> <strong><u>translation schemes</u></strong> </dt>
<dd> semantic rules and the order in which they get evaluated </dd>
</dl>
<p>

In practice, attributes get <em>stored</em> in parse tree nodes, and the
semantic rules are evaluated either (a) during parsing (for easy rules) or
(b) during one or more (sub)tree traversals.
<p>


<h3> Two Types of Attributes:</h3>
<dl>
<dt> synthesized
<dd> attributes computed from information contained within one's children.
     These are generally easy to compute, even on-the-fly during parsing.
<dt> inherited
<dd> attributes computed from information obtained from one's parent or siblings
     These are generally harder to compute.  Compilers may be able to jump
     through hoops to compute some inherited attributes during parsing,
     but depending on the semantic rules this may not be possible in general.
     Compilers resort to tree traversals to move semantic information around
     the tree to where it will be used.
</dl>


<h3> Attribute Examples </h3>

<h4> Isconst and Value </h4>

Not all expressions have constant values; the ones that do may allow
various optimizations.

<table border>
<tr>
<th> CFG	<th> Semantic Rule
</tr>
<tr>
<td>
E<sub>1</sub> : E<sub>2</sub> + T
<td>
E<sub>1</sub>.isconst = E<sub>2</sub>.isconst && T.isconst<br>
if (E<sub>1</sub>.isconst)<br>
&nbsp;&nbsp;&nbsp; E<sub>1</sub>.value = E<sub>2</sub>.value + T.value<br>
</tr>
<tr>
<td>
E : T
<td>
E.isconst = T.isconst<br>
if (E.isconst)<br>
&nbsp;&nbsp;&nbsp; E.value = T.value<br>
</tr>
<tr>
<td>
T : T * F
<td>
T<sub>1</sub>.isconst = T<sub>2</sub>.isconst && F.isconst<br>
if (T<sub>1</sub>.isconst)<br>
&nbsp;&nbsp;&nbsp; T<sub>1</sub>.value = T<sub>2</sub>.value * F.value<br>
</tr>
<tr>
<td>
T : F
<td>
T.isconst = F.isconst<br>
if (T.isconst)<br>
&nbsp;&nbsp;&nbsp; T.value = F.value<br>
</tr>
<tr>
<td>
F : ( E )
<td>
F.isconst = E.isconst<br>
if (F.isconst)<br>
&nbsp;&nbsp;&nbsp; F.value = E.value<br>
</tr>
<tr>
<td>
F : ident
<td>
F.isconst = FALSE<br>
</tr>
<tr>
<td>
F : intlit
<td>
F.isconst = TRUE<br>
F.value = intlit.ival<br>
<td>
</tr>
</table>

<p>
<font size=1> <A name=19>lecture #19</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt> I have spent 30 hours and am not close to finishing adding the
     500+ tree construction semantic actions required for HW#2!
<dd> I hope as a power programmer, you have learned a powerful programmer's
     editor with a key-memorizing macro facility that can let you pop these
     in very rapidly.  What, you've been typing them in by hand?  Egads!
     Paste the right thing 500 times and then just tweak.  Or paste all
     the constructors with the same number of children in batches, so
     you have less to tweak because you already pasted in the right number
     of kids.
<dt> CxxGrammar.y and CxxLexer.y seem incomplete and hard to use
<dd> Yes, after looking some more at Willink, I recommend Sigala,
100 reduce/reduce conflicts and all. You are still welcome to use Willink
or a grammar of your own devising.  Whichever you use, it would be unwise
to start building tree nodes until you can first make it parse the syntax
correctly.  For example if Willink or Sigala proved to be unusable, it would
be bad to invest in trees for them first before discovering this fact.
<dt> Sigala's parser.y reports a syntax error in the following. What gives?
<pre>
int main()
{
  int x;
  x = 1;
}
</pre>
<dd> I did not plant any bugs, but any bugs in your grammar that prevent
parsing of legal 120++ will have to be fixed. In this case, Sigala
wrote:
<blockquote>
The ISO C++ grammar contains *many* conflicts because it is a
literal copy of the grammar published in the draft standard and no
correction is made for working around the complexity and ambiguity
of the language.  I will track down the conflicts in the future;
please consider this grammar only for informational purposes until I
fix the problems.
</blockquote>
So our reduce/reduce conflicts are seemingly present in the official
ISO C++ 1996 C++ draft standard. I could not have planned a more realistic
real-world HW#2 yacc/bison exercise.  Here is the ISO C++ standard, and
we know every real C++ compiler manages to parse it, and in 1996, I assure
you, g++ was still using a bison-based parser...so we could look for a
copy of g++ circa 1998 and see if its parser would work. Bison parsing
is do-able for the version of C++ described by this grammar, and 120++
is smaller. We can do this.
<p>
 And they seem to be real problems,
not just ignorable.  In this case: historically
curly braces in C had to have all declarations at the top. The grammar
was changed to allow declarations anywhere. In Sigala, a compound
statement contains statement_seq_opt, which contains statement_seq, which
contains statement, which contains declaration_statement, which contains
block_declaration, which contains simple_declaration, which ought to
allow "int x;". It doesn't, and we need to know why not, and how to
fix it. Someone suggested a workaround of just allowing declarations
at the top of compound statements in the old C way, and that would work
for about 98% of Dr. Soule's 120 text, but sadly his book does contain
spots where declarations appear after executable code.
</dl>

<h3> Observations on Debugging the ANSI C++ Grammar to be more YACC-able </h3>

<dl>
<dt>Expectation
<dd> not that you pick it up by magic and debug it all yourself,
     but rather that you spend enough time monkeying with yacc grammars
     to be familiar with the tools and approach, and to ask the right questions.
<dt> Tools
<dd> YYDEBUG/yydebug, --verbose/--debug/y.output
<dt> Approach
<dd>
<ul>
<li> Run with yydebug=1 to study current behavior
<li> Do the minimum number of edits necessary to fix*
<li> reduce obvious epsilon vs. epsilon
<li>  Examine y.output to understand remaining reduce/reduce conflicts.
<li>  Delete the causes if they are not in 120++
<li> Refactor the causes if they are in 120++
</dl>
<p>

*why?  why not?
<p>

<h3> What I did last night </h3>

<ul>
<li> turned on yydebug=1
<li> trace shows reduce to ctor_initializer_opt before '{', looks good
<li> shift of '{' OK
<li> shift of INT leads to reduce of simple_type_specifier, seems OK
<li> should reduce to type_specifier, decl_specifier, decl_specifier_seq,
      decl_specifier_seq_opt: should be OK
<li> init_declarator_list_opt should go to init_declarator_list,
      init_declarator, declarator, direct_declarator, declarator_id,
      id_expresssion, unqualified_id, identifier, IDENTIFIER
<li> _opt grammar rules introduce epsilons, they were for the convenience
     of the ISO C++ committee, but we know epsilons are evil
<li> removed _opt grammar rules; adjacent ones, at least, were trouble
<li> removing adjacent ones helped, but removing others did not
<li> built with --verbose and --debug, looked at y.output
<li> before removing _opt's, reduce/reduce conflicts were very
     distributed 2,1,5,6,5,3,4,4,6,6,2,6,6,6,6,2,6,6,6,6,6
<li> after removing _opt's, reduce/reduce conflicts were more concentrated:
     45, 13, 9, 9, 9, 9 and a couple strays
<li> about half of reduce/reduce were due to "pseudo-destructors". delete
<li> 13 due to template_id's not knowing whether they are class_name or unqualified_id's. could delete trivially for 120++ (no templates)
<li> four batches of 9 reduce/reduce: type_name could be simple_type_specifier or declarator_id, COLONCOLON type_name not knowing if it is a simple_type_specifier or declarator_id, nested_name_specifier type_name not knowing if it is a simple_type_specifier or a declarator_id, and COLONCOLON nested_name_specifier type_name not knowing if it is a simple_type_specifier or a declarator_id
<li> a couple oddballs: class_key identifier not knowing if it is an elaborated_type_specifier or a class_head, with and without a nested_name_specifier after the class_key
<li> ^-- these latter two bullets identify grammar rules we will either
      delete (if not in 120++) or refactor
</ul>
<p>

<font size=1> <A name=20>lecture #20</A> began here</font><p>
<p>

<h3> Fix the Date of the Midterm </h3>

We voted, and will have a midterm on Thursday October 16 in class.


<h3> Mailbag </h3>

<dl>
<dt> Your new reference parser reports a bogus error on this simple
     120++ program of mine that uses strings:
<pre>
int main(){
  string name;	
}
</pre>
<dd> Yes, g++ reports an error here too, it is not bogus.  And interestingly,
g++ still reports a syntax error if you add #include &lt;string&gt;. In order
to recognize the non-built-in type string, the
C++ program has to have "using namespace std;" and include one of:
&lt;string&gt;, &lt;iostream&gt;, or &lt;fstream&gt;.  It turns out the
{io,f}stream  includes include &lt;string&gt;.  If these conditions are
present, you should insert "string" into a type names table, such that
your lexical analyzer returns CLASS_NAME when it sees <code>string</code>.

<dt> Your new reference parser reports a bogus error on this simple
      120++ program that declares a class:
<pre>
class Foo{
public:
   Foo();   
   int play();
};
Foo::play(){
  return 0;
}
</pre>
<dd> Aside from probably needing the reserved word "int" before
Foo::play, the reference code posted does not populate the
"type names table" with the names of classes that it encounters.
Part of HW#2 would include this feedback from the parser to the
lexical analyzer.
</dl>


<h3> On the mysterious TYPE_NAME </h3>

The C/C++ typedef construct is an example where all the beautiful
theory we've used up to this point breaks down.  Once a typedef is
introduced (which can first be recognized at the syntax level), certain
identifiers should be legal type names instead of identifiers.  To make
things worse, they are still legal variable names: the lexical analyzer
has to know whether the syntactic context needs a type name or an
identifier at each point in which it runs into one of these names. This
sort of feedback from syntax or semantic analysis back into lexical
analysis is not un-doable but it requires extensions added by hand to
the machine generated lexical and syntax analyzer code.
<p>
<pre>
typedef int foo;
foo x;                    /* a normal use of typedef... */
foo foo;                  /* try this on gcc! is it a legal global? */
void main() { foo foo; }  /* what about this ? */
</pre>
<p>



<h3> Symbol Table Module </h3>

Symbol tables are used to resolve names within name spaces. Symbol
tables are generally organized hierarchically according to the
scope rules of the language.  Although initially concerned with simply
storing the names of various that are visible in each scope, symbol
tables take on additional roles in the remaining phases of the compiler.
In semantic analysis, they store type information.  And for code generation,
they store memory addresses and sizes of variables.
<p>

<dl>
<dt> mktable(parent)
<dd> creates a new symbol table, whose scope is local to (or inside) parent
<dt> enter(table, symbolname, type, offset)
<dd> insert a symbol into a table
<dt> lookup(table, symbolname)
<dd> lookup a symbol in a table; returns structure pointer including type and offset.  lookup operations are often <em>chained</em> together progressively from most local scope on out to global scope.
<dt> addwidth(table)
<dd> sums the widths of all entries in the table.  ("widths" = #bytes, sum of
   widths = #bytes needed for an "activation record" or "global data section").
   Worry not about this method until code generation you wish to implement.
<dt>enterproc(table, name, newtable)
<dd> enters the local scope of the named procedure
</dl>

<h3> Variable Reference Analysis </h3>

The simplest use of a symbol table would check:

<ul>
<li> for each variable, has it been declared?  (undeclared error)
<li> for each declaration, is it already declared? (redeclared error)
</ul>


<h4> Reading Tree Leaves </h4>

In order to work with your tree, you must be able to tell, preferably
trivially easily, which nodes are tree leaves and which are internal nodes,
and for the leaves, how to access the lexical attributes.
<p>
Options:
<ol>
<li> encode in the parent what the types of children are
<li> encode in each child what its own type is (better)
</ol>
How do you do option #2 here?
<p>
Perhaps the best approach to all this is to unify the tokens and parse tree
nodes with something like the following, where perhaps an nkids value of -1
is treated as a flag that tells the reader to use
lexical information instead of pointers to children:

<pre>
struct node {
int code;		/* terminal or nonterminal symbol */
int nkids;
union {
   struct token { ...  } leaf;
   struct node *kids[9];
   }u;
} ;
</pre>

There are actually nonterminal symbols with 0 children (nonterminal with
a righthand side with 0 symbols) so you don't necessarily want to use
an nkids of 0 is your flag to say that you are a leaf.

<font size=1> <A name=21>lecture #21</A> began here</font>

<h3> HW Code Sharing Policy Reminder </h3>

<ul>
<li>You can share ideas but are not to share code with your classmates.
<li>On HW#1, several submissions gave me more deja vu that they should have.
<li>If you used an external source, be sure to cite it and make clear
the scope/extent of code that is not your own.
<li>If anything is shared in this class (e.g. yacc grammars, or donuts) it must
be shared with the whole class.  Otherwise, it ruins the level playing field
and makes grading impossible...
<li>On anything else that gives me excessive deja vu in this class,
     I will give zeros, or refer you to the
     appropriate university committee.
</ul>

<H3> Semantic Analysis in Concrete Terms </h3>

Broadly, we can envision the semantic analysis as two passes:

<dl>
<dt> Pass 1: Symbol Table Population
<dd> Symbol table population is a syntax tree traversal in which
we look for nodes that introduce symbols, including the creation
and population of local scopes and their associated symbol tables.
As you walk the tree, we look for specific nodes that indicate
symbols are introduced, or new local scopes are introduced. What
are the tree nodes that matter (from <A href="cgram.y">cgram.y</A>)
in this particular example?
<ol>
<li> create a global symbol table (initialization)
<li> each function_declarator introduces a symbol.
<li> each init_declarator introduces a symbol.
<li> oh by the way, we have to obtain the types for these.
<li> "types" for functions include parameter types and return type
<li> "types" for init_declarators come from declaration_specifiers,
     which are "uncles" of init_declarators
</ol>

<dt> Pass 2: Type Checking
<dd> Type checking occurs during a bottom up traversal of the expressions
within all the statements in the program.
</dl>


<h3> Discussion of <A href="semantic.c">a Semantic Analysis Example</A></h3>

<ul>
<li> several example tree traversals that do different tasks
     during semantic analysis.
<li> this was "ripped out" of a past project
<li> goal: give you ideas
<li> not meant to force you to use this code, or do things this way
</ul>

<p>


<font size=1> <A name=22>lecture #22</A> began here</font>

<h3> Changes to Sigala's ISO 96 C++ Grammar Made for 120++ in 120gram.y </h3>

<ul>
<li> changes were motivated by a need to eliminate reduce/reduce
     conflicts.
<li> removing adjacent optional items was generally mandatory
<li> removing optional items at beginning and ending of a rule
       was usually required
<li> optional items in the middle of a rule were often OK
</ul>

<dl>
<dt> removed of namespace_alias from namespace_name
<dd> ambiguity of these identifier-like rules not needed since we aren't
     doing namespaces properly in 120++.
<dt> removed :: prefixed primary expressions
<dd> overriding current namespace not necessary since we aren't doing
     namespaces properly in 120++.
<dt> removed template_id from unqualified_id
<dd> we aren't doing templates in 120++
<dt> refactored TEMPLATE_opt into two productions in qualified_id
<dt> refactored class_or_namespace_name and nested_name_specifier_opt in
      nested_name_specifier
<dd> class_or_namespace_name basically gave two ways to use an identifier;
      difference is semantic
<dt> removed a rule starting with simple_type_specifier in postfix_expression
<dt> factored out adjacent optionals in postfix_expression
<dt> remove pseudo_destructor names
<dt> pulled '*' and '&' out of unary_operator to avoid reduce/reduce conflicts
<dd> but allow them explicitly in unary_expression
<dt> factored out COLONCOLON_opt in new_expression and delete_expression
<dt> removed possibility of empty simple_declaration (empty ; is not a
     declaration) and init_declarator_list with no decl_specifier_seq
     in front of it
<dt> refactored adjacent optionals in
     simple_declaration,
     simple_type_specifier, elaborated_type_specifier
      qualified_namespace_specifier, using_declaration, direct_declarator,
      direct_abstract_declarator, parameter_declaration_clause,
      member_declaration, base_specifier
<dt> removed optionality of ENUM_opt in enum_specifier
<dd> not that 120++ has to do enum's
<dt> removed optionals at beginning and end of ptr_operator
<dt> refactored optional at end of cv_qualifier_seq
<dt> refactored optional begin of declarator_id
<dt> refactored optional beginning and internal element of function_definition
<dt> refactored class_head to avoid adjacent optionals, removed
      possibility of class head with no identifier
<dt> refactored optionals at end of member_declarator
<dt> removed optionality of identifiers in type_parameter
</dl>



<h4> Type Checking </h4>

Perhaps the primary component of semantic analysis in many traditional
compilers consists of the type checker.  In order to check types, one first
must have a representation of those types (a type system) and then one must
implement comparison and composition operators on those types using the
semantic rules of the source language being compiled.  Lastly, type checking
will involve adding (mostly-) synthesized attributes through those parts of
the language grammar that involve expressions and values.

<h4> Type Systems </h4>

Types are defined recursively according to rules defined by the source
language being compiled. A type system might start with rules like:

<ul>
<li> Base types (int, char, etc.) are types
<li> Named types (via typedef, etc.) are types
<li> Types composed using other types are types, for example:
    <ul>
    <li> array(T, indices) is a type. In some
         languages indices always start with 0, so array(T, size) works.
    <li> T1 x T2 is a type (specifying, more or
         less, the tuple or sequence T1 followed by T2;
	 x is a so-called cross-product operator).
    <li> record((f1 x T1) x (f2 x T2) x ... x (fn x Tn)) is a type
    <li> in languages with pointers, pointer(T) is a type
    <li> (T<sub>1</sub> x ... T<sub>n</sub>) -> T<sub>n+1</sub> is a
         type denoting a function mapping parameter types to a return type
    </ul>
<li> In some language type expressions may contain variables whose values
     are types.
</ul>

In addition, a type system includes rules for assigning these types
to the various parts of the program; usually this will be performed
using attributes assigned to grammar symbols.




<h3> Representing C (C++, Java, etc.) Types </h3>

The type system is represented using data structures in the compiler's
implementation language.
In the symbol table and in the parse tree attributes used in type checking,
there is a need to represent and compare source language types.  You might
start by trying to assign a numeric code to each type, kind of like the
integers used to denote each terminal symbol and each production rule of the
grammar.  But what about arrays?  What about structs?  There are an infinite
number of types; any attempt to enumerate them will fail.  Instead, you
should create a new data type to explicitly represent type information.
This might look something like the following:
<p>

<pre>struct c_type {
   /*
    * Integer code that says what kind of type this is.
    * Includes all primitive types: 1 = int, 2=float,
    * Also includes codes for compound types that then also
    * hold type information in a supporting union...
    * 7 = array, 8 = struct, 9 = pointer etc. */
   int base_type;
   union {
      struct array {
         int size; /* allow for missing size, e.g. -1 */
	 struct c_type *elemtype; /* pointer to c_type for elements in array,
	 				follow it to find its base type, etc.*/
      } a;
      struct struc {		/* structs */
         char *label;
	 int nfields;
         struct field **f;
	 } s;
      struct ctype *p;		/* pointer type, points at another type */
   } u;
}

struct field {			/* members (fields) of structs */
   char *name;
   struct ctype *elemtype;
}

</pre>

Given this representation, how would you initialize a variable to
represent each of the following types:

<pre>
int [10][20]
struct foo { int x; char *s; }
</pre>



<h3> Lessons From the Godiva Project </h3>

By way of comparison, it may be useful for you to look at
some symbol tables and type representation code that were written for
the Godiva programming language project.  Being a dialect of Java, Godiva
has compile-time type checking and might provide relevant ideas for OOP
languages.

<ul>
<li> <A href="type.h">type.h</A>
<li> <A href="type.c">type.c</A>
<li> <A href="symtab.h">symtab.h</A>
<li> <A href="symtab.c">symtab.c</A>
</ul>

<p>
<em> You could have a discussion of packages and "import" declarations
here, if the source language this semester supports them.</em>
<p>
<!--
<h3> Discussion of "Import", and more Generally, Packages </h3>

Suggested approaches for implementing semantic analysis of packages/imports:

<dl>
<dt> "import" == concatenate, or include
<dd>
<Ul>
<li> Pros: moderately easy to implement
<li> import x.y.z means class z out of package x.y
<li> Cons: code duplication, reparsing stuff a lot
</ul>
<dt> "import" == symbol table insert/merge
<dd>
<ul>
<li> Pro: don't have cons of the include approach
<li> Con: either have to reparse whole files in order to suck in types
     for symbols we import OR have to write out symbol tables as external
     files/repositories of info about compiled packages/classes
</ul>
</dl>
-->


<p>
<font size=1> <A name=23>lecture #23</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt> Do I need to add <code>%type &lt;treeptr&gt; nonterm</code> for every
symbol in the grammar in order to have everything work
<dd> yes
<dt> when we run into using namespace std; we place string into our
      type name table, but what about cin/cout/endl ?
<dd> In HW#2 we only need the names of types, because they are needed in
      order to parse successfully and not get syntax errors. In addition
      to string, the type names ifstream, ofstream, and fstream
      appear in 120++ and should get added if the include(s) and
      "using namespace std" appear in the program.
<dt> Are we supporting (syntactically) nested classes/structs?
<dd> no
<dt> Do we have to parse anything with ::
<dd> classname::function name (including classname::constructor) appear to
     be the only uses of :: in 120++.
<dt> What do I do with epsilon rules? Empty tree nodes?
<dd> I previously said to use either $$ = NULL or $$ = alctree(RULE, 0).
     Whether the latter is preferable
</dl>

<h3> Having Trouble Debugging? </h3>

To save yourself on the semester project in this class, you should
learn gdb (or some other source level debugger) as well as you can.
Sometimes it can help you find your bug in seconds where you would have
spent hours without it.  But only if you take the time to read the manual
and learn the debugger.

<p>

To work on segmentation faults: recompile all .c files with -g and run your
program inside gdb to the point of the segmentation fault.  Type the gdb
"where" command.  Print the values of variables on the line mentioned in the
debugger as the point of failure.  If it is inside a C library function, use
the "up" command until you are back in your own code, and then print the
values of all variables mentioned on that line.

<p>

After gdb, the second tool I recommend strongly is valgrind.  valgrind
catches some kinds of errors that gdb misses.  It is a non-interactive
tool that runs your program and reports issues as they occur, with a big
report at the end.
<p>

There is one more tool you should know about, which is useful for certain
kinds of bugs, primarily subtle memory violations.  It is called electric
fence.  To use electric fence you add
<pre>
	/home/.../libefence.a
</pre>
to the line in your makefile that links your object files together to
form an executable.  Assuming you can find or build a copy of libefence.a
somewhere.

<h3>Discussion of Tree Traversals that perform Semantic Tests</h3>

This example illustrates just one of the
myriad-of-specialty-traversal-functions that might be used.
This mindset is one way
to implement semantic analysis. <p>

Suppose we have a grammar rule
<pre>
AssignStmt : Var EQU Expr
</pre>

We want to detect if a variable has not been initialized, before it is
used.  We can add a boolean field to the symbol table entry, and set it
if we see, during a tree traversal, an initialization of that variable.
What are the limitations or flaws in this approach?

<p>
We can write traversals of the whole tree after all parsing
is completed, but for some semantic rules, another option is to
extend the C semantic action for that rule with
extra code after building our parse tree node:
<pre>
AssignExpr : LorExpr '=' AssignExpr { $$ = alctree(..., $1, $2, $3);
	lvalue($1);
	rvalue($3);
	}
</pre>

<ul>
<li> In this example, <code>lvalue()</code> and <code>rvalue()</code>
are mini-tree traversals for the lefthand side
and righthand side of an assignment statement.
<li> Their missions are to
propagate information from the parent, namely, inherited attributes
that tell nodes whether their values are being assigned to (initialized)
or being read from.
<li> Warning: since this is happening during parsing,
it would only work if all semantic information that it depends on,
for example symbol tables, was also done during parsing.

<li> Side note: I might be equally or more interested in implementing a
semantic check to make sure the left-hand-side of an assignment is actually
an assignable variable. How would I check for that?
</ul>

<pre>
void lvalue(struct tree *t)
{
   if (t-&gt;label == IDENT) {
      struct symtabentry *ste = lookup(t->u.token.name);
      ste->lvalue = 1;
   }
   for (i=0; i&lt;t-&gt;nkids; i++) {
      lvalue(t->child[i]);
      }
}
void rvalue(struct tree *t)
{
   if (t-&gt;label == IDENT) {
      struct symtabentry *ste = lookup(t-&gt;u.token.name);
      if (ste-&gt;lvalue == 0) warn("possible use before assignment");
   }
   for (i=0; i&lt;t-&gt;nkids; i++) {
      rvalue(t-&gt;child[i]);
      }
}
</pre>

<h4> What is different about real life as opposed to this toy example</h4>

This example illustrated walking through subtrees looking for specific
nodes where some information was inserted into the tree.  In real life...
<ul>
<li> information passed down (i.e. inherited attributes) may be passed
     as a (second or subsequent)
     parameter after the tree node the traversal is visiting.
<li> this example might apply mainly to local variables whose definition
     and use are in this same (function definition) subtree
<li> if you wanted to ensure a class or global variable was initialized before
     use, you might build a flow graph (often used in an optimization or
     final code generation phase anyhow)
<li> variable definition and use attributes are more reliably analyzed
     using a flow graph instead of the syntax tree.
</ul>

For example, if the program starts by calling
a subroutine at the bottom of code which initializes all the
variables, the flow graph will not be fooled into generating warnings
like you would if you just started at the top of the code and checked
whether for each variable, assignments appear earlier in the source
code than the uses of that variable.

<p>
<font size=1> <A name=24>lecture #24</A> began here</font>
<p>


<h3> Example Semantic Rules for Type Checking </h3>

Interestingly, I note that type checking is in Chapter 6 of your text,
not chapter 5.  Nevertheless, I will assert it as a primary example
of using synthesized semantic attributes.

<table border>
<tr>
<th> grammar rule <th> semantic rule
<tr>
<td>E<sub>1</sub> : E<sub>2</sub> PLUS E<sub>3</sub>
<td>E<sub>1</sub>.type = check_types(PLUS, E<sub>2</sub>.type, E<sub>3</sub>.type)
<tr>
</table>

Where check_types() returns a (struct c_type *) value.  One of the values
it can return is TypeError.  The operator (PLUS) is passed in to
the check types function because behavior may depend on the operator --
the result type for array subscripting works different than the result
type for the arithmetic operators, which may work different (in some
languages) than the result type for logical operators that return booleans.

<h3> In-class brainstorming: what other type-check rules can we derive? </h3>

Consider the class project. What else will we need to check during semantic
analysis, and specifically during type checking?


<h3>Type Promotion and Type Equivalence</h3>

When is it legal to perform an assignment x = y?  When x and y are
identical types, sure.  Many languages such as C have automatic
promotion rules for scalar types such as shorts and longs.
The results of type checking may include not just a type attribute,
they may include a type conversion, which is best represented by
inserting a new node in the tree to denote the promoted value.
Example:
<pre>
int x;
long y;
y = y + x;
</pre>
<p>

For records/structures, some languages use name equivalence, while
others use structure equivalence.  Features like typedef complicate
matters.  If you have a new type name MY_INT that is defined to be
an int, is it compatible to pass as a parameter to a function that
expects regular int's?  Object-oriented languages also get interesting
during type checking, since subclasses usually are allowed anyplace
their superclass would be allowed.





<h3> Implementing Structs (a C thing) </h3>

<ol>
<li> storing and retrieving structs by their label -- the struct label is
    how structs are identified.  You do not have to do typedefs and such.
    The labels can be keys in a separate hash table, similar to the global
    symbol table.  You can put them in the global symbol table so long as
    you can tell the difference between them and variable names.

<li> You have to store fieldnames and their types, from where the struct is
    declared.  You could use a hash table for each struct, but a link list
    is OK as an alternative.

<li> You have to use the struct information to check the validity of each
    dot operator like in rec.foo.  To do this you'll have to lookup rec
    in the symbol table, where you store rec's type.  rec's type must be
    a struct type for the dot to be legal, and that struct type should
    include a hash table or link list that gives the names and types of
    the fields -- where you can lookup the name foo to find its type.
</ol>



<h3> Type Checking Example </h3>

Work through a type checking example for the function call to foo() in:

<pre>
int foo(int x, char *y);

int main()
{
   int z = foo(5, "funf");
   return 0;
}
</pre>

<p>
After parsing, the syntax tree (for the call) looks like:
<p>

<img src="semantic.jpg">
<p>

Before we can type check the call to foo(),
we must work out exactly what is in the symbol table, besides
labeling it as type FUNC.



<h3> Need Help with Type Checking? </h3>

<ul>
<li> Implement the C Type Representation given previously
<li> Read the Book
<li> What OPERATIONS (functions) do you need, in order to check
     whether types are correct?  What parameters will they take?
</ul>

<p>
<font size=1> <A name=26>lecture #26</A> began here</font>
<p>

<h3> Building Type Information </h3>

<ul>
<li> Last lecture I was asked to discuss pseudo-code for populating
     the symbol table, as needed for the type-check example that
     we worked on.
<li> Note: we more-or-less covered the key
     functions with handwaiving pseudocode last week.
<li> What we didn't cover much was: constructing C type representations
     from syntax trees.
<li> If we had time, we could stand to do a more concrete example of
     populating the symbol table
</ul>

<img src="c_type_rep.png">


<pre>
/*
 * Build Type From Prototype (syntax tree) Example
 */
void btfp(nodeptr n)
{
   if (n==NULL) return;
   for(int i = 0; i < n->nkids; i++) btfp(n->child[i]);
   switch (n->prodrule) {
   case INT:
      n->type = get_type(INTEGER);
      break;
   case CHAR:
      n->type = get_type(CHARACTER);
      break;
   case IDENTIFIER:
      n->type = get_type(DONT_KNOW_YET);
      break;
   case '*':
      n->type = get_type(POINTER);
      break;
   case PARAMDECL_1:
      n->type = n->child[0]->type;
      break;
   case THINGY:
      n->type = n->child[0]->type;
      break;
   case PARAMDECL_2:
      n->type = clone_type(n->child[1]->type);
      n->type->u.p.elemtype = n->child[0]->type;
      break;
   case PARAMDECLLIST_2:
      n->type = n->child[0]->type;
      break;
   case PARAMDECLLIST_1:
      n->type = get_type(TUPLE)
      n->type->u.t.nelems = 2;
      n->type->u.t.elems = calloc(2, sizeof(struct typeinfo *));
      n->type->u.t.elems[0] = n->child[0]->type;
      n->type->u.t.elems[1] = n->child[1]->type;
      break;
   case INITIALIZER_DECL:
      n->type = get_type(FUNC)
      n->type->u.f.returntype = get_type(DONT_KNOW);
      n->type->u.f.params = n->child[1].type;
      break;
   case SIMPLE_DECLARATION_1:
      n->type = clone_type(n->child[1]->type);
      n->type->u.f.returntype = n->child[0]->type;
   }
}
</pre>

<p>
<font size=1> <A name=27>lecture #27</A> began here</font>
<p>

<h3> Filling in the remainder of btfp() </h3>

Let's peek backwards a tad and look at the remaining three tree nodes
that I filled in...

<h3> Type Checking Function Calls </h3>

<ul>
<li> at every node in our tree, we build a .type field
<li> Probably logically two separate jobs:
<ul>
<li> Build types for declarations, insert them into symbol table(s).
      Performed during the declarations pass of semantic analysis.
<li> Build types for expressions, lookup symbols from symbol table(s)
      Performed during the typecheck pass of semantic analysis.
</ul>

<li> For the typecheck pass, a recursive function, typecheck(n), traverses the
     tree sticking types into expression nodes.
<li> you may choose to write a helper function check_types(OPERATOR,
operandtype, operandtype) to do the heavy lifting at each node
<li> What will type checking a function call need?
<li> Can we just check the type of the symbol against the type of the call
expression?
<li> Type of symbol: constructed as per last lecture. In symbol table.
<li> Type of call expression (built within expression part of grammar), SANS
     return type.
<li> Type check verifies it and replaces it with return type.
</ul>

<pre>
void typecheck(nodeptr n)
{
   if (n==NULL) return;
   for(int i; i<n->nkids; i++) typecheck(n->child[i]);
   switch(n->prodrule) {
   ...
   case POSTFIX_EXPRESSION_3: {
      n->type = check_types(FUNCALL, n->child[0]->type, n->child[2]->type);
      }
   }
}

...

typeptr check_types(int operand, typeptr x, typeptr y)
{
   switch (operand) {
   case  FUNCALL: {
      if (x->basetype != FUNC)
         return type_error("function expected", x);
      if (y->basetype != TUPLE)
         return type_error("tuple expected", y);
      if (x->u.f.nparams != y->u.t.nelems)
         return type_error("wrong number of parameters", y);

      /*
       * for-loop, compare types of arguments
       */
      for(int i = 0; i < x->u.f.nparams; i++)
         if (check_types(PARAM, x->u.f.params[i], y->u.t.elems[i]) ==
	     TYPE_ERROR) return TYPE_ERROR;
      /*
       * If the call is OK, our type is the function return type.
       */
      return x->u.f.returntype;
      break;
      }
   }
}
</pre>


<h3> Semantic Analysis and Classes </h3>

What work is performed during the semantic analysis phase, to support classes?

<ul>
<li> Build class-level symbol tables
<li> Within class member functions, three-level symbol lookup
     (local first, then class, then global).
<li> In the implementation of x.y and x->y, lookup y within x's
     type's symbol table, using privacy rules.
<li> ...what are the other issues for semantic analysis of objects in 120++??
</ul>

<p>
<font size=1> <A name=28>lecture #28</A> began here</font>
<p>

<h3> Feedback on HW#2 </h3>

<dl>
<dt> Using { $$ = $4; } is probably a bad idea
<dd> By way of midterm review: under what circumstances would this be fine?
<dt> Using { $$ = $1; } goes without saying
<dd> It is the default...
<dt> What is wrong with this hash?
<pre>
for(i=0;i&lt;strlen(s);i++) {
   sum+=s[i];
   sum%=ARRAYSIZE;
   }
</pre>
<dd> Amalgam of things wrong in several students' hash functions...
<dt> passing an fopen() or a malloc() as a parameter into a function is
     probably a bad idea
<dd> usually, this is a resource leak
<dt> using landscape mode is far better than linewrapping code illegibly
<dd> although it gives me a crick in my neck from trying to read sideways
<dt> Some of you are still not commenting to a minimum professional level
     needed for you to understand your own code in 6 months
</dl>




<h3> Old Questions (and Answers) </h3>

<dl>
<dt>
1) What is the difference between a function declaration and a variable declaration, when it comes to adding the symbols to the table?  as far as the tree is concerned they are almost exactly the same, with the exception of which parent node you had.  Is there (or should there be) a line in the symbol entry which states the entry as a function vs a variable?
<dd>
You add the symbols to the same table, with different type information for
functions (whose type includes their parameters and return type) than for
simple variables.

<dt>
I have code written which (hopefully) creates the symbol table entry for variables.  This code uses a function which spins on direct_declarator to get the identifier. Can I use this same function to get the identifier for a function because a function is "direct_function_declarator: direct_declarator LP" followed by some other useful things that I'm not sure need to be in the symbol table entry. 
<dd>
You can re-use functions that work through symmetric subtrees, either as-is
(if the subtrees really use the same parts of the grammar) or by generalizing
or making generic the key decisions about what to do based on production rule.

<dt>
You state in the assignment "you can emit semantic errors for anything not defined".  Can we stop the compiler at the lexical phase, whenever a keyword which is not supported is detected?  Instead of building the whole tree (and starting a semantic walk) for a program which has something unsupported in it, can we just stop as soon as we see the unsupported feature token?
<dd>
Reluctantly, I might live with this.

<dt>
you state "You do not have to support nested local scopes".  Does this mean there will only be a global scope, or will there be a global + function scopes, but no secondary scopes inside the local functions?
<dd> Correct, function scopes for locals and parameters, but not nested local
scopes inside those.

<dt>
you have the type checking done at the same time as the symbol table entry.  Is there any reason not to break these out into 2 separate functions?
<dd> No, no reason at all. In the old days there were reasons.

<dt>
5) In the enter_newscope() function, what is "t = (typ==CLASS_TYPE) ? alcclasstype(s, new):alcmethodtype(NULL,NULL,new);"  It looks like it is code to deal with methods and classes, do we need this code, since we do not have classes?
<dd> No, you do not need this code, it was for an OOP language.  You might need
something for functions that allocates a C type representation for them.

<dt>
6) Are we supporting declaration_specifiers: type_specifier declaration_specifiers?  AKA: Are we supporting multiple types?  I'm reasonably certain that line is there for cases such as unsigned int, not for void int or int char, are we supporting unsigned int?  Or, on a similar note const int?
<dd>
You can either ignore (e.g. unsigned or short) or print a semantic error for
declaration specifiers that are beyond our subset.
</dl>



<h3> How to TypeCheck Square Brackets </h3>

This is about the grammar production whose right-hand side is:
<pre>
postfix_expression LB expression RB
</pre>

<ol>
<li> recursively typecheck $1 and $3 ... compute/synthesize their .type fields.
<li> What type(s) does $1 have to be?  ARRAY (or TABLE, if a table type exists)
<li> What type(s) does $3 have to be?  INTEGER (or e.g. ARRAY OF CHAR, for tables)
<li> What is the result type we assign to $$?  Lookup the element type from $1
</ol>





<h3>Run-time Environments </h3>

How does a compiler (or a linker) compute the addresses for the various
instructions and references to data that appear in the program source code?
To generate code for it, the compiler has to "lay out" the data as it will
be used at runtime, deciding how big things are, and where they will go.

<ul>
<li> Relationship between source code names and data objects during execution
<li> Procedure activations
<li> Memory management and layout
<li> Library functions
</ul>





<h4> Scopes and Bindings </h4>

Variables may be declared explicitly or implicitly in some languages
<p>

Scope rules for each language determine how to go from names to declarations.
<p>

Each use of a variable name must be associated with a declaration.
This is generally done via a symbol table. In most compiled languages
it happens at compile time (in contrast, for example ,with LISP).

<h4> Environment and State </h4>

Environment maps source code names onto storage addresses (at compile time),
while state maps storage addresses into values (at runtime).  Environment
relies on binding rules and is used in code generation; state operations
are loads/stores into memory, as well as allocations and deallocations.
Environment is concerned with scope rules, state is concerned with things
like the lifetimes of variables.
<p>

<p>
<font size=1> <A name=29>lecture #29</A> began here</font>
<p>

<h3> HW2 Feedback </h3>

<dl>
<dt> If you didn't submit, or submitted with a name
     other than hw2.zip, you may need to take action.
<dd> I will be compiling and running this homework as part of grading
<dt> Several submitted printouts do not have working parse trees
<dd> If you are not in possession of working parse trees at this point,
     Get Help Now, or plan to take CS 445 next year with "Smiling Bob"
     Heckendorn.
</dl>

<h3> Runtime Memory Regions </h3>

Operating systems vary in terms of how the organize program memory
for runtime execution, but a typical scheme looks like this:

<table border>
<tr><th>code
<tr><th>static data
<tr><th>stack (grows down)
<tr><td>heap (may grow up, from bottom of address space)
</table>

The code section may be read-only, and shared among multiple instances
of a program.  Dynamic loading may introduce multiple code regions, which
may not be contiguous, and some of them may be shared by different programs.
The static data area may consist of two sections, one for "initialized data",
and one section for uninitialized (i.e. all zero's at the beginning).
Some OS'es place the heap at the very end of the address space, with a big
hole so either the stack or the heap may grow arbitrarily large.  Other OS'es
fix the stack size and place the heap above the stack and grow it down.

<h4> Questions to ask about a language, before writing its code generator</h4>

<ol>
<li> May procedures be recursive? (Duh, all modern languages...)
<li> What happens to locals when a procedure returns? (Lazy deallocation rare)
<li> May a procedure refer to non-local, non-global names?
     (Pascal-style nested procedures, and object field names)
<li> How are parameters passed? (Many styles possible, different
     declarations for each (Pascal), rules hardwired by type (C)?)
<li> May procedures be passed as parameters?  (Not too awful)
<li> May procedures be return values? (Adds complexity for non-local names)
<li> May storage be allocated dynamically (Duh, all modern languages...
     but some languages do it with syntax (new) others with library (malloc))
<li> Must storage by deallocated explicitly (garbage collector?)
</ol>


<h3> "Modern" Runtime Systems </h3>

The preceding discussion has been mainly about traditional languages such as
C.  Object-oriented programs might be much the same, only every activation
record has an associated object instance; they need one extra "register" in
the activation record.   In practice, modern OO runtime systems have many
more differences than this, and other more exotic language features imply
substantial differences in runtime systems.  Here are a few examples of
features found in runtimes such as the Java Virtual Machine and .Net CLR.
 <p>

<ul>
<li> Garbage collection. Automatic storage management plays a prominent role
     in most modern languages; it is one of the single most important features
     that makes programming easier.
<p>

The Basic problem in garbage collection: given a piece of memory, are there
any pointers to it?  (And if so, where exactly are all of them please).
Approaches:

<ul>
<li> reference counting
<li> traversal of known pointers (marking)
	<ul>
	<li> copying (2 heaps approach)
	<li> compacting (mark and sweep)
	<li> generational
	</ul>
<li> conservative collection
</ul>
Note that there is a fine 
<A href="http://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf">
paper</A> presenting a "unified theory of garbage collection</A>

<li> Reflection.  Modern languages' values can often describe themselves.
     This plays a central role in Visual GUI builders and Visual IDE's,
     component architectures and other uses.

<li> Just-in-time compilation.  Modern languages often have a virtual machine
     model...and a compiler built-in to the VM that converts VM instructions
     to native code for frequently executed methods or code blocks.

<li> Security model.  Modern languages may attempt to guarantee certain
     security properties, or prevent certain kinds of attacks.


</ul>


Goal-directed programs have an activation tree each instant, due to
suspended activations that may be resumed for additional results.  The
lifetime view is a sort of multidimensional tree, with three types of nodes.
<p>

<p>

<h4>Activation Records</h4>

Activation records organize the stack, one record per method/function call.

<table border>
<tr><td><td>return value
<tr><td><td>parameter
<tr><td><td>...
<tr><td><td>parameter
<tr><td><td>previous frame pointer (FP)
<tr><td><td>saved registers
<tr><td><td>...
<tr><td>FP--&gt;<td>saved PC
<tr><td><td>local
<tr><td><td>...
<tr><td><td>local
<tr><td><td>temporaries
<tr><td>SP--&gt;<td>...
</table>

At any given instant, the live activation records form a chain and
follow a stack discipline.  Over the lifetime of the program, this
information (if saved) would form a gigantic tree.  If you remember
prior execution up to a current point, you have a big tree in which
its rightmost edge are live activation records, and the non-rightmost
tree nodes are an execution history of prior calls.
<p>



<!--<h3> Reference Implementation Files </h3>

Here are some files containing example code you may use in your compiler
project.  Note that understanding and using someone else's code may require
just as much or more work than writing it yourself, and that no warranty
is expressed or implied: you are responsible for your compiler working,
even if it uses code from these files and it turns out they have a bug!

<ul>
<li> <A href="hw4/main.c">main.c</A>
<li> <A href="hw4/tree.h">tree.h</A>
<li> <A href="hw4/type.h">type.h</A>
<li> <A href="hw4/sym.h">sym.h</A>
<li> <A href="hw4/sym.c">sym.c</A>
<li> <A href="hw4/semantic.c">semantic.c</A>
</ul>-->

<p>
<font size=1> <A name=30>lecture #30</A> began here</font>
<p>

<h3> Semantics Things Checked </h3>

<dl>
<dt> extern
<dd> this word does not appear in 120++
<dt> static
<dd> this word does not appear in 120++
<dt> default constructor
<dd> this is used (without discussion) in an example in 120++
     Your semantic rule would be: if no constructor is present,
     insert a default constructor into the symbol table for a class.
<dt> delete
<dd> this is used in a trivial way on a simple pointer variable.
     Your typecheck rule would be: its operand must be a pointer.
</dl>

The 120++ Manual has been updated with these and various related
semantics topics. Keep the questions coming.

<h3> More HW2 Feedback </h3>

<dl>
<dt> Hash tables have collisions
<dt> Some far more readable than others
</dl>

<h3> Reduction in Typecheck Work </h3>

<dl>
<dt>
120++ does not use pointer arithmetic
<dd> so no pointer + integer.
<dt> 120++ does not mention any type promotion
<dd> so no char + integer or integer + double.
</dl>


<h3> Midterm Exam Review </h3>

The Midterm will cover lexical analysis, finite automatas, context free
grammars, syntax analysis, parsing, and semantic analysis.
For semantic analysis, you will only be asked questions based on lecture
and reading, not questions based on coding.<p>


Q: What is likely to appear on the midterm?
<p>

A: questions that allow you to demonstrate that you know
<ul>
<li> regular expressions
<li> the difference between an DFA and an NFA
<li> lex and flex and tokens and lexical attributes
<li> the %union and yylval interface between flex and bison
<li> context free grammars:
   ambiguity, factoring, removing left recursion, etc.
<li> bison syntax and semantics
<li> parse trees
<li> semantic attributes, type checking
</ul>


<p>

Sample problems:

<ol>
<li> Write a regular expression for numeric quantities of U.S. money
     that start with a dollar sign, followed by one or more digits.
     Require a comma between every three digits, as in $7,321,212.
     Also, allow but do not require a decimal point followed by two
     digits at the end, as in $5.99
<li> Use Thompson's construction to write a non-deterministic finite
     automaton for the following regular expression, an abstraction
     of the expression used for real number literal values in C.<pre>
     (d+pd*|d*pd+)(ed+)? </pre>
<li> Write a regular expression, or explain why you can't write a
     regular expression, for Modula-2 comments which use (* *) as
     their boundaries.  Unlike C, Modula-2 comments may be nested,
     as in (* this is a (* nested *) comment *)
<li> Write a context free grammar for the subset of C expressions
     that include identifiers and function calls with parameters.
     Parameters may themselves be function calls, as in f(g(x)),
     or h(a,b,i(j(k,l)))
<li> What are the FIRST(E) and FOLLOW(T) in the grammar: <pre>
     E : E + T | T
     T : T * F | F
     F : ( E ) | <b>ident</b></pre>
<li> What is the &epsilon;-closure(move({2,4},b)) in the following NFA?
     That is, suppose you might be in either state 2 or 4 at the time
     you see a symbol b: what NFA states might you find yourself in
     after consuming b?<br> (<em>automata to be written on the board</em>)
</ol>


<p>
<font size=1> <A name=31>lecture #31</A> began here</font>
<p>

<H3> Mailbag </h3>

<dl>
<dt> Does 120++ really require comma-separated lists of variables in
     declarations?  It would be <em>so</em> much easier if it only did one
     variable per declaration.
<dd> Soule's text does have examples like
<pre>
int tempx, tempy;
</pre>
So maybe we should talk about how hard that's going to be.
An init_declarator_list that consisted of just an IDENTIFIER leaf might
be easier, I admit, but an init_declarator_list that is (essentially)
just a linked list of identifiers isn't that much harder. Write a helper
function that does nothing but walk through init_declarator_list chains.
Start the walk from the simple_declaration (and/or any similar non-terminals
where a list of variables are being declared).
Pass type information obtained from synthesizing the decl_specifier_seq
as a parameter if you want -- this is a perfectly fine way of managing
the implementation of an inherited attribute.

<dt> I am not sure what to do with endl, cout, and cin.  I've checked that
 namespace std appears and that iostream is included, but I'm not sure what
 type to give them.  Should I mark them as methods... or perhaps class
 names?
<dd> There are two answers: "what these really are in C++" and
     "what our subset 120++ would find adequate".
     As you may recall, you are always allowed to do things more C++-ish
     than the toy behavior I will recommend.
<dt> OK so what about endl?
<dd> Really: according to cplusplus.com endl is an "IO manipulator" that
     inserts a newline and flushes the stream.
     What 120++ could live with: insert into your global symbol table the
      equivalent of having seen:
<pre>
const char endl = '\n';
</pre>
<dt> And what about cin and cout?
<dd> Really: these are are predefined global symbols of type ostream and
    istream. What 120++ could live with: CS 120 does not
    distinguish ostream from ofstream, or istream from ifstream.
    Insert into your global symbol table the equivalent of:
<pre>
ofstream cout;
ifstream cin;
</pre>
<dt> Doesn't that beg the question of what to insert for these predefined
     classes?
<dd> Its worse than that. 120++ does not go into operator overloading, but
     we need &lt;&lt; and &gt;&gt; to be predefined to work on them. Table A.5
     in Soule's appendix, also found in our 120++ reference manual, mentions
     a few methods defined on streams.

<dt> What about the class name string?
<dd> I would guess we need to predefine the class string, like we do
ofstream and ifstream.  I don't think defining it as char * will work,
unless 120++ never actually uses methods of class string, and only
passes them as parameters.
</dl>

<h3> Thoughts on the Predefined Classes </h3>

One way would be to write actual source code for (the tiny subset of) these
     classes that we need, and feed it into your compiler as if it were an
     #include.  Another way would be to just execute code that performs the
     corresponding symbol table inserts and type constructors.

<pre>
class ifstream {
   public:
      ignore();
   }
class ofstream {
   }
</pre>

<h3> On Type-checking of &lt;&lt; </h3>

Consider

<pre>
   cout &lt;&lt;"Player"&lt;&lt;current_player&lt;&lt;endl;
</pre>

We deduce:
<ul>
<li> &lt;&lt; requires a stream on its left side
<li> &lt;&lt; accepts string, char *, int, or double on its right side
       (but not: ... what?)
<li> &lt;&lt; is left-associative
<li> &lt;&lt; produces a value of type ifstream as its result
</ul>

<p>
<font size=1> <A name=32>lecture #32</A> began here</font>
<p>

<h3> Followup on Semantic Questions </h3>

Tables 1.4 and 1.5 of the 120++ manual present Dr. Soule's reference
on built-in classes such as string, ifstream, and ofstream.
Chapter 4 of the <A href="120pp.pdf">120++ Manual</A> summarize
what I am finding in actual code examples in the 120 book.  I am
still checking and still welcome reports and questions.

<h3> Deadlines?  Extensions? </h3>

I have been asked by a couple folks for extensions on HW#3.
If you allow three weeks for intermediate code and three weeks
for final code generation, that leaves us one week of slippage to
divide amongst semantic analysis, intermediate code generation,
and final code generation.


<A name="codegen">
<h3> Intermediate Code Generation </h3>
</A>

Goal: list of machine-independent instructions for each procedure/method
in the program.  Basic data layout of all variables.
<p>

Can be formulated as syntax-directed translation
<ul>
<li> add new semantic attributes where necessary. For expression E we might have
<dl>
<dt>E.place
<dd> the name that holds the value of E
<dt> E.code
<dd> the sequence of intermediate code statements evaluating E.
</dl>
<li> new helper functions, e.g.
<dl>
<dt><code>newtemp()</code>
<dd> returns a new temporary variable each time it is called
<dt><code>newlabel()</code>
<dd> returns a new label each time it is called
</dl>
<li> actions that generate intermediate code formulated as semantic rules
</ul>

<table border>
<tr><th>Production</th><th>Semantic Rules</th></tr>

<tr>
<td>S -&gt; id ASN E  <td> S.code = E.code || gen(ASN, id.place, E.place)
<tr>
<td>E -&gt; E1 PLUS E2 <td> E.place = newtemp();<br>
			    E.code = E1.code || E2.code || gen(PLUS,E.place,E1.place,E2.place);

<tr>
<td>E -&gt; E1 MUL E2 <td> E.place = newtemp();<br>
			    E.code = E1.code || E2.code || gen(MUL,E.place,E1.place,E2.place);

<tr>
<td>E -&gt; MINUS E1 <td> E.place = newtemp();<br>
			    E.code = E1.code || gen(NEG,E.place,E1.place);

<tr>
<td>E -&gt; LP E1 RP <td> E.place = E1.place;<br>
			  E.code = E1.code;

<tr>
<td>E -&gt; IDENT <td> E.place = id.place;<br>
			  E.code = emptylist();

</table>

<A name="tac">
<h4> Three-Address Code </h4>
</A>

Basic idea: break down source language expressions into simple pieces that:
<ul>
<li> translate easily into real machine code
<li> form a linearized representation of a syntax tree
<li> allow us to check our own work to this point
<li> allow machine independent code optimizations to be performed
<li> increase the portability of the compiler
</ul>
<p>

<b>Instruction set:</b>

<table>
<tr><th>mnemonic<th>C equivalent</th><th> description
<tr><th> ADD, SUB,MUL,DIV <th>x := y op z</th><td> store result of binary operation on y and z to x
<tr><th> NEG<th>x := op y </th><td> store result of unary operation on y to x
<tr><th> ASN<th>x := y </th><td> store y to x
<tr><th>ADDR<th>x := &y </th><td> store address of y to x
<tr><th>LCONT<th>x := *y </th><td> store contents pointed to by y to x
<tr><th>SCONT <th>*x := y </th><td> store y to location pointed to by x
<tr><th>GOTO<th>goto L </th><td> unconditional jump to L
<tr><th>BLT,...<th>if x rop y then goto L </th><td> binary conditional jump to L
<tr><th>BIF<th>if x then goto L </th><td> unary conditional jump to L
<tr><th>BNIF<th>if !x then goto L </th><td> unary negative conditional jump to L
<tr><th>PARM<th>param x </th><td> store x as a parameter
<tr><th>CALL<th>call p,n,x </th><td> call procedure p with n parameters, store result in x
<tr><th>RET<th>return x </th><td> return from procedure, use x as the result
</table>
<p>

<b>Declarations (Pseudo instructions):</b>

These declarations list size units as "bytes"; in a uniform-size environment
offsets and counts could be given in units of "slots", where a slot (4 bytes
on 32-bit machines) holds anything.

<table>
<tr><th>global x,n1,n2</th><td>declare a global named x at offset n1 having n2 bytes of space
<tr><th>proc x,n1,n2</th><td>declare a procedure named x with n1 bytes of parameter space and n2 bytes of local variable space
<tr><th>local x,n</th><td>declare a local named x at offset n from the procedure frame. This is optional but it would allow you to use names in your
three-address instructions to denote the offset. Beware scope.
<tr><th>label Ln</th><td>designate that label Ln refers to the next instruction
<tr><th>end</th><td>declare the end of the current procedure
</table>



<h3>TAC Adaptations for Object Oriented Code</h3>

<table>
<tr><th>x := y field z</th><td>lookup field named z within y, store address to x
<tr><th>class x,n1,n2</th><td>declare a class named x with n1 bytes of class variables and n2 bytes of class method pointers
<tr><th>field x,n</th><td>declare a field named x at offset n in the class frame
<tr><th>x := new Foo,n</th><td>create a new instance of class named x, store
address to x.  Constructure is called with n parameters (previously pushed
on the stack).
</table>

<p>

<p>
<font size=1> <A name=33>lecture #33</A> began here</font>
<p>

<h3> Discussions from the E-mail </h3>

<dl>
<dt> Prototypes verses Function Definitions
<dd> Prototypes insert something into a global symbol table, enough to
     typecheck any calls to the prototyped function.  They do not need
     a local symbol table, and would normally ignore names of parameters.
     They typically would have a boolean flag or other means of remembering
     in the global symbol table that they are just a prototype, so that
     they would not trigger a semantic error when the definition of that
     function finally shows up.  When the function definition occurs on a
     function that has an existing prototype, it should trigger a unique
     typecheck on the number and type of existing parameters.
</dl>

<h3> Variable Reference, Dereference, and Assignment Semantics </h3>

Given, say, x having a value of 2, what does the following compute?

<pre>
   int y = x + (x = x + 1) + x;
</pre>

OK, what about

<pre>
   int y = x + x + (x = x + 1) + x;
</pre>

In order to get the answers right, one has to understand the moment at which
a variable reference is computed versus the moment at which it is dereferenced
to obtain its value, versus the moment at which it is assigned a new value.
<p>

Operator precedence (and parentheses) determine what order the expressions
are evaluated.  But evaluating something as simple as <em>expr</em>+<em>expr</em>




<h3> Variable Allocation and Access Issues</h3>

Given a variable name, how do we compute its address?
<dl>
<dt> globals
<dd> easy, symbol table lookup
<dt> locals
<dd> easy, symbol table gives offset in (current) activation record
<dt> objects
<dd> Is it "easy"? If no virtual semantics*, symbol table gives offset in
     object, activation record has
     pointer to current object in a standard location. (<em>This is the
     reason C++ does not use virtual semantics by default</em>.)
     <br>
     For virtual semantics, generate code to look up offset
     in a table at runtime, based on the current object's type/class.
<dt> locals in some enclosing block/method/procedure
<dd> ugh.  Pascal, Ada, and friends offer their own unique kind of pain.
     Q: does the current block support recursion?  Example: for procedures
     the answer would be yes; for nested { { } } blocks in C the answer
     would be no.
<ul>
<li> if no recursion, just count back some number of frame pointers based
     on source code nesting
<li> if recursion, you need an extra pointer field in activation record
     to keep track of the "static link", follow static link back some
     # of times to find a name defined in an enclosing scope
</dl>

<h3> What are "Virtual" Semantics? </h3>

C++ is (just about) the only major object-oriented language
that has to compete with C in the performance arena. For this
reason, it chose early on to be different than every other
OO language. By default, if you are working on a class Foo
object, you can find Foo's member variables and call Foo's
methods by compile-time-determinable memory offsets and
addresses.  So a class is basically no worse than a struct to
generate code for.
<p>

If you say the keyword "virtual" in C++, or if you use just
about any other OOP language, subclassing and interfacing
semantics mean that the address referred to by o.x or o.m()
has to be calculated at runtime by looking up o's actual
class, using runtime type information.





<h4> Sizing up your Regions and Activation Records </h4>

Add a size field to every symbol table entry. Many types are not required
for your C445 project but we might want to discuss them anyhow.<br>

<ul>
<li> The size of chars is 1. We could make them use an alignment of 8,
     but then arrays of char would be all...wrong.<br>
<li> The size of integers is 8 (for x86_64; varies by CPU).<br>
<li> The size of reals is... 8 (for x86_64 doubles; varies by CPU).<br>
<li> The size of strings is... 8? (varies by CPU and language)
<li> The size of arrays is (sizeof (elementype)) * the number of elements.
      In static languages.
   The size of arrays/lists in dynamic languages might be more complicated.
<li> what about sizes of structs/objects?  They are the size of the sum of
     their members... after adding in padding to meet alignment requirements.
</ul>
<p>

You do this sizing up once for each scope.  The size of each scope is the
sum of the sizes of symbols in its symbol table.


<h4> On Trees and Attributes </h4>

This sounds like it is semantic analysis talk, but it is just as much
about intermediate code generation.

<dl>
<dt> main problem in semantic analysis and intermediate code generation:
<dd> Move Information Around the Tree.
<dt> Moving info up the tree
<dd> easy, follows the pattern used to build the tree in the first place.
<dt> to move the information down the tree
<dd> write tree traversal functions
</dl>

<h4> Tree Traversals for Moving Information Around </h4>

<ul>
<li> NOT a "blind" traversal that does the same thing at each node.
<li> often a switch statement
<li> switch cases are the grammar rules used to build each node
<li> often does different work depending on what
	nonterminal AND what grammar rule a given node represents.
</ul>

Alternative: depending on how you like gigantic recursive functions
consisting of gigantic switch statements, an alternative is
to write each traversal as a suite of mutually-recursive functions that know
how to do work for each different rule or each different type of
non-terminal node for that traversal.


<h4> Traversal code example </h4>

The following code sample illustrates a code generation tree traversal.
Note the gigantic switch statement.  A student once asked the question
of whether the link lists might grow longish, and if one is usually appending
instructions on to the end, wouldn't a naive link list do a terrible
O(n<sup>2</sup>) job.  To which the answer was: yes, and it would be good
to use a smarter data structure, such as one which stores both the head
and the tail of each list.

<pre>
void codegen(nodeptr t)
{
   int i, j;
   if (t==NULL) return;

   /*
    * this is a post-order traversal, so visit children first
    */
   for(i=0;i&lt;t-&gt;nkids;i++)
      codegen(t-&gt;child[i]);

   /*
    * back from children, consider what we have to do with
    * this node. The main thing we have to do, one way or
    * another, is assign t-&gt;code
    */
   switch (t-&gt;label) {
   case PLUS: {
      t-&gt;code = concat(t-&gt;child[0].code, t-&gt;child[1].code);
      g = gen(PLUS, t-&gt;address,
              t-&gt;child[0].address, t-&gt;child[1].address);
      t-&gt;code = concat(t-&gt;code, g);
      break;
      }
   /*
    * ... really, a bazillion cases, up to one for each
    * production rule (in the worst case)
    */
   default:
      /* default is: concatenate our children's code */
      t-&gt;code = NULL;
      for(i=0;i&lt;t-&gt;nkids;i++)
         t-&gt;code = concat(t-&gt;code, t-&gt;child[i].code);
   }
}
</pre>





<h4> Run Time Type Information </h4>

Some languages would need the type information around at runtime; for
example, dynamic object-oriented languages.  Its almost the case that one
just writes the type information, or symbol table information that includes
type information, into the generated code in this case, but perhaps one
wants to attach it to the actual values held at runtime.

<pre>
struct descrip {
   short type;
   short size;
   union {
      char *string;
      int ival;
      float rval;
      struct descrip *array;
      /* ... for other types */
      } value;
   };
</pre>


<p>
<font size=1> <A name=34>lecture #34</A> began here</font>
<p>

<h4>Mailbag </h4>

<dl>
<dt>
The problem is, there's just so much to type check (I mean, literally
everything has a type!); can you suggest any ways to go about this in a
quicker manner, or anything in the aforementioned list that could be
pruned/ignored?
<dd>
<ul>
<li> Not literally. The expression grammar.
     And the subset of declarations that you must support.
<li>
     The type checking will typically not happen on $$=$1 rules, so
     the expression grammar has around 18 productions where type checking
     goes.
<li> Feel free to rewrite the grammar to reduce the number of productions
     where you do type checking.
<li> Type checking rules for like-minded operators are identical; use that.
<li> Write helper functions, share common logic.
<li> You may aggressively unsupport operators not used in 120++
<li> The 120++ manual mentions about 24 of C++'s ~35 operators.
</ul>
</dl>

<h4> Compute the Offset of Each Variable </h4>

Add an address field to every symbol table entry.
The address contains a region plus an offset in that region.
No two variables may occupy the same memory at the same time.

<h4> Locals and Parameters are not Contiguous </h4>

For each function you need either to manage two separate regions
for locals and for parameters, or else you need to track where
in that region the split between locals and parameters will be.

<h3> Basic Blocks </h3>

Basic blocks are defined to be sequence of 1+ instructions in which
there are no jumps into or out of the middle.  In the most extreme
case, every instruction is a basic block.  Start from that perspective
and then lump adjacent instructions together if nothing can come between
them.<p>

What are the basic blocks in the following 3-address code?
("read" is a 3-address code to read in an integer.)

<pre>
	read x
	t1 = x > 0
	if t1 == 0 goto L1
	fact = 1
	label L2
	t2 = fact * x
	fact = t2
	t3 = x - 1
	x = t3
	t4 = x == 0
	if t4 == 0 goto L2
	t5 = addr const:0
	param t5		; "%d\n"
	param fact
	call p,2
	label L1
	halt
</pre>


<h4> Discussion of Basic Blocks </h4>

<dl>
<dt> Basic blocks are often used in order to talk about
     specific types of optimizations.
<dd> For example, there are optimizations that are only safe to do
     within a basic block, such as "instruction reordering for superscalar
     pipeline filling".
<dt> So, why introduce basic blocks here?
<dd> our next topic is intermediate code for control flow, which includes
     gotos and labels, so maybe we ought to start thinking in terms of
     basic blocks and flow graphs, not just linked lists of instructions.
<dt> view every basic block as a hamburger
<dd> it will be a lot easier to eat if you
     sandwich it inside a pair of labels:
<pre>
	label START_OF_BLOCK_7031
	<em>...code for this basic block...</em>
	label END_OF_BLOCK_7031
</pre>
<dt> the label sandwich lets you:
<dd> <ul>
     <li> target any basic block as a control flow destination
     <li> skip over any basic block
     </ul>
      For example, for an if-then statement, you may need to jump to
      the beginning of the statement in the then-part...or you may need
      to jump over it, the choice depending on the outcome of a boolean.
</dl>

Yeah, these lecture notes repeat themselves about the label sandwich, almost
immediately. That must be on purpose.


<h4> C Operators </h4>

In case you were fuzzy on the operators you need to support:

<table border><tr><th> Essential <th> Non-essential
<tr><td> = <td> += -= *= /= %= &lt;&lt;= &gt;&gt;= &amp;= ^= |=
<tr><td> + - * / % <td> &gt;&gt; &lt;&lt; ++ -- ^
<tr><td> &amp;&amp; || ! <td> &amp; | ~
<tr><td> &lt; &lt;= &gt; &gt;= == != <td> ternary x ? y : z
<tr><td> expr[expr] <td> &amp;x x-&gt;y *x x.y
</table>

<h4>Intermediate Code for Control Flow </h4>

Code for control flow (if-then, switches, and loops) consists of
code to test conditions, and the use of goto instructions and
labels to route execution to the correct code.  Each chunk of code
that is executed together (no jumps into or out of it) is called
a <em>basic block</em>.  The basic blocks are nodes in a control flow graph,
where goto instructions, as well as falling through from one basic block
to another, are edges connecting basic blocks.
<p>

Depending on your source language's semantic rules for things like
"short-circuit" evaluation for boolean operators, the operators
like || and && might be similar to + and * (non-short-circuit) or
they might be more like if-then code.
<p>
<p>
<font size=1> <A name=35>lecture #35</A> began here</font>
<p>

A general technique for implementing control flow code:
<ul>
<li> add new attributes to tree nodes to hold labels that denote the
possible targets of jumps.
<li>  The labels in question are sort of
analogous to FIRST and FOLLOW
<li>for any given list of instructions
corresponding to a given tree node,
add a .first attribute to the tree to hold the label for the beginning
of the list, and a .follow attribute to hold the label for the next
instruction that comes after the list of instructions.
<li>The .first attribute can be easily synthesized.
<li> The .follow attribute must be
inherited from a sibling.
</ul>



The labels have to actually be allocated and attached to instructions
at appropriate nodes in the tree corresponding to grammar production
rules that govern control flow.  An instruction in the middle of a
basic block need neither a first nor a follow.

<table border>
<tr><th>C code<th>Attribute Manipulations
<tr><td>S-&gt;if E then S<sub>1</sub><td>E.true = newlabel();<br>
					E.false = S.follow;<br>
					S<sub>1</sub>.follow = S.follow;</br>
					S.code = E.code || gen(LABEL, E.true)||<br>
					&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					S<sub>1</sub>.code
<tr><td>S-&gt;if E then S<sub>1</sub> else S<sub>2</sub>
<td>E.true = newlabel();<br>
    E.false = newlabel();<br>
	S<sub>1</sub>.follow = S.follow;</br>
	S<sub>2</sub>.follow = S.follow;</br>
	S.code = E.code || gen(LABEL, E.true)||<br>
					&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	S<sub>1</sub>.code || gen(GOTO, S.follow) ||<br>
					&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	gen(LABEL, E.false) || S<sub>2</sub>.code
</table>

Exercise: OK, so what does a while loop look like?

<!--
<h4> BASIC </h4>

As a student reported in class, Color BASIC supports statements in
the bodies of THEN and ELSE as an alternative to a line number to GOTO.
Indeed, it supports colon-separated lists of statements.<p>

This means the discussion in class about generating labels for the
starts and ends of then-parts and else-parts applies to BASIC, not
just C.<p>

BASIC also supports boolean AND, OR and NOT operators, and computes them
as integer (0 = FALSE, -1 = TRUE) but they are not short-circuit so the
handling of them is identical to (and simpler than) arithmetic operators
such as + and -.  Since we have not considered them up to now, and they
do not introduce interesting new challenges but instead are just handled
the same as required operators, Booleans are OPTIONAL EXTRA CREDIT for
your homework #4 and 5.  Don't bother with them unless everything else
in your assignment is finished.<p>
-->

<p>

<p>
<font size=1> <A name=36>lecture #36</A> began here</font>
<p>


<h3> More on Generating Code for Boolean Expressions </h3>

 
Last time we looked at code generation for control structures such as if's
and while's.  Understanding the big picture on these requires an
understanding of how to generate code for the boolean expressions that
control these constructs.

<h4> Comparing Regular and Short Circuit Control Flow </h4>

Different languages have different semantics for booleans.
<ul>
<li> Pascal treat them as identical to arithmetic operators
    (other than computing a true or a false result)
<li> C (and many others) specify "short-circuit"
evaluation in which operands are not evaluated once the answer to
the boolean result is known.
<li> Some ("kitchen-sink" design) languages have two sets of
     boolean operators: short circuit and non-short-circuit.
     (Does anyone know a language that has both?)
</ul>
<p>

Implementation techniques for these alternatives include:

<ol>
<li> treat boolean operators same as arithmetic operators, evaluate
     each and every one into temporary variable locations.
<li> <b>add extra attributes</b> to keep track of code locations that are
     targets of jumps. The attributes store link lists of those instructions
     that are <em>targets to backpatch</em> once a destination label is known.
     Boolean expressions' results evaluate to jump instructions and program
     counter values (where you get to in the code implies what the boolean
     expression results were).
<li> one could change the machine execution model so it implicity routes
     control from expression failure to the appropriate location.  In
     order to do this one would
     <ul>
     <li> mark boundaries of code in which failure propagates
     <li> maintain a stack of such marked "expression frames"
     </ul>
</ol>

<h4> Non-short Circuit Example</h4>

<pre>
a&lt;b || c&lt;d && e&lt;f
</pre>
translates into
<pre>
100:	if a&lt;b goto 103
	t<sub>1</sub> = 0
	goto 104
103:	t<sub>1</sub> = 1
104:	if c&lt;d goto 107
	t<sub>2</sub> = 0
	goto 108
107:	t<sub>2</sub> = 1
108:	if e&lt;f goto 111
	t<sub>3</sub> = 0
	goto 112
111:	t<sub>3</sub> = 1
112:	t<sub>4</sub> = t<sub>2</sub> AND t<sub>3</sub>
	t<sub>5</sub> = t<sub>1</sub> OR t<sub>4</sub>
</pre>

<h4> Short-Circuit Example </h4>

<pre>
a&lt;b || c&lt;d && e&lt;f
</pre>
translates into
<pre>
	if a&lt;b goto L1
	if c&lt;d goto L2
	goto L3
L2:	if e&lt;f goto L1
L3:	t = 0
	goto L4
L1:	t = 1
L4:	...
</pre>

Note: L3 might instead be the target E.false; L1 might instead be E.true;
no computation of a 0 or 1 into t might be needed at all.
<p>


<h4> While Loops </h4>

So, a while loop, like an if-then, would have attributes similar to:

<table border>
<tr><th>C code<th>Attribute Manipulations
<tr><td>S-&gt;while E do S<sub>1</sub><td>E.true = newlabel();<br>
					E.false = S.follow;<br>
					S<sub>1</sub>.follow = E.first;</br>
					S.code = gen(LABEL, E.first) ||<br>&nbsp;&nbsp;&nbsp;E.code || gen(LABEL, E.true)||<br>
					&nbsp;&nbsp;&nbsp;S<sub>1</sub>.code || <br>
					&nbsp;&nbsp;&nbsp;gen(GOTO, E.first)
</table>



C for-loops are trivially transformed into while loops, so they pose no new
code generation issues.

<!--
<h4> BASIC FOR Loops </h4>

Is the same true for a BASIC FOR loop?  If not,
So how is a FOR loop different from C while loop?
Well, for one thing, there are two separate statements (FOR and NEXT)
that are matched together only by the variable name that controls the loop.

<p>
Example BASIC FOR-loop:
<pre>
10 FOR I = 1 to 10
20 PRINT I
30 NEXT I
</pre>

One possible 3-address code equivalent for this is:
<pre>
L10:				; line 10
	asn G:24 C:1		; I = 1
L10A:				; "end of line 10" == loop test
	bgr G:24 C:10 L30A	; How did we decide to go to end of 30?
L20:				; line 20
	param G:24		; push I onto stack for print
	call  print,1		; call print
L30:				; line 30
	add G:24 G:24 C:1	; I = I + 1
	goto L10A		; go to end of line 10
L30A:
	...
</pre>

There is an obvious question, which is how do the FOR and the NEXT find
each other?  Minimally, one might be looking at writing tree traversal
code to match up FOR and NEXT statements.
-->
<!--
<p>
<h3> FOR loops, continued </h3>

Last time we considered generating code for simple FOR-loops.
In reality FOR loops are stickier than this; FOR really isn't like
a C-style for- or while-loop.
There can be several NEXT statements corresponding to a given
FOR statement.  A FOR statement cannot determine a destination
to which to branch when the loop is finished; it never fails.
If you execute the following lines, BASIC prints a "2":
<pre>
10 FOR I = 2 TO 1
20 PRINT I
30 NEXT I
</pre>

Instead, a NEXT statement must not only do the increment, but also
test the loop's exit condition, and either branch back up to the
top of the loop (if the loop is not finished) or fall through.
In order to communicate between a FOR and any of several NEXT's,
use <em>variables</em>.  As I see it, you need about two variables
to do FOR loops: one variable to store: what code region address
to goto to jump back to do the next iteration of the loop, and
one to store the upper limit of the loop (for STEP versions of FOR
you need a third variable to hold the increment).  With these two
variables, the simple FOR loop looks more like:
<p>
<pre>
L10:				; line 10
	asn G:24 C:1		; I = 1
	asn G:28 L10A
	asn G:32 C:10
	asn G:36 C:1
L10A:				; "end of line 10" == place for NEXT to go
L20:				; line 20
	param G:24		; push I onto stack for print
	call  print,1		; call print
L30:				; line 30
	add G:24 G:24 G:36	; I = I + 1
	bleq G:24 G:32 G:28	; if i <= 10 goto L10A
L30A:
	...
</pre>


<!--
<h4> Note on BASIC's INPUT statement </h4>

By the way, Color BASIC's INPUT statement
has a convenient, optional, PRINT-like prompt clause.  Instead of
<pre>
10 PRINT "Enter your Mastercard Number: "
20 INPUT M$
</pre>
you can write:
<pre>
10 INPUT "Enter your Mastercard Number: "; M$
</pre>
This is shown in Chapter 11 of the Color BASIC book.
-->

<h3> Code generation for Switch Statements </h3>

Consider the C switch statement
<pre>
switch(e) of {
   case v<sub>1</sub>:
      S<sub>1</sub>;
   case v<sub>2</sub>:
      S<sub>2</sub>;
   ...
   case v<sub>n-1</sub>:
      S<sub>n-1</sub>;
   default:
      S<sub>n</sub>;
}
</pre>

The intermediate code for this might look like:

<table><td>
<pre>
	<em>code for e, storing result in temp var t</em>
	goto Test
L<sub>1</sub>:
	<em>code for S<sub>1</sub>
L<sub>2</sub>:
	<em>code for S<sub>2</sub>
	...
L<sub>n-1</sub>:
	<em>code for S<sub>n-1</sub>
L<sub>n</sub>:
	<em>code for S<sub>n</sub>
	goto Next
Test:
	if t=v<sub>1</sub> goto L<sub>1</sub>
	if t=v<sub>2</sub> goto L<sub>2</sub>
	...
	if t=v<sub>n-1</sub> goto L<sub>n-1</sub>
	goto L<sub>n</sub>
Next:
</pre>
<td>
Note that C "break" statements<br>
are implemented in S<sub>1</sub>-S<sub>n</sub><br>
by "goto Next" instructions.
<br><br><br><br><br><br><br><br><br><br>
</table>


<p>
<font size=1> <A name=37>lecture #37</A> began here</font>
<p>

<h3> Brief Followup on Boolean-Integer Compatibility </h3>

Professor Soule's text <em>almost</em> cleanly separates bools and integers,
but not quite: he uses an int variable to hold a boolean value in one
example, and then uses
<pre>
   if (i) ...
</pre>
and suggests in a homework
exercise a feature that would extend it to
<pre>
if (i &amp;&amp; anotherbooleancondition...) ...
</pre>

<h3> Intermediate Code Generation Examples </h3>

Consider the following small program.  It would be fair game as input
to your compiler project.  In order to show blow-by-blow what the code
generation process looks like, we need to construct the syntax tree and
do the semantic analysis steps.
<p>

<table border>
<tr>
<td>
<pre>
void print(int i);
void main()
{
   int i;
   i = 0;
   while (i < 20)
      i = i * i + 1;
   print(i);
}
</pre>
<td>
<img src="codgen_tree.png">
</table>

<p>
<font size=1> <A name=38>lecture #38</A> began here</font>
<p>
<h3> Additional discussion of code generation </h3>

The tree was revised to be more legible, and nonterminal names were
changed more closely resemble the Sigala grammar.

<p>
<font size=1> <A name=39>lecture #39</A> began here</font>
<p>

<h3> Grammar Tweak: Casting and Implicit Type Conversion </h3>

<ul>
<li> you have been told that professor Soule's code examples do not include
      type casts, or at least, no implicit conversions.
<li> One of you e-mailed me a correction of that statement awhile back.
<li> It is true that Soule's book uses implicit type conversion from int to
     double, <em>at least for the constant 0</em>.  He compares a double to
     0 (bad idea), and returns a 0 from a function that returns type double.
<li> Professor Soule's book also mentions implicit and explicit type
     conversion, including an example of the new-style cast syntax, int(d)
     for some double value.
<li> I had previously removed some of Sigala's grammar rules related to the
     new-style cast syntax due to reduce-reduce conflicts.
<li> I revised the reference 120gram.y to include new-style cast syntax for
     the scalar numeric built-in types.
<li> However, you are not required to do anything more with casts than is
     already stated in the 120++ Manual.
</ul>



<!--
Here is an example BASIC program to compile:
<pre>
10	i = 0
20	IF i &gt;= 20 THEN 50
30	i = i * i + 1
40	GOTO 20
50	PRINT i
</pre>

This program corresponds to the following syntax tree, which a
successful homework #5 would build.  Note that it has a height of
approximately 10, and a maximum arity of approximately 4.  Also: your
exact tree might have more nodes, or slightly fewer; as long as the
information and general shape is there, such variations are not a problem.
<p>
<img src="syntree.jpg">
<em> A syntax tree, with attributes obtained from lexical and semantic
analysis, needs to be shown here.</em>
-->

<p>

The code for the boolean conditional expression controlling the while
loop is a list of length 1, containing the instruction t0 = i < 20,
or more formally

<table border>
<tr><th>opcode<th>dest<th>src1<th>src2</tr>
<tr><td>LT<td>t0<td>i<td>20</tr>
</table>
<p>

The actual C representation of addresses dest, src1, and src2 is a

<table border><td>region<br><hr>offset</table> pair, so the
picture of this intermediate code instruction really looks something like this:

<p>

<table border>
<tr><th>opcode<th>dest<th>src1<th>src2</tr>
<tr><td>LT<td>local<br><hr>t0.offset<td>local<br><hr>i.offset<td>const<br><hr>20</tr>
</table>

<p>

Regions are expressed with a simple integer encoding like:
global=1, local=2, const=3.
Note that address values in all regions are offsets from the start of the
region, except for region "const", which
stores the actual value of a single integer as its offset.

<p>

<table border>
<tr><th>opcode<th>dest<th>src1<th>src2</tr>
<tr><td>MUL<td>local<br><hr>t1.offset<td>local<br><hr>i.offset<td>local<br><hr>i.offset</tr>
</table>

<p>

<!-- Using this same representation, the next instruction found by bottom-up
traversal of the tree is -->

<p>
<font size=1> <A name=40>lecture #40</A> began here</font>
<p>


<h3>Code generation examples</h3>

Let us build one operator at a time.  You should implement your
code generation the same way, simplest expressions first.<p>

Zero operators.<p>

<pre>
if (x) S
</pre>
translates into

<pre>
if x != 0 goto L1
goto L2
label L1
...code for S
label L2
</pre>


or if you are being fancy

<pre>
if x == 0 goto L1
...code for S
label L1
</pre>
I may do this without comment in later examples, to keep them short.
<p>

One relational operator.<p>

<pre>
if (a &lt; b) S
</pre>
translates into

<pre>
if a &gt;= b goto L1
...code for S
label L1
</pre>

One boolean operator.<p>

<pre>
if (a &lt; b  &&  c &gt; d) S
</pre>
translates into

<pre>
if (a &lt; b)
   if (c &gt; d)
      ...code for S
</pre>
which if we expand it

<pre>
if a &gt;= b goto L1
if c &lt;= d goto L2
...code for S
label L2
label L1
</pre>

by mechanical means, we may wind up with lots of labels for the same
target (instruction), this is OK.<p>


<pre>
if (a &lt; b  ||  c &gt; d) S
</pre>
translates into

<pre>
if (a &lt; b) ...code for S
if (c &gt; d) ...code for S
</pre>
but its unacceptable to duplicate the code for S!  It might be huge!
Generate labels for boolean-true-yes-we-do-this-thing, not just for
boolean-false-we-skip-this-thing.

<pre>
if a &lt; b goto L1
if c &gt; d goto L2
goto L3
label L2
label L1
...code for S
label L3
</pre>

<h4> Object-Oriented Changes to Above Examples </h4>

The previous examples were assuming a C-like language semantics.
For an object-oriented language, the generated code for these examples
gets more interesting.  For example, the semantics of
<pre>
if (x) S
</pre>
if x is an object, may be defaulted to be equivalent to
<pre>
if (x != NULL) S
</pre>
or more generally, the different types may have (hardwired, or overrideable)
conversion rules to convert them to booleans for use in tests, such as
<pre>
tempvar := x.as_boolean()
if (tempvar) S
</pre>

<h4> Array subscripting!  </h4>

So far, we have only said, if we passed an array as a parameter we'd have to
pass its address.  3-address instructions have an "implicit dereferencing
semantics" which say all addresses' values are fetched / stored by default.
So when you say t1 := x + y, t1 gets values at addresses x and y, not the
addresses.  Once we recognize arrays are basically a pointer type, we need
3-address instructions to deal with pointers.  <p>

now, what about arrays?  reading an array value: x = a[i].  Draw the
picture.  Consider the machine uses byte-addressing, not word-addressing.
Unless you are an array of char, you need to multiply the subscript index
by the size of each array element...

<pre>
t0 := addr a
t1 := i * 4
t2 := plus t0 t1
t3 := deref t2
x  := t3
</pre>

What about writing an array value?

<p>

There are similar object-oriented adaptation issues for arrays: a[i]
might not be a simple array reference, it might be a call to
a method, as in
<pre>
x := a.index(i)
</pre>
or it might be implemented like:
<pre>
x := a field i
</pre>

The main issue to keep straight in both the C-like example and the
object-oriented discussion is: know when an instruction constructs an
address and stores an address in a memory location. When you want to
read or write to the address pointed to by the constructed address,
you may need to do an extra level of pointer-following. Three address
instructions have "implicit" pointer-following since all addresses are
followed when reading or writing memory, but if what is in the address
is another address, you have to be careful to keep that straight.

<p>

<!--
Consider the Ct "table" type for a moment.  We have said before now:
one could generate code for tables either by extending the three-address
instruction set with new instructions, or by generating function calls.
How might you implement

<dl>
<dt> table construction: table x
<dt> table insert: x[s] = s2
<dt> table lookup: s = x[s2]
</dl>
-->


<h3> Supplemental Comments on Code Generation for Arrays </h3>

In order to generalize from our example last lecture,
the 3-address instructions for
<pre>
expr [ expr ]
</pre>
ideally should generate code that computes an address that can
subsequently be read from or written to. One can certainly write
a three address instruction to compute such an address.
With arrays this is pointer arithmetic.
<!--
With tables, the main wrinkle is: what to do
if the key is not in the table?  The behavior might be different
for reading a value or writing a value:

<table border>
<tr>
<th> syntax <th> behavior
<tr>
<td> t[x] := y <td> if key is not in table, insert it
<tr>
<td> y := t[x] <td> if key is not in table, one of:
<ul>
<li> produce a default value
<li> raise an exception
<li> ??
</ul>
</table>
<p>
-->

<!--
The remainder of class was spent working out the details of assembling the
code generation for the entire while loop from our example before: <p>

<img src="intermed.jpg">
-->
<p>


<h3> Debugging Miscellany </h3>

Prior experience suggests if you are having trouble debugging, check:

<dl>
<dt> makefile .h dependencies!
<dd> if you do not list makefile dependencies for important .h files,
     you may get coredumps!
<dt> traversing multiple times by accident?
<dd> at least in my version, I found it easy to accidentally re-traverse
     portions of the tree. this usually had a bad effect.
<dt> bad grammar?
<dd> our sample grammar was adapted from good sources, but don't assume its
     impossible that it could have a flaw or that you might have messed it up.
<dt> bad tree?
<dd> its entirely possible to build a tree and forget one of your children
<!--
<dd> cocogram.y was adapted from an old BASIC grammar and from the available
     books on color computer BASIC.  But, it is certain that it still
     has bugs (missing pieces).  Our goal is not to do the whole of Color
     BASIC, but if there is a bug in the reasonable subset we've defined,
     fix it.
-->
</dl>

<h3> A few observations from Dr. D</h3>

I went shopping for more intermediate code examples, and while I didn't find
anything as complete as I wanted, I did find updated notes from the same
Jedi Master who trained me, check it:
<p>
<A href="IntermediateCodeGeneration.pdf">Dr. Debray's Intermediate Code Generation notes</A>.


<p>

You can consider these a recommended supplemental reading material, and we
can scan through them to look and see if they add any wrinkles to our prior
discussion.


<p>
<font size=1> <A name=41>lecture #41</A> began here</font>
<p>

<h3> A Bit of Skeletal Assistance with Three Address Code </h3>

<ul>
<li> <A href="tac.h">tac.h</A>
<li> <A href="tac.c">tac.c</A>
<li> <A href="codegen.c">codegen.c</A>
</ul>

<h3>Intermediate Code Generation for Classes and OO </h3>

<ul>
<li> OO CodeGen varies a lot depending on the OO language semantics.
<li> Lecture notes with ideas relevant to Java, ActionScript, or Unicon
may not do things identically to what a C++ subset needs.
<li> For 120++ we are needing to invent some stuff.
<li> Maybe some new 3 address opcodes/instructions, for example.
<li> Next section is for C++ subset, 120++
<li> More general OO considerations deferred to later
</ul>
<p>

Consider the following simplest possible C++ class example program:

<pre>
#include &lt;iostream&gt;
using namespace std;
class pet {
  private:
     int happy;
  public:
      pet() { happy = 50; }
      void play() { cout << "Woof!\n"; happy += 5; }
};
int main()
{
    pet pet1;
    pet1.play();
    return 0;
}
</pre>

What are the code generation issues?
<br><br><br><br><br><br><br><br><br><br><br><br>

Did we get:
<ul>
<li> allocation
<li> initialization via constructor
<li> method calling
<li> member variable referencing
</ul>

<p>
<font size=1> <A name=42>lecture #42</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt>For class definitions, how do we size them?
<dd>  For example, on class definitions I've marked them as prototypes in my code.
<dt> Do I need to give them a region/offset before I create an instance?
<dd>  My thought is to give them a size, but only assign a region/offset to an instance of the class.  Does this sound right?
<dt>
Does the size of a class function include the size of the private members that are declared inside the class but used inside the function?
<dt>
How do I designate float constants?
<dd>
  My thought is that they cannot be treated like integer constants, but I'm unsure of where to go from there.
</dl>

<h3> Object Allocation </h3>

<dl>
<dt> memory allocation of an object is similar to other types.
<dd> it can be in the global, local (stack-relative) or heap area
<dt> the # of bytes (size) of the object must be computed from the class.
<dd> each symbol table should track the size of its members
<dt> for a global or local object, add its byte-count size requirement
     to its containing symbol table / region.
<dd> effectively, no separate code generation for allocation
<dt> translate a "new" expression into a malloc() call...
<dd> plus for all types of object creation, a constructor
     function call has to happen.
</dl>

<h3> Initialization via Constructor </h3>

<ul>
<li> A major difference between objects and, say, integers, is that
     objects have constructors.
<li> Constructor, like all other member functions, takes a 0th parameter
     that is a pointer to the object instance.  Could be implemented as
     a register variable, similar to %ebp procedure frame pointer.
<li> For a local, the object variable declaration translates into a
     constructor function call that happens at that point in the code body.
     Just catenate .code into the linked list.
<li> For a "new" object, the constructor function call happens right after
     the (successful) call to allocate the object.
<li> For a global, how do we generate code for its constructor call?
     When does it execute? ... Good news, everyone!  120++ almost
     does not support globals at all, and only has integer globals.
</ul>

<h3> Method Invocation </h3>

Now let's discuss how to generate code for <p>

<code>o.f(arg1,...,argN)</code>

<ul>
<li> In 120++ it's just a method invocation.

<li>
The 120++ case: o's class C is known at compile time and methods are
non-virtual.
You can generate <code>C__f(&o, arg1, ..., argN)</code>.

<li> Note the flip side of this: when you generate code for the member
function body, you do the same name mangling, and add the same extra
one-word "this" parameter to the symbol table.

</ul>

<h3> Member variable references </h3>

<dl>
<dt> inside a member function, i.e. access member variable x.
<dd> Handle like with arrays, by allocating a new temporary variable
     in which to calculate the address of this->x.  Take the address
     in the "this" variable and add in x's offset as given in the
     symbol table for this's class.
<dt> outside an object, o.x.  120++ almost does not do this btw.
<dd> Handle as above, using o's address instead of this's.
     You would also check o's class to make sure x is public.
</dl>



<h3> OOP code generation for more dynamic OO languages </h3>

<ul>
<li>
In a real OO language, for o.f(...) you'd do semantic analysis to know
whether f is a method of o's class, or a member variable that happens
to hold a function pointer
<li> Non 120++ case: What if f is a method that C inherits from some superclass S?

<li> Non 120++ case: o's class not known at compiled time and/or methods
are virtual. Calculate at runtime which method f to use for o.
What are your options???

</ul>

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
Your brilliant suggestions should have included: insert function pointers
for all methods into the instance.<p>

Now let's consider a simple real-world
example.  Class TextField, a small, simple GUI widget. A typical
GUI application might have many textfields on each of many dialogs; many
instances of this class will be needed. <p>

The source code for TextField
is only 767 lines long, with 17 member variables and 47 member functions.
But it is a subclass of class Component, which is a subclass of three other
classes...by the time inheritance is resolved, we have 44 member variables,
and 149 member functions.  If we include function pointers for all methods
in the instance, 77% of instance variable slots will be these function
pointers, and these 77% of the slots will be identical/copied for all
instances of that class.
<p>

The logical thing to do is to share a single copy of the function pointers,
either in a "class object" that is an instance of a meta-class, or more
minimally, in a struct or array of function pointers that might be
called (by some) a <em>methods vector</em>.



<h3> Methods Vectors </h3>

Suppose you have class A with methods f(), g(), and h(), and class B
with methods e(), f(), and g().  Suppose further that you have code
that calls method f(), that is designed to work either either A or B.
This might happen due to polymorphism, interfaces, subclassing, virtual
methods, etc.  The kicker will be that in order to generate code for
o.f(), a runtime lookup will be performed to obtain
the function/method pointer associated with symbol f.

Instead a separate structure (the "methods vector") is allocated and
shared by all the instances of a given class.  In this case, o.f()
becomes o.__methods__.f(o)




<p>
<font size=1> <A name=43>lecture #43</A> began here</font>
<p>

<h3> Minor note on HW#4 </h3>

Correction to the HW#4 specification: no paper copy is needed, just an
electronic turnin of a .tar file in UNIX <code>tar(1)</code> format please.
Note: not gzipped tar, just tar.


<A name="tacordie">
<h3> TAC or die trying </h3>
</A>

We need a simple example, in which you see

<ul>
<li> Systematic traversal to populate explicit symbol table
<li> Systematic traversal to assign .place (populate implicit symbols)
<li> Systematic traversal to assign .first/.follow/.true/.false
<li> Finally, build linked list of TAC instructions (.code)
</ul>

It is easy to spend too much class time on
front-end stuff before getting to a too-short and still under-explored
TAC code generation phase. Our Goal:
<ul>
<li> manage a slightly larger ("interesting") example
<li> with syntax and semantic analysis already done
<li> for which we can go more blow by blow through the intermediate code
generation.
</ul>

The perfect example would include a few statements, expressions,
control flow constructs, and function calls.  Here is an attempt:

<pre>
void printf(char *, int);
int fib(int i);
int readline(char a[]);
int atoi(char a[]);
int main() {
   char s[64];
   int i;
   while (readline(s)!=0 && s[0]!='\004') {
      i = atoi(s);
      if (i <= 0) break;
      printf("%d\n", fib(i));
      }
}
</pre>

Note that this is a C language example.

<!--
<pre>
package {
    public class fibber {
	public function fib(n : int): int {
	   if (n <= 1) return 1
	   else return fib(n-1) + fib(n-2)
	}
	public function fibber() {
            var s: String
	    var i: int
            while (s = read()) {
	       i = int(s)
	       if (i <= 0) break
	       trace(fib(i))
	       }
	}
    }
}
</pre>
-->

For this exercise, use a syntax tree, not a parse tree
(generally, no internal nodes with only one child).
<p>




We omit from the tree, tokens that are simply punctuation
and reserved words needed for syntax.  After that simplification, the
syntax tree is (greatly) further reduced.  Note that in real life I
might not want to remove <em>every</em> unary tree node, some of them
have associated semantics or code to be generated.
<p>

<img src="tac-example.png">
<p>

Using <A href="cgram.y">cgram.y</A> nonterminal names, let's focus on
code generation for the main procedure.

<p>
<font size=1> <A name=44>lecture #44</A> began here</font>
<p>

<h3> TAC-or-die: the First-level </h3>

<em>Potentially, this is a separate pass after labels have been generated
per the last class.</em>
<p>

<table
<tr><td>
<img src="tac-example.png" width=500 height=768>
<td>

The first tree node that TAC code hits in its bottom up traversal is
IDENT<sub>readline</sub> (no .code), followed by IDENT<sub>s</sub> (no .code).
Working up to the function call (postfix_expr), I realize that one of those
non-terminals-with-only-one-child matters and needs to be in the tree:
argument_expression_list.  Each time it churns out an actual parameter,
TAC code generates a PARAM instruction to copy the value of the parameter
into the parameter region. PARAM8 indicates an 8-byte (long) parameter,
as opposed to the default 4-byte (int).
<p>
<pre>
	ADDR   loc:68,loc:0
	PARAM8 loc:68
</pre>
<p>

The postfix_expr is a function call, whose TAC codegen rule should say:
allocate a temporary variable t<sub>0</sub> (or as we called it: LOC:72)
for the return value, and generate a CALL instruction

<pre>
	CALL readline,1,loc:72
</pre>

The next leaf (ICON<sub>0</sub>) has no .code, which brings code generation
up to the <code>!=</code> operator. Here the code depends on
the .true (L5) and .false (L2) labels.  The TAC code generated is

<pre>
	BNE loc:72,const:0,lab:5
	GOTO lab:2
</pre>

After that, the postfix traversal works over to IDENT<sub>s</sub> (no .code),
ICON<sub>0</sub> (no .code), and up to the postfix expression for the subscript
operator for <code>s[0]</code>.  It needs to generate .code that will place
into a temporary variable (its .place, loc:80) what s[0] is.<p>

The basic expression for a[i] is baseaddr + index * sizeof(element).
sizeof(element) is 1 in this case, so we can
just add baseaddr + index.  And index is 0 in this case, so an optimizer
would make it all go away.  But we aren't optimizing by default, we are
trying to solve the general case.  Calling temp = newtemp() we get a new
location (loc:88) to store index * sizeof(element)
<pre>
	MUL	loc:88,const:0,const:1
</pre>
We want to then add that to the base address, but
<pre>
	ADD	loc:96,loc:0,loc:88
</pre>
would add the (word) <em>contents</em> of s[0-7].  Instead, we need
<pre>
	ADDR	loc:96,loc:0
	ADD	loc:96,loc:96,loc:88
</pre>
A label L5 needs to be prefixed into the front of this:
<pre>
	LABEL	lab:5
</pre>

</table>

<p>
Note: an alternative to ADDR would be to define opcodes for reading and
writing arrays.  For example
<pre>
	SUBSC1   <em>dest</em>,<em>base</em>,<em>index</em>
</pre>
might be defined to read from base[index] and store the result in dest.
Similar opcodes for ASNSUB1, SUBSC8, and ASNSUB8 could be added that
assign to base[index], and to perform these operations for 8-byte elements.
Even if you do this, you may need the more general ADDR instruction for
arrays of arbitrary sized elements.

<p>
CCON<sub>^D</sub> has no .code, but the <code>!=</code> operator has
to generate code to jump to its .true (L4) or .false (L2) as in the previous
case.  Question: do we need to have a separate TAC instruction for
char compares, or sign-extend these operands, or what?  I vote: separate
opcode for byte operations.  BNEC is a branch if not-equal characters
instruction.

<pre>
	BNEC loc:76,const:4,lab:4
	GOTO lab:2
</pre>

The code for the entire local_and_expr is concatenated from its children:
<pre>
	ADDR   loc:72,loc:0
	PARAM8 loc:72
	CALL   readline,1,loc:80
	BNE    loc:80,const:0,lab:5
	GOTO   lab:2
	LABEL  lab:5
	MUL    loc:88,const:0,const:1
	ADDR   loc:96,loc:0
	ADD    loc:96,loc:96,loc:88
	BNEC   loc:96,const:4,lab:4
	GOTO   lab:2
</pre>

<p>
<font size=1> <A name=45>lecture #45</A> began here</font>
<p>

Tree traversal then moves over into the body of the while loop: its statements.
<p>

IDENT<sub>i</sub> has no .code.  The code for <code>atoi(s)</code> looks
almost identical to that for readline(s). The assignment to i tacks on
one more instruction:
<pre>
	ADDR   loc:104,loc:0
	PARAM8 loc:104
	CALL   atoi,1,loc:112
	ASN    loc:64,loc:112
</pre>

For the second statement in the while loop, the IF statement, there is
the usual conditional-followed-by-unconditional branch, the interesting
part is where they go.  The E.true should do the then-part (the break
statement) for which we generate a .first of lab:6.  The E.false should
go for whatever instruction follows the if-statement, for which lab:3
has been designated.

<pre>
	BLE    loc:64,const:0,lab:6
	GOTO   lab:3
</pre>

The then-part is a break statement. All then-parts will need to have a
label for their .first instruction, which in this case is a trivial GOTO,
but where does it go?

<pre>
	LABEL  lab:6
	GOTO   ??
</pre>

The <code>break</code> is a major non-local goto that even
the parent node (the if-statement) cannot know the target for, without
obtaining it from about 7 tree-nodes higher!  It is iteration_statement's
.first that is the target for <code>continue</code>, and its .follow is
the target for <code>break</code>.

<h3> Labels for <code>break</code> and <code>continue</code> Statements </h3>

<ul>
<li> label generation of .first and .follow isn't hard
<li> its the <em>propagating</em> of that information <em>way down</em>
     into the subtrees where it is needed that is a challenge.
<li> Option #1: add inherited attributes .loopfirst and
     .loopfollow to the treenodes. Use them to pass a loop's
     .first and .follow
     down into the "break" and "continue" statements that
     need them: 
<li> Option #2: write a specialized tree traversal whose first parameter is the
     tree (node) we are traversing, and whose second and third parameters
     are pointers to the label (struct address) of the nearest enclosing
     loop.  It would be called as <code>do_break(root, NULL, NULL);</code>
<li> Option #3: implement parent pointers within all the nodes of your
     tree, and walk up the parents until you find a loop node.
</ul>
<p>

Sample code for Option #2 is given below. Implied by the BREAK case is
the notion that the .place field for this node type will hold the label
that is the target of its GOTO. How would you generalize it to
handle other loop types, and the <code>continue</code> statement?
There may be LOTS of different production rules for which
you do something interesting, so you may add a lot of cases to this
switch statement.

<pre>
void do_break(nodeptr n, address *loopfirst, address *loopfollow)
{
   switch (n->prodrule) {
   case BREAK:
      if (loopfollow != NULL)
	 n->place = *loopfollow;
      else semanticerror("break with no enclosing loop", n);
      break;
   case WHILE_LOOP:
      loopfirst = &(n-&gt;loopfirst);
      loopfollow = &(n-&gt;loopfollow);
      break;
      ...
      }

   for(i=0; i<n->nkids; i++)
      do_break(nodeptr n, loopfirst, loopfollow);
}
</pre>

<h3> Back to the TAC-or-die example </h3>

So by one of options #1-3, we find the nearest enclosing iteration_statement's
.follow field says LAB:2. Note that since we have here a label target that
is itself a GOTO, an optimizer would chase back to the branch instructions
that go to label 6, and have them go to label 2, allowing us to remove this
instruction.  By the way, if there were an else statement, the
code generation for the then-part would include another GOTO (to skip over
the else-part) that we'd hopefully remove in optimization.

<pre>
	LABEL  lab:6
	GOTO   lab:2
</pre>

Having completed the then part, it is time to assemble the entire
if-statement:

<pre>
	BLE    loc:64,const:0,lab:6
	GOTO   lab:3
	LABEL  lab:6
	GOTO   lab:2
	LABEL  lab:3
</pre>

The next statement is a printf statement. We need to push the parameters
onto the stack and execute a call instruction.  The code will be: code
to evaluate the parameters (which are non-empty this time), code to push
parameters (in the correct order, from their .place values),
then the call.  <em>Question: does it matter whether the evaluations
all occur before the PARAM instructions, or could they (should they) be
interleaved?</em>
<p>

The code for parameter 1 is empty. Here is the code for parameter 2,
storing the return value in a new temporary variable.

<pre>
	PARAM8 loc:64
	CALL   fib,1,loc:120
</pre>

The code for the outer call is then

<pre>
	PARAM8 loc:64
	CALL   fib,1,loc:120
	PARAM8 sconst:0
	PARAM8 loc:120
	CALL   printf,2,loc:128
</pre>

Given this, whole while-loop's code can finally be assembled.  The while
prepends a label and appends a GOTO back to the while loop's .first field.
The whole function's body is just this while loop, with a procedure
header and a return statement at the end:

<pre>
proc main,0,128
	LABEL  lab:1
	ADDR   loc:72,loc:0
	PARAM8 loc:72
	CALL   readline,1,loc:80
	BNE    loc:80,const:0,lab:5
	GOTO   lab:2
	LABEL  lab:5
	MUL    loc:88,const:0,const:1
	ADDR   loc:96,loc:0
	ADD    loc:96,loc:96,loc:88
	BNEC   loc:96,const:4,lab:4
	GOTO   lab:2
	ADDR   loc:104,loc:0
	PARAM8 loc:104
	CALL   atoi,1,loc:112
	ASN    loc:64,loc:112
	BLE    loc:64,const:0,lab:6
	GOTO   lab:3
	LABEL  lab:6
	GOTO   lab:2
	LABEL  lab:3
	PARAM8 loc:64
	CALL   fib,1,loc:120
	PARAM8 sconst:0
	PARAM8 loc:120
	CALL   printf,2,loc:128
	GOTO   lab:1
	RETURN
</pre>


<p>
<font size=1> <A name=46>lecture #46</A> began here</font>
<p>


<a name="finalcode">
<h3> Final Code Generation </h3>
</a>

<ul>
<li> Goal: execute the program we have been translating, somehow.
<li> Note: in real life we would execute a major optimization phase
on the intermediate code, before generating final code.
</ul>

Alternatives for Final Code:
<dl>
<dt> interpret the source code
<dd> we could build an interpreter instead of a compiler, in which the
     source code was kept in string or token form, and re-parsed every
     execution. Early BASIC's did this, but it is Really Slow.
<dt> interpret the parse tree
<dd> we could have written an interpreter that executes the program
     by walking around on the tree doing traversals of various subtrees.
     This is still slow, but successfully used by many "scripting languages".
<dt> interpret the 3-address code
<dd> we could interpret the link-list or a more compact binary representation
     of the intermediate code
<dt> translate into VM instructions
<dd> popular virtual machines such as JVM or .Net allow execution from an
     instruction set that is often higher level than hardware, may be
     independent of the underlying hardware, and may be oriented toward
     supporting the specific language features of our source language.
     For example, there are various BASIC virtual machines out there.
<dt> translate into "native" instructions
<dd> "native" generally means hardware instructions.
</dl>

In mainstream compilers,
final code generation
<ol>
<li> takes a linear sequence of 3-address intermediate
code instructions, and
<li> translates each 3-address instruction into one or
more native instructions.
</ol>
<p>

The big issues in code generation are:
<dl>
<dt> (a) instruction selection, and
<dt> (b)
register allocation and assignment.
</dl>

<h4> Collecting Information Necessary for Final Code Generation </h4>

<dl>
<dt> Option #A: a top-down approach to learning your native target code.
<dd>
     Study a reference work supplied by the chip manufacturer, such
     as the Intel 80386 Programmer's Reference Manual
<dt> Option #B: a bottom-up approach to learning your native target code.
<dd>
     study an existing compiler's native code.  For example, running
     "gcc -S" for various toy C programs
     you can learn native instructions corresponding to each C construct,
     including ones equivalent to the various 3-address instructions.
</dl>


<h4>Instruction Selection</h4>

A modern CPU usually has many different sequences of instructions
that it could use to accomplish a given task.  Instruction selection
must choose a particular sequence.
<ul>
<li> how many registers are tied to particular instructions?
<li> is a special case instruction available for a particular
     computation?
<li> what addressing mode(s) are supported for a given instruction?
</ul>

Given a choice among equivalent/alternative sequences, the decision on which
sequence of instructions to use is usually based on estimates or
measurements of which sequence executes the <em>fastest</em>.

<ul>
<li> "fastest" is often approximated by the
number of memory references incurred during execution, including the
memory references for the instructions themselves.
<li>  picking the
<em>shortest</em> sequence of instructions is often a good approximation of the
optimal result, since fewer instructions usually translates into fewer
memory references.
</ul>

<p>

A good set of examples of instruction selection are to be
found in the <A href="http://www.stanford.edu/class/cs343/resources/superoptimizer.pdf">superoptimizer</A> paper. From that paper:

<ul>
<li> a longer instruction sequence may be faster if it avoids gotos
<li> sometimes the fastest sequence exploits specific constants in the
     operands and is really, really surprising.
</ul>


<h3> Register Allocation and Assignment </h3>

<ul>
<li> reading values in registers is much much faster than accessing main memory.
<li>
<em>Register allocation</em> denotes the selection of which variables
will go into registers.
<li>
<em>Register assignment</em> is the determination of exactly
which register to place a given variable.
<li> goal: minimize the total number of memory accesses required
by the program.
</ul>
<p>

<h4> The (register allocation) job changes as CPUs change </h4>

<ul>
<li>In the age of dinosaurs, Load-Store architectures featured
only one (accumulator) register.
Register allocation and assignment was moot.
<li>In the age of minis and micros, it was usually "easy", e.g.
traditional x86 had 4 registers instead of 1.
<li>Recent History features CPU's with 32 or more general purpose
registers.  On such systems,
high quality compiler register allocation and assignment makes a huge
difference in program execution speed.
<li> :-( btw, optimal register
allocation and assignment is NP-complete! Compilers must settle for
doing a "good" job.

<li>usually the # of variables at many given time exceeds the number
of registers available (the common case)
<li> variables may be used (slowly)
     directly from memory IF the instruction set supports
     memory-based operations.
<li>
When an instruction set does not support memory-based operations, all
variables must be loaded into a register in order to perform arithmetic
or logic using them.
</ul>
<p>

Even if an instruction set does support memory-based operations, most
compilers should load a value into a register while it is
being used, and then spill it back out to main memory when the register
is needed for another purpose.  The task of minimizing memory accesses
becomes the task of minimizing register loads and spills.

<p>

<h3> Code Generation Examples </h3>

<h4> Reusing a Register </h4>

Consider the statement:
<pre>
   a = a+b+c+d+e+f+g+a+c+e;
</pre>
A naive three address code generator would generate a
lot of temporary variables here, when really one big number is being added.
How many registers does the expression need?  Some variables
are referenced once, some twice.  GCC (32-bit) generates:
<p>

<pre>
	movl	b, %eax
	addl	a, %eax
	addl	c, %eax
	addl	d, %eax
	addl	e, %eax
	addl	f, %eax
	addl	g, %eax
	addl	a, %eax
	addl	c, %eax
	addl	e, %eax
	movl	%eax, a
</pre>

<p>

Now consider
<pre>
   a = (a+b)*(c+d)*(e+f)*(g+a)*(c+e);
</pre>
How many registers are needed here?
<pre>
	movl	b, %eax
	movl	a, %edx
	addl	%eax, %edx
	movl	d, %eax
	addl	c, %eax
	imull	%eax, %edx
	movl	f, %eax
	addl	e, %eax
	imull	%eax, %edx
	movl	a, %eax
	addl	g, %eax
	imull	%eax, %edx
	movl	e, %eax
	addl	c, %eax
	imull	%edx, %eax
	movl	%eax, a
</pre>

And now this:
<pre>
   a = ((a+b)*(c+d))+((e+f)*(g+a))+(c*e);
</pre>
which compiles to
<pre>
	movl	b, %eax
	movl	a, %edx
	addl	%eax, %edx
	movl	d, %eax
	addl	c, %eax
	movl	%edx, %ecx
	imull	%eax, %ecx
	movl	f, %eax
	movl	e, %edx
	addl	%eax, %edx
	movl	a, %eax
	addl	g, %eax
	imull	%edx, %eax
	leal	(%eax,%ecx), %edx
	movl	c, %eax
	imull	e, %eax
	leal	(%eax,%edx), %eax
	movl	%eax, a
</pre>



<p>
<font size=1> <A name=47>lecture #47</A> began here</font>
<p>

<h3> Brief Comparison of 32-bit and 64-bit x86 code </h3>

What can be gleaned from this side-by-side of 32-bit and 64-bit assembler
for a=a+b+c+d+e+f+g+a+c+e.
Note that the actual variable names are in the assembler because the variables
in question are globals.
<p>

<table border>
<tr>
<th> x86 32-bit
<th> x86_64
<tr>
<td>
<pre>
	movl	b, %eax
	addl	a, %eax
	addl	c, %eax
	addl	d, %eax
	addl	e, %eax
	addl	f, %eax
	addl	g, %eax
	addl	a, %eax
	addl	c, %eax
	addl	e, %eax
	movl	%eax, a
</pre>
<td>
<pre>
	movq	a(%rip), %rdx
	movq	b(%rip), %rax
	addq	%rax, %rdx
	movq	c(%rip), %rax
	addq	%rax, %rdx
	movq	d(%rip), %rax
	addq	%rax, %rdx
	movq	e(%rip), %rax
	addq	%rax, %rdx
	movq	f(%rip), %rax
	addq	%rax, %rdx
	movq	g(%rip), %rax
	addq	%rax, %rdx
	movq	a(%rip), %rax
	addq	%rax, %rdx
	movq	c(%rip), %rax
	addq	%rax, %rdx
	movq	e(%rip), %rax
	leaq	(%rdx,%rax), %rax
	movq	%rax, a(%rip)
</pre>
</table>

Should we be disappointed that the 64-bit code looks a lot longer?

<p>

The globals are declared something like the following.
<ul>
<LI><CODE>.comm</code> stands for data in a "common" (a.k.a. global data) section.
<li><code>.globl</code> and <code>.type</code> are used for functions, and are really part of
the function header before the function code starts.
</ul>
  If you
allocated your globals as a region, you might have
one .comm of 56 bytes named globals (or whatever) and give the
addresses of your globals as numbers such as <code>globals+32</code>.
Names are nicer but having to treat globals and locals very differently
is not.
<p>
<pre>
	.comm	a,8,8
	.comm	b,8,8
	.comm	c,8,8
	.comm	d,8,8
	.comm	e,8,8
	.comm	f,8,8
	.comm	g,8,8
	.text
.globl main
	.type	main, @function
</pre>


<h3> Brief Comparison of 64-bit x86 globals vs. locals </h3>

How does this difference inform, and affect, what we might want in
our three-address code?
<p>

<table border>
<tr>
<th> x86_64 (local vars)
<th> x86_64
<tr>
<td>
<pre>
	movq	-48(%rbp), %rax
	movq	-56(%rbp), %rdx
	leaq	(%rdx,%rax), %rax
	addq	-40(%rbp), %rax
	addq	-32(%rbp), %rax
	addq	-24(%rbp), %rax
	addq	-16(%rbp), %rax
	addq	-8(%rbp), %rax
	addq	-56(%rbp), %rax
	addq	-40(%rbp), %rax
	addq	-24(%rbp), %rax
	movq	%rax, -56(%rbp)
</pre>
<td>
<pre>
	movq	a(%rip), %rdx
	movq	b(%rip), %rax
	addq	%rax, %rdx
	movq	c(%rip), %rax
	addq	%rax, %rdx
	movq	d(%rip), %rax
	addq	%rax, %rdx
	movq	e(%rip), %rax
	addq	%rax, %rdx
	movq	f(%rip), %rax
	addq	%rax, %rdx
	movq	g(%rip), %rax
	addq	%rax, %rdx
	movq	a(%rip), %rax
	addq	%rax, %rdx
	movq	c(%rip), %rax
	addq	%rax, %rdx
	movq	e(%rip), %rax
	leaq	(%rdx,%rax), %rax
	movq	%rax, a(%rip)
</pre>
</table>

We then went into a discussion of parameters, introducing the
example

<pre>
#include <stdio.h>

long f(long,long,long);

int main()
{
   long rv = f(1, 2, 3);
   printf("rv is %d\n", rv);
}

long f(long a, long b, long c)
{
   long d, e, f, g;
   d = 4; e = 5; f = 6; g = 7;
   a = ((a+b)*(c+d))+(((e+f)*(g+a))/(c*e));
   return a;
}
</pre>

for which the generated code was

<pre>
	.file	"expr.c"
	.section	.rodata
.LC0:
	.string	"rv is %d\n"
	.text
.globl main
	.type	main, @function
main:
.LFB0:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	subq	$16, %rsp
	movl	$3, %edx
	movl	$2, %esi
	movl	$1, %edi
	call	f
	movq	%rax, -8(%rbp)
	movl	$.LC0, %eax
	movq	-8(%rbp), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$0, %eax
	call	printf
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE0:
	.size	main, .-main
.globl f
	.type	f, @function
f:
.LFB1:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	movq	%rdi, -48(%rbp)
	movq	%rsi, -56(%rbp)
	movq	%rdx, -64(%rbp)
	movq	$4, -40(%rbp)
	movq	$5, -32(%rbp)
	movq	$6, -24(%rbp)
	movq	$7, -16(%rbp)
	movq	-56(%rbp), %rax
	movq	-48(%rbp), %rdx
	leaq	(%rdx,%rax), %rcx
	movq	-40(%rbp), %rax
	movq	-64(%rbp), %rdx
	leaq	(%rdx,%rax), %rax
	imulq	%rax, %rcx
	movq	-24(%rbp), %rax
	movq	-32(%rbp), %rdx
	leaq	(%rdx,%rax), %rbx
	.cfi_offset 3, -24
	movq	-48(%rbp), %rax
	movq	-16(%rbp), %rdx
	leaq	(%rdx,%rax), %rax
	imulq	%rbx, %rax
	movq	-64(%rbp), %rdx
	movq	%rdx, %rbx
	imulq	-32(%rbp), %rbx
	movq	%rbx, -72(%rbp)
	movq	%rax, %rdx
	sarq	$63, %rdx
	idivq	-72(%rbp)
	leaq	(%rcx,%rax), %rax
	movq	%rax, -48(%rbp)
	movq	-48(%rbp), %rax
	popq	%rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1:
	.size	f, .-f
	.ident	"GCC: (GNU) 4.4.7 20120313 (Red Hat 4.4.7-3)"
	.section	.note.GNU-stack,"",@progbits
</pre>

We learned that (the first 6+) parameters are passed in registers, but
you can allocate local variable space for them, and copy them into
their local space, after which they can be treated exactly like other
locals.


<p>
<font size=1> <A name=48>lecture #48</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt> Do we need to specify the RET instruction at the end of a function
 or does the END instruction imply that the function returns?
<dd> I think of END as a non-instruction that marks the end of a procedure,
but there is no reason you could not define its semantics to also do a RET.

<dt> If we have nothing to return, can we just say RET with no parameter or
must the parameter x always be there, i.e. RET x?

<dd> I would accept a RET with no operand as a shorthand for RET const:0

<dt> Can you give me an example of when to use the GLOBAL and LOCAL
declaration instructions?

<dd> These are pseudo-instructions, not instructions.
Globals are listed as required; at the minimum, if your program has any
global variables you must have at least one GLOBAL declaration to give the
size of (the sum of) the global variables. You can do one big GLOBAL and
reference variables as offsets, or you can declare many GLOBAL regions,
effectively defining one named region for each variable and therefore
rendering the offsets moot.

<p>
The LOCAL pseudo-instruction is listed as optional and advisory; think of it
as debugging symbol information, or as an assist to the reader of your
generated assembler source.

<dt> What sort of type checking is needed for new?
<dd> <code>new</code>'s operand can be almost any type (what would be illegal
there?).
Its return type is a pointer to its operand type, and that pointer type
is type-checked against its enclosing expression, such as an assignment.

</dl>

<h3> "new" in final code FYI </h3>

<pre>
class C {
  private: long x, y;
  public:  C() { x=3; y=4; }
};

int main()
{
   C *a = new C;
}
</pre>

generates

<pre>
	.file	"new.cpp"
	.section	.text._ZN1CC2Ev,"axG",@progbits,_ZN1CC5Ev,comdat
	.align 2
	.weak	_ZN1CC2Ev
	.type	_ZN1CC2Ev, @function
_ZN1CC2Ev:
.LFB1:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	movq	%rdi, -8(%rbp)
	movq	-8(%rbp), %rax
	movq	$3, (%rax)
	movq	-8(%rbp), %rax
	movq	$4, 8(%rax)
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE1:
	.size	_ZN1CC2Ev, .-_ZN1CC2Ev
	.weak	_ZN1CC1Ev
	.set	_ZN1CC1Ev,_ZN1CC2Ev
	.text
.globl main
	.type	main, @function
main:
.LFB3:
	.cfi_startproc
	.cfi_personality 0x3,__gxx_personality_v0
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%rbx
	subq	$24, %rsp
	movl	$16, %edi
	.cfi_offset 3, -24
	call	_Znwm
	movq	%rax, %rbx
	movq	%rbx, %rax
	movq	%rax, %rdi
	call	_ZN1CC1Ev
.L5:
	movq	%rbx, -24(%rbp)
	movl	$0, %eax
	addq	$24, %rsp
	popq	%rbx
	leave
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE3:
	.size	main, .-main
	.ident	"GCC: (GNU) 4.4.7 20120313 (Red Hat 4.4.7-3)"
	.section	.note.GNU-stack,"",@progbits
</pre>

A short summary: the final code for a new calls a memory allocator
(nwm) whose return value (%rax) gets copied in as a parameter (%rdi)
to the constructor (N1CC1Ev), with an interesting side trip to %rbx.


<h4>Where we left off (LEAL!)</h4>

In the previous example, complicated arithmetic drove GCC to start
"leal'ing".
<ul>
<li>leal (load effective address) is a complex instruction usually used for
pointer arithmetic, i.e. its output is usually a pointer.
<li> due to x86 CISC addressing modes, leal can actually add two registers,
multiplying one of those registers by 1, 2, 4, or 8, and then adding
a constant offset in as well.  It is a "more than 3 address instruction".
<li> the instruction selection module of gcc knows it can be used for
addition.
<li>Unlike "add" instruction, it does not set the condition flag,
<li>This property might allow it to execute in parallel with some
other arithmetic operation that <em>does</em> use the condition flag.
</ul>
<p>

Lastly (for now) consider:
<pre>
   a = ((a+b)*(c+d))+(((e+f)*(g+a))/(c*e));
</pre>
The division instruction adds new wrinkles.  It operates on an implicit
register accumulator which is twice as many bits as the number you divide
by, meaning 64 bits (two registers) to divide by a 32-bit number.  Note
in this code that gcc would rather spill than use %ebx.  %ebx is
reserved by the compiler for some good reason such as to remember the
current procedure frame.  %edi and %esi are similarly ignored/not used.
<table border>
<tr>
<th> 32-bit <th> 64-bit
<tr>
<td>
<pre>
	movl	b, %eax
	movl	a, %edx
	addl	%eax, %edx
	movl	d, %eax
	addl	c, %eax
	movl	%edx, %ecx
	imull	%eax, %ecx
	movl	f, %eax
	movl	e, %edx
	addl	%eax, %edx
	movl	a, %eax
	addl	g, %eax
	imull	%eax, %edx
	movl	c, %eax
	imull	e, %eax
	movl	%eax, -4(%ebp)
	movl	%edx, %eax
	cltd
	idivl	-4(%ebp)
	movl	%eax, -4(%ebp)
	movl	-4(%ebp), %edx
	leal	(%edx,%ecx), %eax
	movl	%eax, a
</pre>
<td>
<pre>
	pushq	%rbx
	subq	$88, %rsp
	movq	$1, -72(%rbp)
	movq	$2, -64(%rbp)
	movq	$3, -56(%rbp)
	movq	$4, -48(%rbp)
	movq	$5, -40(%rbp)
	movq	$6, -32(%rbp)
	movq	$7, -24(%rbp)
	movq	-64(%rbp), %rax
	movq	-72(%rbp), %rdx
	leaq	(%rdx,%rax), %rcx
	movq	-48(%rbp), %rax
	movq	-56(%rbp), %rdx
	leaq	(%rdx,%rax), %rax
	imulq	%rax, %rcx
	movq	-32(%rbp), %rax
	movq	-40(%rbp), %rdx
	leaq	(%rdx,%rax), %rbx
	.cfi_offset 3, -24
	movq	-72(%rbp), %rax
	movq	-24(%rbp), %rdx
	leaq	(%rdx,%rax), %rax
	imulq	%rbx, %rax
	movq	-56(%rbp), %rdx
	movq	%rdx, %rbx
	imulq	-40(%rbp), %rbx
	movq	%rbx, -88(%rbp)
	movq	%rax, %rdx
	sarq	$63, %rdx
	idivq	-88(%rbp)
	leaq	(%rcx,%rax), %rax
	movq	%rax, -72(%rbp)
	movl	$.LC0, %eax
	movq	-72(%rbp), %rdx
	movq	%rdx, %rsi
	movq	%rax, %rdi
	movl	$0, %eax
	call	printf
	addq	$88, %rsp
	popq	%rbx
</pre>
</table>

In the 32-bit version, you finally see some register spilling.
In the 64-bit version, there is
<ul>
<li> saving a register so you can use it (%rbx)
<li> allocating a whole local region of 88 bytes
<li> storing immediate values into main memory
<li> addition by leaq'ing registers
</ul>


<h3> Three Kinds of Dependence </h3>

In all three of these examples, a dependence relationship implies that
in the program semantics, the second instruction depends on the first
one in some way.
<p>

<ul>
<li>  How are they different?
<li>How do these affect, e.g., decisions about which registers are in use?
<li>What about concurrency/superscalar CPU's ?
</ul>
<p>

<table border>
<td>
<pre>
a = b + c;
...
d = a + e;
</pre>
<td>
<pre>





a = b + c;
...
b = d + e;
</pre>
<td>
<pre>
















a = b + c;
...
a = d + e;
</pre>
</table>

<p>
<font size=1> <A name=49>lecture #49</A> began here</font>
<p>

<h3> Mailbag </h3>

<dl>
<dt> What do I have to do to get a "D"?

<dd> You are graded relative to your peers.  In previous semesters the
answer to this has been something like: pass the midterm and final, and
convince me that you really did semantic analysis.  If you failed the
midterm, you might want to try and do better on the final, and you might
want to get some three address code working.  Do you really want to settle
for a "D"?

<dt> I am confused about how to access private class members via the "this"
pointer.  I am unsure how to do the offsets from the "this" pointer in
three address code without creating a new symbol table for class instances.
<dd> The this pointer is a parameter; offsets relative to it are done via
pointer arithmetic.

<dt> Do you have an example that uses each of the pseudo instructions
     (global, proc, local, label, and end), so we
     know how these should be formatted?
<dd> No.  The pseudo instructions should have opcodes and three address
fields; their presence in the linked list of three address codes is the
same as an instruction. Their format when you print them out is not very
important since this is just intermediate code. But:
instructions are on a single line that begins with a tab character, and
pseudo instructions are on a single line that does not begin with a tab
character.

<dt> We have const that can hold an int/(int)char/boolean, a string region
for holding a string offset, but what should we do about float/double
values?

<dd> Real number constants have to be allocated space similar to strings.
They could either be allocated out of a separate "real number constant
region", or the constants of different types could all be allocated out of
the same region, with different offsets and sizes as needed. Note that
not all integer constants fit in instructions, so at least potentially
they may be allocated as static data also.

<dt> I did some research and it appears the .cfi* statements are used for
exception handling and you can get rid of them using the gcc flag
-fno-asynchronous-unwind-tables
<dd> Thank you!

</dl>

<h3> Brief Comment on HW Resubmissions </h3>

At various points in this course you have a choice between completing/fixing
a previous homework, or working on the next homework.  But sometimes you
have to complete/fix an old homework for the next one to be implementable.
In addition to your HW#4/#5, I will accept up to 2 old/late homework
resubmissions from you from now up until the end of dead week.  I will award
full credit for such late submissions.  Test your work; I won't be able to
just keep regrading it until it passes.


<h3> More on DIV instruction </h3>

When I looked for more, I found this
<A href="http://www.cs.uaf.edu/2009/fall/cs301/support/x86_64/index.html">
Cheat Sheet</A>, which pointed at the big books
(<A href="instructionsAM.pdf">A-M</A><A href="instructionsNZ.pdf">N-Z</A>).

<uL>
<li> The cheat sheet says div divides reg. ax by [src], ratio in ax, remainder in
dx.
<li> It also says dx must be 0 to start or you get a SIGFPE.
<li>  The big book
says your basic full-size divide instruction divides a big value stored in
a pair of registers (32 bit: EDX:EAX or 64 bit: RDX:RAX), by which point I
am thinking I have to give the general introduction to X86_64 before this
should be remotely understandable.
</ul>

<h3> Helper Function for Managing Registers </h3>

Define a <code>getreg()</code> function that returns a location L
to hold the value of x for the assignment <br>  <code>x := y op z</code>

<ol>
<li> if y is in a register R that holds the value of no other names,
AND y is not live after the execution of this instruction, THEN
return register R as your location L
<li> ELSE return an empty register for L if there is one
<li> ELSE if x has a next use in the block or op is an operator
     that requires a register (e.g. indexing), find an occupied
     register R. Store R into the proper memory location(s), update
     the address descriptor for that location, and return R
<li> if x is not used in the block, or no suitable occupied register
can be found in (3), return the memory location of x as L.
</ol>



<h3> Putting It All Together: A Simple Code Generator </h3>

<ul>
<li> Register allocation will be done only within a basic block.
     All variables that are live at the end of the block are stored
     in memory if not already there.
<li> Data structures needed:
<dl>
<dt> Register Descriptor
<dd> Keeps track of what is in each register. Consulted when a new register
is needed. All registers assumed empty at entry to a block.
<dt> Address Descriptors
<dd> Keep track of the location(s) where the curent value of a name can be
found at runtime. Locations can be registers, memory addresses, or stack
displacements. (can be kept in the symbol table).
</dl>
</ul>

<h3> Code Generation Algorithm </h3>

For each three-address statement of the form <br>
x := y op z

<ol>
<li> Use getreg() to determine location L where the result of the computation
y op z should be stored.
<li> Use the address descriptor for y to determine y', a current location for
y. If y is currently in a register, use the register as y'. If y is not already
in L, generate the instruction "MOV y',L" to put a copy of y in L.
<li> Generate the instruction "OP z',L" where z' is a current location for z.
Again, prefer a register location if z is currently in a register.
<p> Update the descriptor of x to indicate that it is in L. If L is a register,
update its descriptor to indicate that it contains x. Remove x from all other
register descriptors.
<li> If y and/or z have no next uses and are in registers, update the register
descriptors to reflect that they no longer contain y and/or z respectively.
</ol>


<h3> Register Allocation </h3>

Need to decide:
<ul>
<li> which values should be kept in registers (register allocation)
<li> which register each value should be in (register assignment)
</ul>

<h4> Approaches to Register Allocation </h4>

<ol>
<li> Partition the register set into groups that are use for different kinds
of values. E.g. assign base addrs to one group, pointers to the stack to
another, etc. <p>
Advantage: simple<br>
Disadvantage: register use may be inefficient
<li> Keep frequently used values in registers, across block boundaries. E.g.
assign some fixed number of registers to hold the most active values in each
inner loop.<p>
Advantage: simple to implement<br>
Disadvantage: sensitive to # of registers assigned for loop variables.
</ol>

<p>
<font size=1> <A name=50>lecture #50</A> began here</font>
<p>

<h3> End of Semester Planning </h3>

<ul>
<li> 
<li> Final Exam Review: Thursday December 11
<li> Homework #5 due: Friday December 12, 11:59pm
<li> Final Exam: Monday December 15, 3:00-5:00pm
<li> Compiler Demos: by appointment, Dec 15-19
</ul>

<h3> x86_64 Floating Point </h3>

<h4> Float Operations </h4>

There is <A href="http://web.cecs.pdx.edu/~apt/cs322/x86-64.pdf">
a useful set of notes</A> from Portland State University.
Arithmetic operations on floats
have different opcodes, and results have to be stored in
floating point registers, not integer registers.

<pre>
	movsd	-56(%rbp), %xmm0
	movapd	%xmm0, %xmm1
	addsd	-48(%rbp), %xmm1
</pre>

<h4> Float Constants </h4>

Doubles are the same 64-bit size as longs.  They can be loaded into memory
or registers using the normal instructions like movq.  A spectacular x86_64
opcode named movabsq takes an entire floating point constant as an immediate
(bit pattern given as a decimal integer!) and stores it in a register.

<pre>
	movabsq	$4620355447710076109, %rax
	movq	%rax, -8(%rbp)
</pre>

<h3> Simple Machine Model </h3>

This model is probably relevant for selecting between equivalent instructions
but is presented here as food for thought regarding which variables deserve
to stay in registers.

<dl>
<dt> Instruction Costs
<dd> for an instruction I, cost(I) = 1 + sum(cost(operands(I))) <p>
operand costs:
<ul>
<li> if operand is a register, cost = 0
<li> if operand is memory, cost = 1
</ul>
<br>
<dt> Usage Counts
<dd> In this model, each reference to a variable x accrues a savings of
1 if x is in a register.

<ul>
<li> For each use of x in a block that is <b>not preceded by an assignment</b>
in that block, savings = 1 if x is in a register.
<li> If x is live on exit from a block in which it is assigned a value,
and is allocated a register, then we can avoid a store instruction (cost = 2)
at the end of the block. <p>

Total savings for x ~ sum(use(x,B) + 2 * live(x,B) for all blocks B)
<p>

This is very approximate, e.g. loop frequencies are ignored.
</ul>
</dl>
<p>

<h4>Cost savings flow graph example</h4>

For the following flow graph, how much savings would be earned by leaving
variables (a-f) in a register across basic blocks?
<p>
<img src="liveness.png">

<table>
<tr><th> Savings <th> B1 <th> B2 <th> B3 <th> B4 <th> Total
<tr><td>    a	 <td> 2	 <td> 1	 <td> 1	 <td> 0	 <td> 4
<tr><td>    b	 <td> 2	 <td> 0	 <td> 2	 <td> 2	 <td> 6
<tr><td>    c	 <td> 1	 <td> 0	 <td> 1	 <td> 3	 <td> 3
<tr><td>    d	 <td> 3	 <td> 1	 <td> 1	 <td> 1	 <td> 6
<tr><td>    e	 <td> 2	 <td> 0	 <td> 2	 <td> 0	 <td> 4
<tr><td>    f	 <td> 1	 <td> 2	 <td> 1	 <td> 0	 <td> 4
</table>





<h3> x86_64 Discussion </h3>

<ul>
<li> <A href="http://www.cs.cmu.edu/~fp/courses/15213-s07/misc/asm64-handout.pdf">machine level programming</A>
<!--
<li> <A href="http://www.x86-64.org/documentation/assembly.html">Gentle Introduction to x86_64</A>-->
<li> 
<A href="code.html"> Dr. J's TAC-to-x86_64 templates </A> (not finished yet)
<li>AMD64
Architecture Programmer's Manual
(<A href="http://support.amd.com/us/Processor_TechDocs/24594_APM_v1.pdf">v1</A>
<A href="http://support.amd.com/us/Processor_TechDocs/24594_APM_v2.pdf">v2</A>
<A href="http://support.amd.com/us/Processor_TechDocs/24594_APM_v3.pdf">v3</A>
<A href="http://support.amd.com/us/Processor_TechDocs/24594_APM_v4.pdf">v4</A>)
<li>
<A href="http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html">Intel Developer Manuals</A>
</ul>


Side detour: per in-class request, we spent some quality time working in
class on another three-address code example.  I have placed the lecture
notes <A href="#tacordie">back in the intermediate code section</A>.
<p>

The main forward progress on that example this lecture was the discussion
of how to propagate "rare" inherited attributes such as labels used by
"break" and "continue" statements.


<h3> (old) E-mail Questions </h3>

I'm having trouble figuring out what TAC I need to generate for a function
definition.  For example, given the function 
<pre>int foo(char x){
   ...somecode
}</pre>

I'm having trouble understanding what code needs to be generated at this level.  I understand that there needs to be (at least) 1 label,at the very start (to be able to call the function).
<blockquote><font color=red>yes. well, in final code the procedure entry point
will include/become a label.</font></blockquote>
<p>

However, I'm having trouble understanding what code I would create for the int return, or to define the space available for parameters.

<blockquote><font color=red>
Returns are generally implicit. Int return type == no code.
Parameter space is actually allocated by the caller prior to the call,
but the procedure pseudoinstruction includes a declaration of how much
space it requires/assumes has been passed in to it.
For a procedure you will generally have to specify the amount of local variable space on the stack
that the procedure requires.  So the pseudoinstructions in
intermediate code that you use is
function. In your case:
<pre>
proc foo,1,<em>nbytes_localspace</em>
</pre></font></blockquote>

If I understand the return properly, I don't actually generate code at this node for the return.  It gets generated at the 'return' line in the body.
<blockquote><font color=red>Yes. There and at the end of any function
that falls off the end.  In final code the return statement will put a
return value in %eax and then jump
down to the end of the function to use its proper function-return assembler
instruction(s).
</font></blockquote>
<p>

So I guess the .place of char x is what is really getting me.  Do I really need to worry about it too much in TAC, because it is just 'local 0' (or whatever number gets generated)?
<blockquote><font color=red>I recommend you consider it (in TAC) to be region
PARAM offset 0.  That could be handled almost identically to locals in final
code, unless you use the fact that parameters are passed in registers...</font></blockquote>
<p>

Then I really end up worrying about it during final code since local 0 might actually be something like %rbp -1 or wherever the location on the stack parameters end up being located.
<blockquote><font color=red>By saying it is PARAM offset 0, the TAC code for
parameters is distinct enough from locals that they can be found to be at
a different location relative to the %rbp (positive instead of negative)
or passed in registers.</font></blockquote>

<h3> For what its worth on Windows 64 </h3>

Warning: the Mingw64 compiler (and possibly other Windows 64-bit c
compilers) do not use the same memory sizes as Linux x86_64!  Beware.
If you were compatible with gcc on Linux you might not be on Windows
and vice versa.


<!--
<h3> Register Allocation and Graph Coloring </h3>

<ul>
<li> A <em>vertex coloring</em> is an assignment of labels or colors
     to each vertex of a graph such that no edge connects two identically
     colored vertices. 
<li>
A <em>k-coloring</em> of a graph G is a vertex coloring that is an assignment of one of k possible colors to each vertex of G (i.e., a vertex coloring) such that no two adjacent vertices receive the same color. 
<li>
The classic "high-powered" way to do register allocation is by
k-coloring a special kind of dependence/liveness graph of variables.
To do that, one must do a bunch of analysis to
tell which variables are live at the same time.
</ul>

<h3> Register Allocation by Graph Coloring </h3>

When a register is needed but all registers are in use, a register 
has to be freed by storing its contents in memory ("spilling"). <p>

Graph coloring is a systematic way of register allocation and managing
spills. <p>

Graph coloring uses two passes:

<dl>
<dt> Pass 1</ht>
<dd> Target machine instructions are selected as though there are an
infinite number of symbolic registers.
<dt> Pass 2
<dd> Physical registers assigned to symbolic ones in a manner that minimises
the cost of spills. This is done by constructing a <em> register-interference
graph</em> for each procedure, then k-coloring this graph (k = # of registers).
</dl>

<p>


<h3> Register Interference Graphs </h3>

<ul>
<li> nodes: "symbolic registers"
<li> there is an edge connecting two nodes if one is live when the other is
defined
<li> if there are k assignable registers, then we have to try and k-color
the interference graph.
<li> k-colorability is NP-complete in general, but the following heuristic
is efficient in practice:
<ul>
<li> if a node n has less than k neighbors, remove n and its edges from the
graph to get a new graph G': a k-coloring of G' can easily be extended to a
k-coloring of the original graph.
<li> By repeating this process, we either get the empty graph (in which case
we can work backwards to produce a k-coloring of the original graph), or we
get a graph where each node has &gt;= k neighbors: in this case we need to
spill a node, modify the interference graph, and proceed as before.<p>

General rule for spills: avoid introducing code into inner loops.
</ul>
</ul>

<h3> Register Allocation Coloring Example </h3>

Courtesy of those fine folks at
<A href="http://pages.cs.wisc.edu/~cs701-1/NOTES/5.REGISTER-ALLOCATION.html#coloring">University of Wisconsin</A>.
<p>

Their way of thinking about register interference is to define
<em>live ranges</em> === set of all nodes (in a control flow graph)
between definitions and uses.
Actually, feels more like it works back from uses to their definitions,
and merges all overlaps on any given variable.
<p>

OK, so what live ranges occur in the following graph? <p>

<img src="ralloc.gif"><p>

Each live range is one node in a register interference graph, and edges
(denoting interference) connect nodes whose underlying CFG nodesets overlap.
-->

<h3> Review of x86_64 Calling Conventions </h3>

64-bit x86 was first done by AMD and licensed afterwards by Intel, so it
is sometimes referred to as AMD64.  Warning: Linux and Windows do things
a lot different!

<ul>
<li> <A href="http://en.wikipedia.org/wiki/Calling_convention">Calling conventions</A> in general
<LI> Section 3.2 of <A href="http://x86-64.org/documentation/abi.pdf">SysV ABI</A> gives Linux conventions.
<li> <A href="http://msdn.microsoft.com/en-us/library/ms235286(v=vs.80).aspx">X86_64 calling conventions from MSDN</A>
</ul>

<p>
<font size=1> <A name=51>lecture #51</A> began here</font>
<p>

looking for your homework #4 feedback? I brought printouts with me but am
not finished grading them. Feel free to stop by my office after class to
take a look at what yours looks like so far.

<h3> Final Code Generation Example </h3>

<ul>
<li> Guess what, in x86_64, locals and parameters in the same region!
     (except what's passed in registers)
<li> <A href="finalcg.icn">finalcg.icn</A>, a program that generates
     <A href="tac.s">native code</A>
<li> Assemble output with command line such as <code>as -o demo1.o demo1.s</code>
<li> needs streamlining, removal of exception directive code per
earlier discussion.
<li> compare previous with
     <A href="final-tac.icn">TAC-C</A>, whose output looks like
     <A href="final-tac-out.c">this</A>
<li> Explanation of
     <A href="http://www.logix.cz/michal/devel/gas-cfi/">CFI directives</A>
     (CFI stands for Call Frame Information)
</ul>

<p>

<h3> Lessons From the Final Code Generation Example </h3>

<ul>
<li> Native TAC-to-native-code not that hard; 110 lines netted about half
     the TAC instruction set in procedure final(); many other opcodes very
     similar.
<li> Most complexity centers around calls / returns
<li> Although you pass them in registers, IF YOU CALL ANYTHING, and
     IF YOU USE YOUR PARAMETERS AFTERWARDS, you will
     have to allocate space on the stack for your incoming parameters,
     and save their values to memory before reusing that register.
</ul>

<p>
<font size=1> <A name=52>lecture #52</A> began here</font>
<p>

(we briefly went back and examined the TAC-C version of the final code
generator, and then went on.)


<h3> About Flow Graphs </h3>

Some of what we say about optimization may well depend on additional
understanding of flow graphs.

<ul>
<li>  A flow graph is a graph in which vertexes are basic blocks
<li> There is a distinguished <em>initial</em> node, the basic block
whose leader is the first instruction.
<li> There is a directed edge from block B<sub>1</sub> to B<sub>2</sub> if:
<ol>
<li> There is a conditional or unconditional jump from the last statement
     of B<sub>1</sub> to the first statement of B<sub>2</sub>
<li> B<sub>2</sub> immediately follows B<sub>1</sub> in the order of the
     program, and B<sub>1</sub> does not end in an unconditional jump.
</ol>
</ul>

<h3> Flow Graph Example </h3>

<pre>
if (x + y &lt;= 10 &amp;&amp; x - y &gt;= 0) x = x + 1;
</pre>

Construct the flow graph from the basic blocks
<table border>
<tr><td><pre>

t<sub>1</sub> := x + y
if t<sub>1</sub> &gt; 10 goto L1

</pre>
<tr><td><pre>

t<sub>2</sub> := x - y
if t<sub>2</sub> &lt; 0 goto L1

</pre>
<tr><td><pre>

t<sub>3</sub> := x + 1
x := t<sub>3</sub>

</pre>
<tr><td><pre>

L1:

</pre>
</table>

<h3> Next-Use Information </h3>

<dl>
<dt> use of a name
<dd> consider two statements
<pre>
I1: x := ... /* assigns to x */
...
I2: ... := ... x ... /* has x as an operand */
</pre>
such that control <em>can</em> flow from I1 to I2 along some path that has no
intervening assignments to x.  Then, I2 <em>uses</em> the value of x
computed at I1. I2 may use several assignments to x via different paths.

<dt> live variables
<dd> a variable x is <em>live</em> at a point in a flow graph if the
value of x at that point is <em>used</em> at a later point.

</dl>

<h3> Computing Next-Use Information (within a block only)</h3>

<ul>
<li> assume we know which names are live on exit from the block
 (needs dataflow analysis; else assume all nontemporary variables
  are live on exit)
<li> scan backwards from the end of the basic block. For each statement
<pre>
 i:  x := y <em>op</em> z
</pre>
do:
<ol>
<li> attach to stmt. i the current information (from symbol table) about
next use and liveness of x, y, and z
<li> in the symbol table, set x to "not live", "no next use"
<li> in the symbol table, set y and z to "live", set next use of y,z to i
<li> treatment of <code>x := y</code> or <code>x := op y</code> is similar
</ol>
Note: order of (2) and (3) matters, x may be on RHS as well
</ul>

<h3> Storage for Temporaries </h3>

<ul>
<li> size of activation records grows with the number of temporaries, so
compiler should try to allocate temporaries carefully
<li> in general, two temporaries can use the same memory location if they
are not live simultaneously
<li> allocate temporaries by examining each in turn and assigning it the
first location in the field for temporaries that does not contain a live
temporary. If a temporary cannot be assigned to a previously created
location, use a new location.
</ul>

<h3> Storage for Temporaries Example </h3>

Consider the dot-product example:
<p>

<table border>
<tr><td>
t1 live<td>
<pre>
	prod := 0
	i := 1
L3:	t1 := 4 * i
	t2 := a[t1]
</pre>
<tr><td>t3 live<td>
<pre>
	t3 := 4 * i
	t4 := b[t3]
</pre>
<tr><td>t5 live<td>
<pre>
	t5 := t2 + t4
	t6 := prod + t5
</pre>
<tr><td>t7 live<td>
<pre>
	prod := t6
	t7 := i + 1
	i := t7
</pre>
<tr><td><td>
<pre>
	if i &lt;= 20 goto L3
</pre>
</table>

<p>
t1, t3, t5, t7 can share the same location.
<p>

Notes:
<ul>
<li> pretty much the same as register allocation
<li> optimal allocation is NP-complete in general
</ul>

<!--

<h3> Imports and Inheritance in Unicon </h3>

Unicon is different from mainstream languages, and this section is not intended
to tell you what you are supposed to do, it is intended to provide a basis for
comparison.

<h4> Syntax Tree Overview </h4>

Unicon uses "iyacc", a variant of Berkeley yacc, which is a cousin of Bison.
The <A href="unigram.y">unigram.y</A> grammar has like, 234 shift reduce
conflicts.  The semantic action at the import statement is illustrative of
tree construction as well as what little semantic analysis Unicon does.

<pre>
import: IMPORT implist {
   $$ := node("import", $1,$2," ")
   import_class($2)
   } ;
</pre>

For what its worth, the tree type in Unicon is very challenging and
sophisticated:
<pre>
record treenode(label, children)
procedure node(label, kids[])
   return treenode(label, kids)
end
</pre>
There are tricks here. It is really a heterogeneous tree with a mixture
of treenode, string, token, and various class objects.

<h4> Idol.icn </h4>

Despite the generic tree, various class objects
from <A href="idol.icn">idol.icn</A> store all the
interesting stuff in the syntax tree.  It is almost
really one class per non-terminal type, and those
non-terminals that have symbol tables have a field
in the class that contains the symbol (hash) table object.
<p>

class Package (one of the only parts of idol.icn I didn't write)
tells a real interesting story.  There is both an in-memory
representation of what we know about the world, and a persistent
on-disk representation (in order to support separate compilation).

<pre>
#
# a package is a virtual syntax construct; it does not appear in source
# code, but is stored in the database.  The "fields" in a package are
# the list of global symbols defined within that package.  The filelist
# is the list of source files that are linked in when that package is
# imported.
#
class Package : declaration(files, dir, classes)
   #
   # Add to the two global tables of imported symbols from this package's
   # set of symbols.  If sym is non-null, we are importing an individual
   # symbol (import "pack.symbol").
   #
   method add_imported(sym)
      local s, f

      if /dir then return
      
      f := open(dir || "/uniclass", "dr") |
	 stop("Couldn't re-open uniclass db in " || dir)
      every s := (if \sym then sym else fields.foreach()) do {
         if member(imported, s) then
             put(imported[s], self.name)
          else {
             imported[s] := [self.name]
          }

         if fetch(f, self.name || "__" || s) then {
            if member(imported_classes, s) then
               put(imported_classes[s], self.name)
            else {
               imported_classes[s] := [self.name]
            }
         }
      }
      close(f)
   end
   method Read(line)
      self$declaration.Read(line)
      self.files := idTaque(":")
      self.files$parse(line[find(":",line)+1:find("(",line)] | "")
   end
   method size()
      return fields$size()
   end
   method insertfname(filename)
      /files := idTaque(":")
      if files.insert(filename) then {
         write(filename, " is added to package ", name)
         writespec()
         }
      else write(filename, " is already in Package ", name)
   end
   method insertsym(sym, filename)
      if fields.insert(sym) then {
         write(sym, " added to package ", name)
         writespec()
         }
      else write(sym, " is already in Package ", name)
   end
   method containssym(sym)
       return \fields.lookup(sym)
   end
   method String()
      s := self$declaration.String()
      fs := files.String()
      if *fs > 0 then fs := " : " || fs
      s := s[1: (*tag + *name + 2)] || fs || s[*tag+*name+2:0]
      return s
   end
   method writespec()
   if \name & (f := open(env,"d")) then {
      insert(f, name, String())
      close(f)
      return
      }
   stop("can't write package spec for ", image(name))
   end
initially(name)
   if name[1] == name[-1] == "\"" then {
      name := name[2:-1]
      self.name := ""
      name ? {
	 if upto('/\\') then {
	    while self.name ||:= tab(upto('/\\')) do self.name ||:= move(1)
	    }
	 self.name ||:= tab(find(".")|0)
	 }
      }
   else {
      self.name := name
      }
   if dbe := fetchspec(self.name) then {
      Read(dbe.entry)
      self.dir := dbe.dir
      }
   /tag := "package"
   /fields := classFields()
end
</pre>

<h4> fetching a specification </h4>

Given a class name, how do we find it?  It must live in a GDBM database
(uniclass) somewhere along the IPATH. A bunch of tedious string parsing
concluding with a GDBM fetch.

<pre>
#
# find a class specification, along the IPATH if necessary
#
procedure fetchspec(name)
   static white, nonwhite
   local basedir := "."
$ifdef _MS_WINDOWS_NT
   white := ' \t;'
   nonwhite := &cset -- ' \t;'
$else
   white := ' \t'
   nonwhite := &cset -- ' \t'
$endif
   name ? {
      while basedir ||:= tab(upto('\\/')) do {
	 basedir ||:= move(1)
	 }
      name := tab(0)
      # throw away initial "." and trailing "/"
      if basedir[-1] == ("\\"|"/") then basedir := basedir[2:-1]
      }
   if f := open(basedir || "/" || env,"dr") then {
      if s := fetch(f, name) then {
	 close(f)
	 return db_entry(basedir, s)
	 }
      close(f)
      }

   if basedir ~== "." then fail # if it gave a path, don't search IPATH

   ipath := ipaths()

   if \ipath then {
      ipath ? {
         dir := ""
	 tab(many(white))
	 while dir ||:= tab(many(nonwhite)) do {
	    if *dir>0 & dir[1]=="\"" & dir[-1] ~== "\"" then {
		dir ||:= tab(many(white)) | { fail }
	       }
	    else {
		if dir[1]==dir[-1]=="\"" then dir := dir[2:-1]
		if f := open(dir || "/" || env, "dr") then {
		    if s := fetch(f, name) then {
			close(f); return db_entry(dir, s) }
		    close(f)
		}
		tab(many(white))
		dir := ""
	    }
	}
     }
  }
end
</pre>
-->


<!--
<h3> Peek at <a href="libctab.h">libctab.h</A> and <A href="libctab.c">libctab.c</A></h3>

<ul>
<li> Did you know that when you invoke the C compiler, you get a
library (-lc) linked in by default even when you did not ask for it?
<li> Similarly, ct can invoke the linker including libctab.o automatically
<li> By the way, in real life a libtab would need to have weirder variable
     names, such as all beginning with __ct__
<li> Using C++ for libctab would make certain aspects better
     (package to protect namespace, destructors to free local tables...)
     but beyond the scope of this class.
</ul>
-->



<h3> <A name="dag">DAG</A> representation of basic blocks </h3>

This concept is useful in code optimization.  Although we are not doing a
homework on optimization, you should understand it to be essential in real
life and have heard and seen a bit of the terminology.


<ul>
<li> Each <em>node</em> of a flow graph (i.e. basic block)
can be represented by a directed acyclic graph (DAG).
<li> Why do it?  May enable optimizations...
</ul>
<p>

A DAG for a basic block is one with the following labels on nodes:

<ol>
<li> leaves are labelled by unique identifiers, either variable names or
constants.
<li> interior nodes are labelled by operator symbols
<li> nodes are optionally given a sequence of identifiers as labels
(these identifiers are deemed to have the value computed at that node).
</ol>

<p>

<h4>Example</h4>

For the three-address code

<pre>
L:	t1 := 4 * i
	t2 := a[t1]
	t3 := 4 * i
	t4 := b[t3]
	t5 := t2 * t4
	t6 := prod + t5
	t7 := i + 1
	i := t7
	if i &lt;= 20 goto L
</pre>

What should the corresponding DAG look like?

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<p>
<font size=1> <A name=53>lecture #53</A> began here</font>
<p>

<ul>
<li> Chapter 6 of the text presents DAGs constructed from syntax
     trees immediately before, rather than after, three address code.
<li> We presented it later than that, because it enables common optimizations.
</ul>

<h3> Constructing a DAG </h3>

<u>Input</u>: A basic block.
<p>

<u>Output</u>: A DAG for the block, containing:
<ul>
<li> a label for each node, and
<li> for each node, a (possibly empty) list of attached identifiers
</ul>
<p>

<u>Method</u>: Consider an instruction x := y op z.
<ol>
<li> If node(y), the node in the DAG that represents the value of y at that
point, is undefined, then create a leaf labelled y. Let node(y) be this node.
Similar for z.

<li> determine if there is a node labelled <u>op</u> with left child node(y)
and right child node(z).  if not, create such a node. let this node be <u>n</u>

<li> <ul>
     <li> a) delete x from the list of attached identifiers for node(x) [if defined]
     <li> b) append x to the list of attached identifiers for node n (from 2).
     <li> c) set node(x) to n
     </ul>
</ol>

<h3> Applications of DAGs </h3>

<ol>
<li> automatically detects common subexpressions
<li> can determine which identifiers have their value used in the block --
these are identifiers for which a leaf is created in step (1) at some  point.
<li> Can determine which statements compute values that could be used outside
the block -- these are statements s whose node n constructed in step (2)
still has node(x)=n at the end of the DAG construction, where x is the
identifier defined by S.
<li> Can reconstruct a simplified list of 3-addr instructions, taking advantage
of common subexpressions, and not performing copyin assignments of the form
x := y unless really necessary.
</ol>

<h3> Evaluating the nodes of a DAG </h3>

<ul>
<li> The evaluation order of the interior nodes of a DAG must be consistent
with a topological sort of the DAG, so that operands are evaluated before an
operator is applied.
<li> In the presence of pointer or array assignments, or procedure calls, not
every topological sort may be permissible.
<th> Example: given a basic block
<pre>
x := a[i]
a[j] := y
z := a[i]
</pre>
</ul>

The "optimized" basic block after DAG construction and common subexpression
elimination equates x and z, but this behaves incorrectly when i = j.



<h3> Code Optimization </h3>

There are major classes of optimization that can significantly speedup
a compiler's generated code.  Usually you speed up code by doing the
work with fewer instructions and by avoiding unnecessary memory reads
and writes. You can also speed up code by rewriting it with fewer gotos.


<h4> Constant Folding </h4>

Constant folding is performing arithmetic at compile-time when the values
are known.  This includes simple expressions such as 2+3, but with more
analysis
some variables' values may be known constants for some of their uses.
<pre>
     x = 7;
     ...
     y = x+5;
</pre>

<h4> Common Subexpression Elimination </h4>

Code that redundantly computes the same value occurs fairly frequently,
both explicitly because programmers wrote the code that way, and implicitly
in the implementation of certain language features.
<p>

Explicit:
<pre>
    (a+b)*i + (a+b)/j;
</pre>

The (a+b) is a common subexpression that you should not have to compute twice.
<p>

Implicit:
<pre>
    x = a[i]; a[i] = a[j]; a[j] = x;
</pre>

Every array subscript requires an addition operation to compute the memory
address; but do we have to compute the location for a[i] and a[j] twice in
this code?


<h4> Loop Unrolling </h4>

Gotos are expensive (do you know why?).  If you know a loop will
     execute at least (or exactly) 3 times, it may be faster to copy the
     loop body those three times than to do a goto.  Removing gotos
     simplifies code, allowing other optimizations.

<table border>
<tr>
<td>
<pre>
for(i=0; i<3; i++) {
   x += i * i;
   y += x * x;
   }
</pre>
<td>
<pre>
   x += 0 * 0;
   y += x * x;
   x += 1 * 1;
   y += x * x;
   x += 2 * 2;
   y += x * x;
</pre>
<td>
<pre>
   y += x * x;
   x += 1;
   y += x * x;
   x += 4;
   y += x * x;
</pre>
</table>

<h4> Hoisting Loop Invariants </h4>

<table border>
<td>
<pre>
for (i=0; i&lt;strlen(s); i++)
   s[i] = tolower(s[i]);
</pre>
<td>
<pre>
t_0 = strlen(s);
for (i=0; i&lt;t_0; i++)
   s[i] = tolower(s[i]);
</pre>
</table>


<h4> Peephole Optimization </h4>

Peephole optimizations look at the native code through a small, moving
window for specific patterns that can be simplified.  These are some of the
easiest optimizations because they potentially don't require any analysis
of other parts of the program in order to tell when they may be applied.
Although some of these are stupid and you wouldn't think they'd come up,
the simple code generation algorithm we presented earlier is quite stupid
and does all sorts of obvious bad things that we can avoid.
<p>

<table border>
<tr>
<th> name
<th> sample
<th> optimized as
<tr>
<td> redundant load or store
<td>
<pre>
MOVE R0,a
MOVE a,R0
</pre>
<td>
<pre>
MOVE R0,a
</pre>
<tr>
<td> dead code
<td>
<pre>
#define debug 0
...
if (debug) printf("ugh");
</pre>
<tr>
<td> control flow simplification
<td>
<pre>
if a &lt; b goto L1
...
L1: goto L2
</pre>
<td>
<pre>
if a &lt; b goto L2
...
L1: goto L2
</pre>
<tr>
<td> algebraic simplification
<td>
<pre>
x = x * 1;
</pre>
<tr>
<td> strength reduction
<td>
<pre>
x = y * 16;
</pre>
<td>
<pre>
x = y << 4;
</pre>
</table>

<p>
<font size=1> <A name=54>lecture #54</A> began here</font>
<p>

<h3> Question from the Mail </h3>

<blockquote>
I saw your in your three address code examples of calling a function that
pass array variables that before PARAM, you always put the array
address into a temporary variable.

like:
<pre>
char s[64];
readline(s)

	ADDR   loc:68,loc:0
	PARAM8 loc:68
	CALL   readline,1,loc:68
</pre>

and  printf("%d\n", 10+2);

<pre>
     addr	    loc:0,string:0
     parm8	    loc:0
     add	    loc:8,im:10,im:2
     parm	    loc:8
     call	    printf,12,loc:12
</pre>

So, before passing the variables to the function, do you have to copy
the variables to the temporary variables? And then PARAM the temporay
variables. Or it is only true for passing array address?

<p>

I know the PARAM will copy the passing arguments into the called function's
activation record's parameter region. So there is no need copy the parameter
variables into temporary variables then PARAM the temporary variables.

</blockquote>

Answer: 
The three-address instructions use addresses, but they normally operate by implicitly fetching and storing values pointed to by those addresses.
<p>

The ADDR instruction does not copy the variable into a temporary variable, it copies the address given, without fetching its contents, into its destination.  This is needed in order to pass an array parameter in C.  In Pascal, by default we would have to allocate (on the stack) an entire physical copy of the whole array in order to pass it as a parameter. This is very expensive.


<h3> Peephole Optimization Examples </h3>

It would be nice if we had time to develop a working demo program for
peephole optimization, but let's start with the obvious.

<p>

<table border>
<tr><th>as generated<th>replace with<th>comment
<tr>
<td>
<pre>
	movq	%rdi, -56(%rbp)
	cmpq	$1, -56(%rbp)
</pre>
<td>
<pre>
	movq	%rdi, -56(%rbp)
	cmpq	$1, %rdi
</pre>
<td> reuse n that's already in a register
<tr>
<td>
<pre>
	cmpq	$1, %rdi
	setle	%al
	movzbl	%al,%eax
	movq	%rax, -8(%rbp)
	cmpq	$0, -8(%rbp)
	jne	.L0
</pre>
<td>
<pre>
	cmpq	$1, %rdi
	jle	.L0
</pre>
<td> 
boolean variables are for wimps.<br>
setle sets a byte register (%al) to contain a boolean <br>
movzbl zero-extends a byte to a long (movsbl sign-extends)
<tr>
<td>
<pre>
	cmpq	$1, %rdi
	jle	.L0
	jmp	.L1
.L0:
</pre>
<td>
<pre>
	cmpq	$1, %rdi
	jg	.L1
.L0:
</pre>
<td> 
Use fall throughs when possible; avoid jumps.

<tr>
<td>
<pre>
	movq	%rax, -16(%rbp)
	movq	-16(%rbp), %rdi
</pre>
<td>
<pre>
	movq	%rax, %rdi
</pre>
<td> 
TAC code optimization might catch this sooner
<tr>
<td>
<pre>
	movq	-56(%rbp), %rax
	subq	$1, %rax
	movq	%rax, %rdi
</pre>
<td>
<pre>
	movq	-56(%rbp), %rdi
	subq	$1, %rdi
</pre>
<td> 
What was so special about %rax again?
<tr>
<td>
<pre>
	movq	%rax, -40(%rbp)
	movq	-24(%rbp), %rax
	addq	-40(%rbp), %rax
</pre>
<td>
<pre>
	addq	-24(%rbp), %rax
</pre>
<td> 
Addition is commutative.
</table>




<h4> Interprocedural Optimization </h4>

Considering memory references across procedure call boundaries;
     for example, one might pass a parameter in a register if both
     the caller and callee generated code knows about it.
<h4> argument culling </h4>
 when the value of a specific parameter is a constant, a custom version
     of a called procedure can be generated, in which the parameter is
     eliminated, and the constant is used directly (may allow additional
     constant folding).

<table border>
<td>
<pre>
f(x,r,s,1);

int f(int x, float y, char *z, int n)
{
  switch (n) {
  case 1:
     do_A; break;
  case 2:
     do_B; break;
     ...
     }
}
</pre>
<td>
<pre>
f_1(x,r,s);

int f_1(int x, float y, char *z)
{
   do_A;
}
int f_2(int x, float y, char *z)
{
   do_B;
}
...
</pre>

</table>



<h3> Dominators and Loops </h3>

Raison d'etre: many/various Loop Optimizations require that loops be
specially identified within a general flow graph context.  If code is
properly structured (e.g. no "goto" statements) these loop optimizations are
safe to do, but in the general case for C you would have to check...

<dl>
<dt> dominator
<dd> node d in a flow graph <em>dominates</em> node n (written as "d dom n")
if every path from the initial node of the flow graph to n goes through d
<dt> dominator tree
<dd> tree formed from nodes in the flow graph whose root is the initial node,
and node n is an ancestor of node m only if n dominates m. Each node in a
flow graph has a unique "immediate dominator" (nearest dominator), hence a
dominator tree can be formed.
</dl>

<img src="domtree.png" width=500>

<h3> Loops in Flow Graphs </h3>

<ul>
<li> Must have a single entry point (the header) that dominates all nodes
<li> Must be a way to iterate; at least one path back to the header
<li> To find loops: look for edges a-&gt;b where b dominates a (back edges)
<li> Given a back edge n-&gt;d, the <em>natural loop</em> of this edge is
d plus the set of nodes that can reach n without going through d.
<li> every back edge has a natural loop...
</ul>

<p>
<font size=1> <A name=55>lecture #55</A> began here</font>
<p>

<h3> Comments on Debugging Assembler </h3>

The compiler writer that generates bad assembler code may need to debug
it in order to understand why it is wrong.
<ul>
<li> See <A href="http://dbp-consulting.com/tutorials/debugging/basicAsmDebuggingGDB.html">this tutorial from DBP Consulting</A> for some good ideas
<li> You almost only need to learn gdb's si and ni commands.
<li> You also need to know "as --gstabs+"
<li> You also need to know "info registers", or "i r" (e.g. "i r eax")
<li> In plain assembler debugging s and n work in lieu of si and ni
</ul>

<H3> Another Word on Interprocedural Optimization </h3>

In general in this optimization unit, I've been mentioning the
biggest categories of compiler optimization and giving very brief
examples.  That "argument culling" example of interprocedural
optimization deserves at least a little more context:

<ul>
<li> Interprocedural optimization (IPO) includes any optimizations that
     apply across function call boundaries, not just culling
<li> Because function call boundaries are what is being optimized, this
     will often focus on analysis of information known about parameters
     and return type
<li> Includes function inlining, if the compiler decides when to do that,
     rather than leave the decision up to the programmer.
<li> Can only do interprocedural optimization on procedures the compiler
     knows about; limited value unless compiling whole program together,
     or embedding in linker
<li> Modern production compilers have extra command-line options for IPO
</ul>


<h3> Algorithm to construct the natural loop of a back edge </h3>

Input: a flow graph <code>G</code> and a back edge <code>n -&gt; d</code> <br>
Output: the set (named <em>loop</em>) consisting of all nodes in the
natural loop of <code>n -&gt; d</code>.
<p>
Method: depth-first search on the reverse flow graph <code>G'</code>.
Start with loop containing only node <code>n</code> and <code>d</code>.
Consider each node <code>m | m != d</code> that is in
<em>loop</em>, and insert <code>m</code>'s predecessors in
<code>G</code> into <em>loop</em>. Each
node is placed once on a stack, so its predecessors will be examined.
Since <code>d</code> is put in <em>loop</em> initially, its predecessors
are not examined.

<pre>
procedure insert(m)
   if not member(loop, m) then {
      loop := loop ++ { m }
      push m onto stack
   }
end

main:
   stack := []
   loop := { d }
   insert(n)
   while stack not empty do {
      pop m off stack
      for each predecessor p of m do insert(p)
      }
</pre>

<h3> Inner Loops </h3>

<ul>
<li> If only natural loops are considered then unless two loops have the same
header, they are either disjoint or one is nested within the other.
<br>
<img src="innerloops0001.png" width=400>

<li> If two loops share the same header, neither is inner to the other,
instead they are treated as one loop.
<br>
<img src="innerloops0002.png" width=200>
</ul>



<ul>
<li> example of calling C library functions from generated final code
     (printf, getchar)
<li> 1 or more examples to illustrate lecture stuff such as natural loops alg.
</ul>



<!--
<h3> Illustration from Unicon </h3>

Unicon resolves each class' inheritance information at compile time, and
generates a <em>field table</em> for runtime calculations that map field
names to slot#/offsets.  Methods vectors are just structs,
shared by objects by means of a pointer (__m) added to class instances.
-->


<h3> Code Generation for Input/Output </h3>

It may be productive to contemplate how to generate code for basic
C input/output constructs, and how that compares with basic C++ I/O.

<dl>
<dt> getchar()
<dd> Basic appearance of a call to getchar() in final code:
<pre>
	call	getchar
	movl	%eax, <em>destination</em>
</pre>
<dt> printf(s...)
<dd> First parameter is passed in %rdi. There is an "interesting"
section in the AMD64 reference manuals about how 32-bit operands are
automatically sign-extended in 64-bit registers, but 8- and 16-bit operands
are not automatically signed extended in 32-bit registers.
If string s has label .LC0
<pre>
	movl	$.LC0, %eax	; load 32-bit addr
				; magically sign-extended to 64-bits
	movq	%rax, %rdi	; place 64-bit edition in param #1 reg.
	call	printf		; call printf
</pre>

<dt> printf(s, i)
<dd> Printf'ing an int ought to be the simplest printf.
The second parameter is passed in %rsi.  If you placed a 32-bit
int in %esi you would still be OK.
<pre>
	movq	<em>source</em>, %rsi	; what we would do
	movl	<em>source</em>, %esi	; "real" C int: 32, 64, same diff
</pre>

<dt> printf(s, c)
<dd> Printf'ing a character involves passing that char as a parameter.
Generally when passing
a "char" parameter one would pass it in a (long word, aligned) slot, and
it is prudent to (basically) promote it to "int" in this slot.
<pre>
	movsbl	<em>source</em>, %esi
</pre>

<dt> printf(s, s)
<dd> Printf'ing a string involves passing that string as a parameter.
For local variable string constant data, gcc appears to be doing
some <A href="x64/printf-s.s">pretty weird stuff</A>.  I'm initially more
comfortable with a <A href="x64/printf-s-2.s">tamer approach</A>

</dl>

<h3> C++ Output </h3>

Now, how about C++?  After some thought, we concluded that each output
(send) operator could be implemented by generating code for one call to
printf, with an appropriate format string for the type of the right operand.
This output is a side effect.
The expression result of the output operator is its left operand.


<p>
<font size=1> <A name=56>lecture #56</A> began here</font>
<p>

Potential optimizations of the preceding method for C++ output operations
include:
<ul>
<li> bundling a string of output operators into a single call to printf, or
     almost the opposite,
<li> using more direct output functions than printf, which is not famous for
     speed.  C++ may have dumped C stdio specifically because they believed
     printf's "mini-intepreter" of format strings was suboptimal.
     For example, fputc(c,stdout) is faster than printf("%c",c).
     Q: what is the fastest way to write out an integer?  What about a double?
</ul>

<h3> C++ Input Operator </h3>

Consider for a moment how to input an integer.  This is pretty fundamental;
even toy programs usually let a user enter a number.  The C program for it
might use scanf("%d", &amp;i), but in C++ one says cin &gt;&gt; i.

<table border>
<tr>
<th> "Real" <th> C-like, for 120++
<tr>
<td>
<pre>
	leaq	-8(%rbp), %rax
	movq	%rax, %rsi
	movl	$_ZSt3cin, %edi
	call	_ZNSirsERi
</pre>
<td>
<pre>
	leaq	-8(%rbp), %rax
	movq	%rax, %rsi
	movl	$.LC0, %edi
	movl	$0, %eax
	call	scanf
</pre>
</table>


<h3> Code Generation for Virtual Machines </h3>

A virtual machine architecture such as the JVM changes the "final" code
generation somewhat.  We have seen several changes, some of which
simplify final code generation and some of which complicate things.

<dl>
<dt> no registers, simplified addressing
<dd> a virtual machine may omit a register model and avoid complex
     addressing modes for different types of variables
<dt> uni-size or descriptor-based values
<dd> if all variables are "the same size", some of the details of
     memory management are simplified.  In Java most values occupy
     a standard "slot" size, although some values occupy two slots.
     In Icon and Unicon, all values are stored using a same-size descriptor.
<dt> runtime type system
<dd> requiring type information at runtime may complicate the
     code generation task since type information must be present
     in generated code.  For example in Java method invocation and
     field access instructions must encode class information.
</dl>

Just for fun, let's compare the generated code for java with that X86
native code we looked at earlier when we were talking about how to make
variables spill out of registers:
<pre>
	iload_1
	iload_2
	iadd
	iload_3
	iload 4
	iadd
	imul
	iload 5
	iload 6
	iadd
	iload 7
	iload_1
	iadd
	imul
	iload_3
	iload 5
	imul
	idiv
	iadd
	istore_1
</pre>

What do you see?
<br><br><br><br><br><br>
<ul>
<li> Stack-machine model. Most instructions implicitly use the stack.
<li> Difference between "iload_3" and "iload 4": Java VM has special
     opcodes that run faster for first 3 locals/temporaries.
</ul>



<h3> Runtime Systems </h3>

Every compiler (including yours) needs a runtime system.  A runtime system
is the set of library functions and possibly global variables maintained by
the language on behalf of a running program.  You use one all the time; in C
it functions like printf(), plus perhaps internal compiler-generated calls
to do things the processor doesn't do in hardware.<p>

So you need a runtime system; potentially, this might be as big or bigger a
job than writing the compiler.  Languages vary from assembler (no runtime
system) and C (small runtime system, mostly C with some assembler) on up to
Java (large runtime system, mostly Java with some C) and in even higher level
languages the compiler may evaporate and the runtime system become gigantic.
The Unicon language has a relatively trivial compiler and gigantic virtual
machine and runtime system.  Other scripting languages might have no compiler
at all, doing everything (even lexing and parsing) in the runtime system.
<p>

<!--
For your project: whether you generate C or X86 or Java, you'll need a plan
for what to do about a runtime system.  And, in principle, I am not opposed
to helping with this part.  But the compiler and runtime system have to fit
together; if I write part of the BASIC runtime system for you, or we write
it together, we have to agree on things such as: what the types of
parameters and return values must look like.<p>

So, what belongs in a Color BASIC runtime system?  Anything not covered
by a three address instruction.  Looking at cocogram.y:
<ul>
<li> INPUT/PRINT
<li> READ/DATA
<li> CLEAR
<li> CLOAD/CSAVE/SKIPF
<li> CLS
<li> SET/RESET
<li> SOUND
<li> CHR$, LEFT$, MID$, RIGHT$, INKEY$
<li> ASC, INT, JOYSTK, LEN, PEEK, RND, VAL
<li> DIM
<li> string +, string compares
</ul>
<p>

What would a runtime system function look like?  It would take in and
pass out BASIC values, represented as C structs.  You would then link
this code in to your generated C or assembler code (if you generated
Java code, you would have to deal with the Java Native Interface or
else write these functions in Java).
<pre>
void PRINT(struct descrip *d)
{
   switch (d->type) {
   case INTEGER: printf("%d",d->value.ival); break;
   case REAL: printf("%f",d->value.rval); break;
   case STRING: printf("%*s",d->size,d->value.string); break;
   case ARRAY: printf("cannot print arrays"); break; /* can't get here */
   default: printf("PRINT: internal error, type %d\n", d->type);
   }
}
</pre>

Now, let's look at the "whole" runtime system:

<ul>
<li> <A href="libb.c">libb.c</A>
</ul>

<h3> More on Memory Management in the BASIC Runtime System </h3>

Arrays are interesting.  They can be used without being declared or DIM'ed.
They can only be DIM'ed once.  If you use them before they are DIM'ed, they
are implicitly DIM'ed to size 11 and can't be re-DIM'ed.
<p>
What do variables A, A(), A$, and A$() look like in memory?  How does our
runtime system make it so?
<p>
Let's take a look at DIM, in libc.c.  This DIM is for arrays of numbers.
How would you handle arrays of strings?
<p>
Can you implement STRCAT for your BASIC runtime system?
<p>
What other BASIC statements, operators, or functions allocate memory?
<p>
How would we avoid memory "leaks"?

<h3> STRCAT </h3>

So, what does your STRCAT look like?  <A href="libb.c#strcat">Here's one.</A>

<h3> GOSUBs </h3>

Our 3-address instruction set has call and return instructions, but basic
is less structured than regular procedural languages; you can GOSUB to any
line number you want.  You can't use a variable to GOSUB to line number X,
but in principle every line number could be the target of a procedure call.
<p>
If you use the "call" (3-address) instruction to do GOSUB, your native code
will have to make a clear distinction between BASIC call's and calls to
runtime system (built-in) functions.  Perhaps it is best to implement BASIC
GOSUB by pushing a "param" (the next instruction following the GOSUB) and
a "goto".  The BASIC RETURN is then a "pop" followed by a "goto".  What,
we don't have a "pop" 3-address instruction?  We do now...  the name of
"param" should probably be "push" anyhow.
<p>

Come to think of it, we've been talking about doing a call to a built-in
function such as PRINT, but that PRINT function we wrote is C code; it
doesn't do a 3-address "ret" instruction, hmmm.  How are we going to
generate the native code for the 3-address "call" instruction?
It may include an assembler call instruction, but it may also involve
instructions to handle the interface between BASIC and C.
-->
<!--
<h3> Flex SDK Comments </h3>

Adobe's Flex SDK web pages point you only at Windows binary downloads,
but if you dig further, it appears to be some big Java open source project.
<ul>
<li><A href="http://opensource.adobe.com/wiki/display/flexsdk/Setup+on+Linux">build instructions</A>
<li> You have to have a good Java and Ant (wormulon didn't, Larry seems
     to have put it on for me
<li> Warning: the SVN checkout can take HOURS on a cable modem.  Literally
     filled my laptop hard drive over several hours without completing.
     It occurs to me this might be their way of making their "open source"
     project as not-open as possible.
     Suggest you append "/trunk" to the name they say to checkout.
     Long checkout seems to be worsened by an idiotic policy of putting open
     sandbox end-user junk in the SVN repository! But also, it is big.
<li> Tried with /trunk on eternium, ran 46 minutes and then died with an
     error prior to completion
<li> Instructions say to "source setup.sh", but this did not go so well for
     me on wormulon (must be running bash, not csh). When I switched over to
     bash, complained about many missing or renamed files and died.
<li>Conclusions:
    (a) easier to just get this on a Windows machine if you have one.
    (b) example of an "open source" project that isn't really open.
</ul>
-->





<h3> Final Exam Review </h3>

The final exam is comprehensive, but with a strong emphasis on "back end"
compiler issues: symbol tables, semantic analysis, and code generation.


<ul>
<li>  Review your lexical analysis, regular expressions, and finite automata.
<li>  Review your syntax analysis, CFG's, and parsing.
<li>  If a parser discovers a syntax error, how can it report what line
      number that error occurs on?  If semantic analysis discovers a
      semantic error (or probable semantic error), how can it report what
      line number that error occurs on?
<li>  What are symbol tables?  How are they used?
      What information is stored there?
<li>  How does information get into a symbol table?
<li>  How many symbol tables does a compiler need?
<li>  What is "semantic analysis"?
<li>  What does "semantic analysis" accomplish? What are its side effects?
<li>  What are the primary activities of a compiler's semantic analyzer?
<li>  What are memory regions, and why does a compiler care?
<li>  What memory regions are there, and how do they affect code generation?
<li>  What does code generation do, anyhow?
<li>  What kinds of code generation are there?
<li>  Why do (almost all) compilers use an "intermediate code"?  What does
      intermediate code look like?  How is it different from final code?
</ul>
